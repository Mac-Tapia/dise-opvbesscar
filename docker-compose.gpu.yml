services:
  # GPU Pipeline Execution
  pvbesscar-pipeline-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: pvbesscar:latest-gpu
    container_name: pvbesscar-pipeline-gpu
    hostname: pvbesscar-pipeline-gpu
    
    volumes:
      - ./data:/app/data
      - ./outputs:/app/outputs
      - ./configs:/app/configs:ro
      - pipeline_cache_gpu:/app/.cache
    
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - OMP_NUM_THREADS=8
      - CUDA_VISIBLE_DEVICES=0
      - TORCH_HOME=/app/.torch
      - PYTHONPATH=/app/src:/app
    
    # GPU runtime
    runtime: nvidia
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          cpus: '12'
          memory: 48G
        limits:
          cpus: '16'
          memory: 64G
    
    restart: unless-stopped
    stdin_open: true
    tty: true
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
        labels: "service=pipeline-gpu"
    
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; print(f'GPU: {torch.cuda.is_available()}')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Checkpoint monitoring with GPU support
  pvbesscar-monitor-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    image: pvbesscar:latest-gpu
    container_name: pvbesscar-monitor-gpu
    hostname: pvbesscar-monitor-gpu
    
    depends_on:
      pvbesscar-pipeline-gpu:
        condition: service_healthy
    
    volumes:
      - ./outputs:/app/outputs:ro
      - ./monitor_logs:/app/monitor_logs
    
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app/src:/app
      - CHECKPOINT_DIR=/app/outputs/oe3/checkpoints
      - TERM=xterm
    
    command: ["python", "scripts/monitor_checkpoints.py"]
    
    runtime: nvidia
    
    restart: unless-stopped
    
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"
        labels: "service=monitor-gpu"
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          cpus: '2'
          memory: 8G
        limits:
          cpus: '4'
          memory: 16G

  # GPU Jupyter Lab
  pvbesscar-jupyter-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    image: pvbesscar:latest-gpu
    container_name: pvbesscar-jupyter-gpu
    hostname: pvbesscar-jupyter-gpu
    
    ports:
      - "8889:8888"
    
    volumes:
      - .:/workspace
      - ./outputs:/workspace/outputs
    
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - PYTHONUNBUFFERED=1
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app/src:/app
    
    command: jupyter lab --ip=0.0.0.0 --allow-root --no-browser
    
    runtime: nvidia
    
    restart: unless-stopped
    
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          cpus: '4'
          memory: 16G
        limits:
          cpus: '8'
          memory: 32G

volumes:
  pipeline_cache_gpu:
