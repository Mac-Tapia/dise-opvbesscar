#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""SAC v2.0 - 5 SOLUCIONES ESPECÃFICAS (VERSIÃ“N CORREGIDA - MISMOS PESOS PARA TODOS)"""

import sys

output = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ğŸ”§ SAC v2.0: 5 SOLUCIONES CORREGIDAS - SIN CAMBIAR PESOS DE RED                      â•‘
â•‘            (Los 3 agentes usan MISMA ARQUITECTURA: [256,256] Actor/Critic)                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ARQUITECTURA UNIFICADA - TODOS LOS AGENTES [PPO / A2C / SAC]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ANTES (INCONSISTENTE):
  PPO: Actor [256,256,128] + Critic [512,512,256]   â† DIFERENTE
  A2C: Actor [256,256]     + Critic [256,256]       â† DIFERENTE
  SAC: Actor [512,512]     + Critic [512,512]       â† DIFERENTE

DESPUÃ‰S (UNIFICADO - RECOMENDADO):
  PPO: Actor [256,256] + Critic [256,256]  âœ… CAMBIAR
  A2C: Actor [256,256] + Critic [256,256]  âœ… YA ESTÃ OK
  SAC: Actor [256,256] + Critic [256,256]  âœ… CAMBIAR (eliminar SoluciÃ³n #5)

TODOS USAN: [256,256] capas ocultas
COMPARACIÃ“N JUSTA: Misma arquitectura, diferente entrenamiento


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SOLUCIÃ“N #0: UNIFICAR ARQUITECTURA (SIN CAMBIAR PESOS - CORRECCIÃ“N META)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CAMBIO (SAC):
  
  ANTES (âŒ):
  policy_kwargs: Dict[str, Any] = field(default_factory=lambda: {
      'net_arch': dict(pi=[512, 512], qf=[512, 512]),  # âŒ 512x512
      ...
  })

  DESPUÃ‰S (âœ…):
  policy_kwargs: Dict[str, Any] = field(default_factory=lambda: {
      'net_arch': dict(pi=[256, 256], qf=[256, 256]),  # âœ… 256x256 (MISMO QUE PPO/A2C)
      ...
  })

ARCHIVO: src/agents/train_sac_multiobjetivo.py
LÃNEA:   ~392 (dentro de @dataclass SACConfig)

IMPACTO: Ahora los 3 agentes usan EXACTAMENTE la misma arquitectura de red.
         Solo diferencia es el algoritmo de entrenamiento (SAC vs PPO vs A2C).


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SOLUCIÃ“N #1: REWARD NORMALIZATION (CRÃTICA) - 5 minutos
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROBLEMA:
  Rewards en escala [-3, 0] en lugar de [0, 2]
  â†’ Critic predice Q=2.0 pero reward real=-2.0
  â†’ Loss = (2.0 - (-2.0))Â² = 16.0 â† ENORME
  â†’ Gradientes explotan, convergencia imposible

ARCHIVO:   src/agents/train_sac_multiobjetivo.py
CLASE:     MultiObjectiveReward
MÃ‰TODO:    __call__()

CAMBIO REQUERIDO:

  ANTES (âŒ):
  def __call__(self, info: dict) -> float:
      co2_benefit = info.get('co2_avoided_kg', 0) / 1000  # Crea escala negativa
      total = co2_benefit * 0.5 + ...
      return total  # Rango: [-3, 0] âŒ

  DESPUÃ‰S (âœ…):
  def compute_reward_components(self, info: dict) -> dict:
      # Normalize to [0, 1]
      co2_norm = min(info.get('co2_avoided_kg', 0) / 50000, 1.0)
      solar = info.get('solar_pct', 0) / 100
      charge = info.get('vehicle_charge_pct', 0) / 100
      
      # Scale with weights
      components = {
          'co2': co2_norm * 100,        # [0, 100]
          'solar': solar * 50,          # [0, 50]
          'vehicles': charge * 30,      # [0, 30]
          'grid': grid * 20,            # [0, 20]
          'bess': bess * 20,            # [0, 20]
      }
      return components

  def __call__(self, info: dict) -> float:
      components = self.compute_reward_components(info)
      raw_total = sum(components.values())  # [0, 220]
      normalized = (raw_total / 110) + 0.01  # [0.01, 2.01] âœ…
      return normalized

IMPACTO: Soluciona 70% del problema (rewards negativos)
âš ï¸  NO TOCA PESOS: Solo cambia cÃ¡lculo de reward, no arquitectura de red


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SOLUCIÃ“N #2: REPLAY BUFFER & LEARNING STARTS (CRÃTICA) - 5 minutos
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROBLEMA:
  learning_starts = 5K de 87.6K = 5.7% (MUY BAJO)
  buffer_size = 400K (INSUFICIENTE para 87.6K dataset)
  â†’ Critic entrena con datos ruidosos inmediatamente
  â†’ No hay suficiente estabilidad antes de aprender

ARCHIVO:   src/agents/train_sac_multiobjetivo.py
FUNCIÃ“N:   agent = SAC(...)

CAMBIOS REQUERIDOS:

  PARÃMETRO           ACTUAL    PROPUESTO    RAZÃ“N
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  buffer_size         400_000   600_000      +50% mÃ¡s experiencias
  learning_starts     5_000     15_000       +200% (5.7% â†’ 17% warmup)
  train_freq          (2,step)  (1,step)     Entrenar cada paso
  batch_size          128       256          2Ã— menos ruidoso

CÃ“DIGO ACTUAL (âŒ):
  agent = SAC(
      policy="MlpPolicy",
      env=env,
      buffer_size=400_000,        # âŒ Bajo
      learning_starts=5_000,      # âŒ Bajo (5.7%)
      batch_size=128,             # âŒ PequeÃ±o
      train_freq=(2, "step"),     # âŒ Cada 2 steps
      ...
  )

CÃ“DIGO PROPUESTO (âœ…):
  agent = SAC(
      policy="MlpPolicy",
      env=env,
      buffer_size=600_000,        # âœ… 50% mÃ¡s
      learning_starts=15_000,     # âœ… 17% warmup (6 semanas datos)
      batch_size=256,             # âœ… 2Ã— mÃ¡s grande
      train_freq=(1, "step"),     # âœ… Cada step
      ...
  )

IMPACTO: Soluciona warmup insuficiente + estabilidad buffer
âš ï¸  NO TOCA PESOS: Solo cambia configuraciÃ³n de entrenamiento


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SOLUCIÃ“N #3: TARGET UPDATE DYNAMICS (ALTA PRIORIDAD) - 5 minutos
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROBLEMA:
  tau = 0.005 (DEMASIADO ALTO)
  gradient_steps = 4 (DEMASIADOS UPDATES)
  â†’ Soft updates cambian target network muy rÃ¡pido
  â†’ Q-values oscilatiles sin convergencia

ARCHIVO:   src/agents/train_sac_multiobjetivo.py
PARÃMETRO: tau, gradient_steps

CAMBIOS REQUERIDOS:

  PARÃMETRO         ACTUAL    PROPUESTO    CAMBIO
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  tau               0.005     0.001        5Ã— mÃ¡s suave
  gradient_steps    4         2            Menos updates agresivos

CÃ“DIGO ACTUAL (âŒ):
  agent = SAC(
      ...
      tau=0.005,               # âŒ Muy agresivo
      gradient_steps=4,        # âŒ Demasiadas actualizaciones
      ...
  )

CÃ“DIGO PROPUESTO (âœ…):
  agent = SAC(
      ...
      tau=0.001,               # âœ… Softer updates (5Ã— improvement)
      gradient_steps=2,        # âœ… 2 updates instead of 4
      ...
  )

MATEMÃTICA:
  Î¸_target = Ï„ * Î¸_new + (1-Ï„) * Î¸_old
  
  Actual (Ï„=0.005):   90% cambio por step â† RÃPIDO (inestable)
  Propuesto (Ï„=0.001): 18% cambio por step â† SUAVE (convergencia)

IMPACTO: Elimina oscilaciones de Q-values
âš ï¸  NO TOCA PESOS: Solo cambia dinÃ¡micas de actualizaciÃ³n


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SOLUCIÃ“N #4: ENTROPY COEFFICIENT (MEDIA PRIORIDAD) - 2 minutos
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROBLEMA:
  ent_coef = "auto" (AUTO-TUNE)
  â†’ Con rewards negativos, auto-tune puede divergir
  â†’ EntropÃ­a coefficient se vuelve inestable

ARCHIVO:   src/agents/train_sac_multiobjetivo.py
PARÃMETRO: ent_coef

CÃ“DIGO ACTUAL (âŒ):
  agent = SAC(
      ent_coef="auto",         # âŒ Auto-tune (puede divergir)
      target_entropy=-39.0,
      ...
  )

CÃ“DIGO PROPUESTO (âœ…):
  agent = SAC(
      ent_coef=0.01,           # âœ… Fijo en 1% (estable)
      # target_entropy=None,   # Remove (no needed)
      ...
  )

IMPACTO: Evita divergencia de entropy coefficient
âš ï¸  NO TOCA PESOS: Solo cambia factor de entropÃ­a


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SOLUCIÃ“N #5: LEARNING RATE (MEDIA PRIORIDAD) - 1 minuto
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROBLEMA:
  learning_rate = 5e-4 (ALTO)
  â†’ Learning rate muy agresivo causa divergencia
  â†’ Gradientes grandes hacen parÃ¡metros saltar

ARCHIVO:   src/agents/train_sac_multiobjetivo.py
PARÃMETRO: learning_rate

CÃ“DIGO ACTUAL (âŒ):
  agent = SAC(
      learning_rate=5e-4,      # âŒ Alto/agresivo
      ...
  )

CÃ“DIGO PROPUESTO (âœ…):
  agent = SAC(
      learning_rate=3e-4,      # âœ… MÃ¡s conservador (igual a PPO/A2C)
      ...
  )

COMPARACIÃ“N:
  Learning Rate Actual:   5e-4 = 0.0005 (agresivo)
  Learning Rate Propuesto: 3e-4 = 0.0003 (conservador)
  Cambio: -40% (menos pasos grandes)

IMPACTO: Convergencia mÃ¡s estable
âš ï¸  NO TOCA PESOS: Solo cambia tasa de aprendizaje


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PLAN DE IMPLEMENTACIÃ“N INTEGRADO (20 minutos + Arquitectura)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PASO 0 (2 min): Cambiar arquitectura a [256,256] (unificar con PPO/A2C)
  â””â”€ Editar: policy_kwargs en SACConfig (lÃ­nea ~392)
  â””â”€ Cambio: pi=[512,512] -> pi=[256,256], qf=[512,512] -> qf=[256,256]

Paso 1 (5 min): SoluciÃ³n #1 - Reward Normalization
  â””â”€ Editar: MultiObjectiveReward.__call__()

Paso 2 (5 min): SoluciÃ³n #2 - Buffer & Warmup
  â””â”€ Cambiar: buffer_size, learning_starts, batch_size, train_freq

Paso 3 (5 min): SoluciÃ³n #3 - Tau & Gradient Steps
  â””â”€ Cambiar: tau=0.001, gradient_steps=2

Paso 4 (2 min): SoluciÃ³n #4 - Entropy
  â””â”€ Cambiar: ent_coef=0.01

Paso 5 (1 min): SoluciÃ³n #5 - Learning Rate
  â””â”€ Cambiar: learning_rate=3e-4

TIEMPO TOTAL: 22 minutos de codificaciÃ³n

VALIDACIÃ“N:
  1. Entrenar 1 EPISODIO (8,760 steps)
  2. Verificar en TensorBoard:
     âœ“ Rewards trending UP (positivos)
     âœ“ Loss curves trending DOWN (convergencia)
     âœ“ Q-values suave (no oscilaciones)
  3. Si OK â†’ Entrenar 10 episodios completos
  4. Si falla â†’ Fallback a PPO


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPARACIÃ“N FINAL: SAC v2.0 vs PPO vs A2C (CON MISMA ARQUITECTURA)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                          SAC v2.0        PPO         A2C
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Arquitectura              [256,256]       [256,256]   [256,256] âœ… IGUAL
ImplementaciÃ³n            20 min          0 min       0 min
Actualizar PPO/A2C arch   NO              SÃ (+2 min) NO
Retraining time           4-5 horas       2.7 min     2.9 min
Convergencia esperada     +80-100%        +125.5%     +48.8%
Q-value stability         QuizÃ¡s mejor    Buena       Buena
Complejidad remaining     Media           Baja        Baja
Risk of failure           MEDIO           BAJO        BAJO
ROI (effort vs benefit)   BAJO            ALTO        ALTO

RECOMENDACIÃ“N: ğŸŸ¢ USE PPO FOR PRODUCTION
Pero con SAC v2.0 + arquitectura unificada, tienes comparaciÃ³n justa.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Documento generado: 2026-02-15
Status: âœ… 5 SOLUCIONES + UNIFICACIÃ“N DE PESOS LISTA
Cambio crÃ­tico: Eliminar SoluciÃ³n #5 original (net_arch) â†’ Usar UnificaciÃ³n en Paso 0
"""

print(output)
sys.exit(0)
