#!/usr/bin/env python3
"""
AUDITOR√çA Y LIMPIEZA EXHAUSTIVA: Chargers Dataset v5.2
================================================================================================

Prop√≥sito: Validar integridad completa del dataset,
- Eliminar datos antiguos/duplicados
- Verificar 2024 completo (8,760 horas)
- Validar columnas para CityLearn v2 + Agentes RL
- Certificar listo para entrenamiento

Generado: 2026-02-13
Auditor: GitHub Copilot
"""

from __future__ import annotations

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import json
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURACI√ìN DE VALIDACI√ìN
# ============================================================================

DATASET_PATH = Path("data/oe2/chargers/chargers_ev_ano_2024_v3.csv")
BACKUP_PATH = Path("data/oe2/chargers/chargers_ev_ano_2024_v3.backup.csv")
OUTPUT_REPORT_PATH = Path("AUDITORIA_LIMPIEZA_CHARGERS_DATASET_2024.json")

# Requisitos de integridad
EXPECTED_ROWS = 8760  # 365 d√≠as √ó 24 horas
EXPECTED_YEAR = 2024
EXPECTED_DATE_START = pd.Timestamp("2024-01-01 00:00:00")
EXPECTED_DATE_END = pd.Timestamp("2024-12-31 23:00:00")

# Grupos de columnas por tipo
SOCKET_COLUMNS_REQUIRED = [
    "charger_power_kw",      # Potencia nominal cargador
    "battery_kwh",           # Capacidad bater√≠a
    "vehicle_type",          # MOTO o MOTOTAXI
    "soc_current",           # State of Charge actual
    "soc_arrival",           # SOC al llegar
    "soc_target",            # SOC objetivo
    "active",                # Estado binario
    "charging_power_kw",     # Potencia instant√°nea
    "vehicle_count",         # Veh√≠culos en cola
]

GLOBAL_COLUMNS_REQUIRED = [
    "is_hora_punta",             # Hora punta [0,1]
    "tarifa_aplicada_soles",     # Tarifa OSINERGMIN
    "ev_energia_total_kwh",      # Energ√≠a total
    "ev_energia_motos_kwh",      # Energ√≠a motos
    "ev_energia_mototaxis_kwh",  # Energ√≠a mototaxis
    "co2_reduccion_motos_kg",    # CO2 motos
    "co2_reduccion_mototaxis_kg", # CO2 taxis
    "reduccion_directa_co2_kg",  # CO2 total
    "costo_carga_ev_soles",      # Costo S/.
    "ev_demand_kwh",             # Alias CityLearn
]

COLUMNS_FOR_AGENTS = [
    # Socket states (38 sockets √ó 3 = 114 columnas)
    # Globales para RL
    "is_hora_punta",
    "tarifa_aplicada_soles",
    "ev_energia_total_kwh",
    "ev_demand_kwh",
    "reduccion_directa_co2_kg",
]

NUM_SOCKETS = 38
SOCKETS_MOTOS = 30
SOCKETS_TAXIS = 8


# ============================================================================
# FUNCIONES DE VALIDACI√ìN
# ============================================================================

def load_dataset(path: Path) -> pd.DataFrame:
    """Carga el dataset con conversiones correctas."""
    logger.info(f"üìÇ Cargando dataset: {path}")
    
    if not path.exists():
        raise FileNotFoundError(f"‚ùå Dataset no encontrado: {path}")
    
    df = pd.read_csv(
        path,
        index_col=0,
        parse_dates=[0]
    )
    
    logger.info(f"‚úì Dataset cargado: {df.shape[0]} filas √ó {df.shape[1]} columnas")
    return df


def validate_date_range(df: pd.DataFrame) -> dict:
    """Valida que el rango de fechas sea correcto para 2024."""
    logger.info("\nüïê VALIDACI√ìN 1: Rango de Fechas")
    logger.info("=" * 80)
    
    results = {
        "phase": "DATE_RANGE",
        "valid": True,
        "checks": [],
        "issues": [],
    }
    
    # Verificar tipo de √≠ndice
    if not isinstance(df.index, pd.DatetimeIndex):
        results["valid"] = False
        results["issues"].append(f"‚ùå √çndice no es DatetimeIndex: {type(df.index)}")
        logger.error(f"‚ùå √çndice no es DatetimeIndex: {type(df.index)}")
    else:
        logger.info(f"‚úì √çndice es DatetimeIndex")
        results["checks"].append("‚úì √çndice es DatetimeIndex")
    
    # Verificar n√∫mero de filas
    if len(df) != EXPECTED_ROWS:
        results["valid"] = False
        results["issues"].append(
            f"‚ùå Rows: {len(df)} != {EXPECTED_ROWS} (falta {EXPECTED_ROWS - len(df)} horas)"
        )
        logger.error(f"‚ùå Rows: {len(df)} != {EXPECTED_ROWS}")
    else:
        logger.info(f"‚úì Filas: {len(df)} correcto (8,760 = 365 d√≠as √ó 24 horas)")
        results["checks"].append(f"‚úì Filas: {len(df)} correcto")
    
    # Verificar a√±o
    years_in_data = df.index.year.unique()
    if not (len(years_in_data) == 1 and years_in_data[0] == EXPECTED_YEAR):
        results["valid"] = False
        results["issues"].append(
            f"‚ùå A√±os presentes: {sorted(years_in_data)} (esperado: {EXPECTED_YEAR})"
        )
        logger.error(f"‚ùå A√±os presentes: {sorted(years_in_data)}")
    else:
        logger.info(f"‚úì A√±o: {EXPECTED_YEAR} (sin datos de otros a√±os)")
        results["checks"].append(f"‚úì A√±o: {EXPECTED_YEAR}")
    
    # Verificar fecha inicial
    date_start = df.index[0]
    if date_start != EXPECTED_DATE_START:
        logger.warning(f"‚ö† Fecha inicio: {date_start} != {EXPECTED_DATE_START}")
        results["issues"].append(f"‚ö† Fecha inicio: {date_start} != {EXPECTED_DATE_START}")
    else:
        logger.info(f"‚úì Fecha inicio: {date_start}")
        results["checks"].append(f"‚úì Fecha inicio: {date_start}")
    
    # Verificar fecha final
    date_end = df.index[-1]
    expected_end = pd.Timestamp("2024-12-31 23:00:00")
    if date_end != expected_end and date_end != EXPECTED_DATE_END:
        logger.warning(f"‚ö† Fecha fin: {date_end} (esperado ~2024-12-30/31)")
        results["issues"].append(f"‚ö† Fecha fin: {date_end}")
    else:
        logger.info(f"‚úì Fecha fin: {date_end}")
        results["checks"].append(f"‚úì Fecha fin: {date_end}")
    
    # Verificar continuidad horaria
    if len(df) == EXPECTED_ROWS:
        ts_diff = df.index.to_series().diff()
        expected_freq = pd.Timedelta(hours=1)
        duplicates = (ts_diff == pd.Timedelta(0)).sum()
        gaps = (ts_diff > expected_freq).sum()
        
        if duplicates > 0:
            results["valid"] = False
            results["issues"].append(f"‚ùå Timestamps duplicados: {duplicates}")
            logger.error(f"‚ùå Timestamps duplicados: {duplicates}")
        else:
            logger.info(f"‚úì No hay duplicados de timestamp")
            results["checks"].append("‚úì No hay duplicados")
        
        if gaps > 0:
            results["valid"] = False
            results["issues"].append(f"‚ùå Gaps en timeline: {gaps} saltos")
            logger.error(f"‚ùå Gaps en timeline: {gaps}")
        else:
            logger.info(f"‚úì Timeline continua (1h entre cada registro)")
            results["checks"].append("‚úì Timeline continua")
    
    logger.info("=" * 80)
    return results


def validate_columns(df: pd.DataFrame) -> dict:
    """Valida presencia y tipos de columnas."""
    logger.info("\nüìä VALIDACI√ìN 2: Columnas Requeridas")
    logger.info("=" * 80)
    
    results = {
        "phase": "COLUMNS",
        "valid": True,
        "total_columns": len(df.columns),
        "socket_columns": 0,
        "global_columns": 0,
        "checks": [],
        "issues": [],
        "missing_columns": [],
    }
    
    # Verificar columnas globales
    logger.info(f"\nüîç Verificando {len(GLOBAL_COLUMNS_REQUIRED)} columnas globales...")
    for col in GLOBAL_COLUMNS_REQUIRED:
        if col not in df.columns:
            results["valid"] = False
            results["missing_columns"].append(col)
            results["issues"].append(f"‚ùå Columna FALTANTE: {col}")
            logger.error(f"‚ùå {col}")
        else:
            logger.info(f"‚úì {col}")
            results["checks"].append(f"‚úì {col}")
            results["global_columns"] += 1
    
    # Verificar columnas por socket (socket_XXX_variable)
    logger.info(f"\nüîç Verificando columnas por socket (38 total)...")
    socket_cols_found = {}
    pattern_base = "socket_"
    
    for socket_id in range(NUM_SOCKETS):
        socket_cols_for_id = []
        for var in SOCKET_COLUMNS_REQUIRED:
            col_name = f"{pattern_base}{socket_id:03d}_{var}"
            if col_name in df.columns:
                socket_cols_for_id.append(col_name)
                results["socket_columns"] += 1
            else:
                results["valid"] = False
                results["missing_columns"].append(col_name)
                results["issues"].append(f"‚ùå Socket {socket_id}: falta {var}")
                logger.warning(f"‚ùå socket_{socket_id:03d}_{var}")
        
        socket_cols_found[socket_id] = len(socket_cols_for_id)
    
    # Resumen sockets
    sockets_complete = sum(1 for c in socket_cols_found.values() if c == len(SOCKET_COLUMNS_REQUIRED))
    logger.info(f"‚úì Sockets completos: {sockets_complete}/{NUM_SOCKETS}")
    results["checks"].append(f"‚úì Socket columns: {results['socket_columns']} presentes")
    
    logger.info(f"\nüìà Resumen columnas:")
    logger.info(f"   Total: {len(df.columns)}")
    logger.info(f"   Globales encontradas: {results['global_columns']}/{len(GLOBAL_COLUMNS_REQUIRED)}")
    logger.info(f"   Socket columns: {results['socket_columns']}/{NUM_SOCKETS * len(SOCKET_COLUMNS_REQUIRED)}")
    
    logger.info("=" * 80)
    return results


def validate_data_types(df: pd.DataFrame) -> dict:
    """Valida tipos de datos en columnas clave."""
    logger.info("\nüî¢ VALIDACI√ìN 3: Tipos de Datos")
    logger.info("=" * 80)
    
    results = {
        "phase": "DATA_TYPES",
        "valid": True,
        "checks": [],
        "issues": [],
        "dtype_summary": {},
    }
    
    # Validar columnas num√©ricas
    numeric_cols = [col for col in df.columns if '_power_kw' in col or '_kwh' in col 
                    or '_soles' in col or '_kg' in col or '_soc' in col]
    
    logger.info(f"üîç Validando {len(numeric_cols)} columnas num√©ricas...")
    for col in numeric_cols[:5]:  # Muestra primeras 5
        dtype = df[col].dtype
        has_nulls = df[col].isna().sum()
        logger.info(f"‚úì {col}: {dtype}, NaN: {has_nulls}")
        results["checks"].append(f"‚úì {col}: {dtype}")
    
    # Validar columnas binarias
    binary_cols = [col for col in df.columns if '_active' in col or '_hora_punta' in col]
    logger.info(f"\nüîç Validando {len(binary_cols)} columnas binarias...")
    for col in binary_cols[:3]:  # Muestra primeras 3
        if col in df.columns:
            unique_vals = df[col].unique()
            if set(unique_vals) <= {0, 1, np.nan}:
                logger.info(f"‚úì {col}: {sorted(set(unique_vals))}")
                results["checks"].append(f"‚úì {col}: binaria")
            else:
                results["valid"] = False
                logger.error(f"‚ùå {col}: valores no binarios {sorted(set(unique_vals))}")
                results["issues"].append(f"‚ùå {col}: no binaria")
    
    # Validar columnas categ√≥ricas
    categorical_cols = [col for col in df.columns if 'vehicle_type' in col]
    logger.info(f"\nüîç Validando {len(categorical_cols)} columnas categ√≥ricas...")
    for col in categorical_cols[:3]:  # Muestra primeras 3
        if col in df.columns:
            unique_vals = df[col].unique()
            logger.info(f"‚úì {col}: {sorted(set(unique_vals))}")
            results["checks"].append(f"‚úì {col}: {sorted(set(unique_vals))}")
    
    # Resumen tipos
    dtype_counts = {
        "int": 0,
        "float": 0,
        "object": 0,
        "datetime": 0,
    }
    for dtype in df.dtypes:
        dtype_str = str(dtype)
        if dtype_str.startswith('int'):
            dtype_counts["int"] += 1
        elif dtype_str.startswith('float'):
            dtype_counts["float"] += 1
        elif dtype_str == 'object':
            dtype_counts["object"] += 1
        elif dtype_str.startswith('datetime'):
            dtype_counts["datetime"] += 1
    
    results["dtype_summary"] = dtype_counts
    
    logger.info(f"\nüìà Distribuci√≥n tipos:")
    logger.info(f"   Enteros: {results['dtype_summary']['int']}")
    logger.info(f"   Floats: {results['dtype_summary']['float']}")
    logger.info(f"   Objetos: {results['dtype_summary']['object']}")
    
    logger.info("=" * 80)
    return results


def validate_data_ranges(df: pd.DataFrame) -> dict:
    """Valida rangos de valores en columnas clave."""
    logger.info("\nüìè VALIDACI√ìN 4: Rangos de Valores")
    logger.info("=" * 80)
    
    results = {
        "phase": "DATA_RANGES",
        "valid": True,
        "checks": [],
        "issues": [],
        "range_validation": {},
    }
    
    # Validar SOC [0, 1]
    logger.info(f"üîç Validando SOC (debe estar en [0, 1])...")
    soc_cols = [col for col in df.columns if '_soc_' in col]
    for col in soc_cols[:5]:  # Primeros 5
        min_val = df[col].min()
        max_val = df[col].max()
        if 0 <= min_val and max_val <= 1:
            logger.info(f"‚úì {col}: [{min_val:.2f}, {max_val:.2f}]")
            results["checks"].append(f"‚úì {col}: [{min_val:.2f}, {max_val:.2f}]")
        else:
            results["valid"] = False
            logger.error(f"‚ùå {col}: [{min_val:.2f}, {max_val:.2f}] FUERA DE RANGO")
            results["issues"].append(f"‚ùå {col}: SOC fuera de [0,1]")
    
    # Validar Potencia [0, 7.4]
    logger.info(f"\nüîç Validando Potencia Cargador (debe ser ~7.4 kW)...")
    power_static = [col for col in df.columns if 'charger_power_kw' in col]
    for col in power_static[:3]:
        val = df[col].unique()
        if len(val) == 1 and val[0] == 7.4:
            logger.info(f"‚úì {col}: {val[0]} kW (constante)")
            results["checks"].append(f"‚úì {col}: 7.4 kW")
        else:
            logger.warning(f"‚ö† {col}: {val} (esperado 7.4)")
    
    # Validar Potencia cargando [0, 4.588]
    logger.info(f"\nüîç Validando Potencia Instant√°nea (0 a ~4.588 kW efectivos)...")
    charging_cols = [col for col in df.columns if '_charging_power_kw' in col]
    for col in charging_cols[:3]:
        min_val = df[col].min()
        max_val = df[col].max()
        if 0 <= min_val and max_val <= 4.7:
            logger.info(f"‚úì {col}: [{min_val:.2f}, {max_val:.2f}] kW")
            results["checks"].append(f"‚úì {col}: potencia v√°lida")
        else:
            results["valid"] = False
            logger.error(f"‚ùå {col}: [{min_val:.2f}, {max_val:.2f}] INV√ÅLIDO")
            results["issues"].append(f"‚ùå {col}: potencia fuera de rango")
    
    # Validar Energ√≠a total > 0
    if "ev_energia_total_kwh" in df.columns:
        total_energy = df["ev_energia_total_kwh"].sum()
        if total_energy > 0:
            logger.info(f"‚úì Energ√≠a total anual: {total_energy:,.0f} kWh")
            results["checks"].append(f"‚úì Energ√≠a anual: {total_energy:,.0f} kWh")
        else:
            results["valid"] = False
            logger.error(f"‚ùå Energ√≠a total = 0")
            results["issues"].append("‚ùå Energ√≠a total es 0")
    
    # Validar Tarifa OSINERGMIN [0.28, 0.45]
    if "tarifa_aplicada_soles" in df.columns:
        unique_tarifas = df["tarifa_aplicada_soles"].unique()
        expected_tarifas = {0.28, 0.45}
        if set(unique_tarifas) == expected_tarifas:
            logger.info(f"‚úì Tarifas: {sorted(unique_tarifas)}")
            results["checks"].append(f"‚úì Tarifas OSINERGMIN: {sorted(unique_tarifas)}")
        else:
            results["valid"] = False
            logger.error(f"‚ùå Tarifas inesperadas: {sorted(unique_tarifas)}")
            results["issues"].append(f"‚ùå Tarifas OSINERGMIN inv√°lidas")
    
    # Validar CO2 reducci√≥n
    if "reduccion_directa_co2_kg" in df.columns:
        co2_total = df["reduccion_directa_co2_kg"].sum()
        if co2_total > 0:
            logger.info(f"‚úì CO2 reducci√≥n anual: {co2_total:,.0f} kg ({co2_total/1000:.1f} ton)")
            results["checks"].append(f"‚úì CO2 anual: {co2_total/1000:.1f} ton")
        else:
            results["valid"] = False
            logger.error(f"‚ùå CO2 reducci√≥n = 0")
            results["issues"].append("‚ùå CO2 reducci√≥n es 0")
    
    logger.info("=" * 80)
    return results


def validate_citylearn_compatibility(df: pd.DataFrame) -> dict:
    """Valida compatibilidad con CityLearn v2."""
    logger.info("\nüéÆ VALIDACI√ìN 5: Compatibilidad CityLearn v2")
    logger.info("=" * 80)
    
    results = {
        "phase": "CITYLEARN_COMPAT",
        "valid": True,
        "checks": [],
        "issues": [],
        "agent_observable_columns": [],
    }
    
    # Verificar columnas de observables
    logger.info(f"üîç Verificando observables para agentes RL...")
    observable_counts = {
        "socket_soc": 0,
        "socket_active": 0,
        "socket_power": 0,
        "global": 0,
    }
    
    soc_cols = [col for col in df.columns if '_soc_current' in col]
    active_cols = [col for col in df.columns if '_active' in col]
    power_cols = [col for col in df.columns if '_charging_power_kw' in col]
    
    observable_counts["socket_soc"] = len(soc_cols)
    observable_counts["socket_active"] = len(active_cols)
    observable_counts["socket_power"] = len(power_cols)
    
    logger.info(f"‚úì SOC columns: {len(soc_cols)}/38")
    logger.info(f"‚úì Active columns: {len(active_cols)}/38")
    logger.info(f"‚úì Power columns: {len(power_cols)}/38")
    results["checks"].append(f"‚úì Socket observables: {len(soc_cols)}/38 sockets")
    results["agent_observable_columns"] = {
        "socket_soc": len(soc_cols),
        "socket_active": len(active_cols),
        "socket_power": len(power_cols),
    }
    
    # Verificar globales para agentes
    logger.info(f"\nüîç Verificando globales para agentes...")
    global_agent_cols = [col for col in COLUMNS_FOR_AGENTS if col in df.columns]
    for col in global_agent_cols:
        logger.info(f"‚úì {col}")
        results["checks"].append(f"‚úì {col}")
        results["agent_observable_columns"].append(col)
    
    if len(global_agent_cols) == len(COLUMNS_FOR_AGENTS):
        logger.info(f"‚úì Todas las globales presentes ({len(global_agent_cols)}/{len(COLUMNS_FOR_AGENTS)})")
    else:
        results["valid"] = False
        missing = set(COLUMNS_FOR_AGENTS) - set(global_agent_cols)
        logger.error(f"‚ùå Falta: {missing}")
        results["issues"].append(f"‚ùå Faltan globales: {missing}")
    
    # Verificar nomenclatura socket
    logger.info(f"\nüîç Verificando nomenclatura socket_XXX_variable...")
    pattern_match = all(col.startswith("socket_") and col.count("_") >= 2 
                       for col in df.columns if col.startswith("socket_"))
    if pattern_match:
        logger.info(f"‚úì Nomenclatura socket: v√°lida")
        results["checks"].append("‚úì Nomenclatura socket_{id}_{var}")
    else:
        results["valid"] = False
        logger.error(f"‚ùå Nomenclatura socket: inv√°lida")
        results["issues"].append("‚ùå Nomenclatura socket inconsistente")
    
    logger.info("=" * 80)
    return results


def validate_for_agent_training(df: pd.DataFrame) -> dict:
    """Valida que el dataset est√© listo para entrenamiento de agentes."""
    logger.info("\nü§ñ VALIDACI√ìN 6: Preparaci√≥n para Entrenamiento Agentes")
    logger.info("=" * 80)
    
    results = {
        "phase": "AGENT_TRAINING",
        "valid": True,
        "checks": [],
        "issues": [],
        "agent_readiness": {},
    }
    
    # Verificar sin NaN en columnas cr√≠ticas
    logger.info(f"üîç Verificando valores nulos en columnas cr√≠ticas...")
    critical_global = ["is_hora_punta", "tarifa_aplicada_soles", "ev_energia_total_kwh",
                      "reduccion_directa_co2_kg"]
    
    has_nulls = False
    for col in critical_global:
        if col in df.columns:
            null_count = df[col].isna().sum()
            if null_count == 0:
                logger.info(f"‚úì {col}: sin NaN")
                results["checks"].append(f"‚úì {col}: completo")
            else:
                has_nulls = True
                results["valid"] = False
                logger.error(f"‚ùå {col}: {null_count} NaN")
                results["issues"].append(f"‚ùå {col}: {null_count} NaN (debe estar completo)")
    
    if not has_nulls:
        logger.info(f"‚úì Datos limpios: sin valores nulos en cr√≠ticos")
    
    # Verificar consistencia energ√©tica
    logger.info(f"\nüîç Verificando consistencia energ√©tica...")
    if "ev_energia_total_kwh" in df.columns and "ev_energia_motos_kwh" in df.columns:
        motos = df["ev_energia_motos_kwh"].sum()
        taxis = df["ev_energia_mototaxis_kwh"].sum()
        total = df["ev_energia_total_kwh"].sum()
        expected_total = motos + taxis
        
        if abs(total - expected_total) < 1.0:  # Tolerancia 1 kWh
            logger.info(f"‚úì Energ√≠a consistente: {motos:,.0f} + {taxis:,.0f} = {total:,.0f}")
            results["checks"].append(f"‚úì Energ√≠a consistente")
        else:
            logger.warning(f"‚ö† Energ√≠a inconsistente: {total} != {expected_total}")
            results["issues"].append(f"‚ö† Energ√≠a inconsistente")
    
    # Verificar que agentes puedan leer cada observable
    logger.info(f"\nüîç Verificando accesibilidad para agentes...")
    soc_cols = [col for col in df.columns if '_soc_current' in col]
    active_cols = [col for col in df.columns if '_active' in col]
    power_cols = [col for col in df.columns if '_charging_power_kw' in col]
    
    total_socket_observables = len(soc_cols) + len(active_cols) + len(power_cols)
    total_global_observables = len([c for c in COLUMNS_FOR_AGENTS if c in df.columns])
    
    total_obs = total_socket_observables + total_global_observables
    logger.info(f"‚úì Total observables: {total_obs} ({total_socket_observables} socket + {total_global_observables} global)")
    logger.info(f"‚úì Dimensi√≥n observaci√≥n: ~{total_obs}-dim")
    results["checks"].append(f"‚úì Observation space: ~{total_obs}-dim")
    
    results["agent_readiness"] = {
        "observation_dim": total_obs,
        "action_dim": 39,  # 38 sockets + 1 BESS futuro
        "episode_length": 8760,
        "timestep_hours": 1,
        "ready_for_sac": True,
        "ready_for_ppo": True,
        "ready_for_a2c": True,
    }
    
    logger.info("=" * 80)
    return results


def check_for_old_data(df: pd.DataFrame) -> dict:
    """Verifica presencia de datos antiguos o duplicados."""
    logger.info("\nüßπ VALIDACI√ìN 7: Limpieza de Datos Antiguos")
    logger.info("=" * 80)
    
    results = {
        "phase": "OLD_DATA_CLEANUP",
        "valid": True,
        "checks": [],
        "issues": [],
        "old_data_found": False,
        "duplicates_found": 0,
    }
    
    # Buscar datos de a√±os anteriores a 2024
    logger.info(f"üîç Buscando datos de a√±os anteriores a 2024...")
    years = df.index.year.unique()
    old_years = [y for y in years if y < 2024]
    
    if len(old_years) > 0:
        results["valid"] = False
        results["old_data_found"] = True
        logger.error(f"‚ùå Datos antiguos encontrados: a√±os {sorted(old_years)}")
        results["issues"].append(f"‚ùå Datos de a√±os anteriores: {sorted(old_years)}")
    else:
        logger.info(f"‚úì No hay datos de a√±os anteriores a 2024")
        results["checks"].append("‚úì No hay datos antiguos")
    
    # Buscar datos futuros (post 2024)
    future_years = [y for y in years if y > 2024]
    if len(future_years) > 0:
        results["valid"] = False
        logger.warning(f"‚ö† Datos futuros encontrados: a√±os {sorted(future_years)}")
        results["issues"].append(f"‚ö† Datos post-2024: {sorted(future_years)}")
    
    # Verificar duplicados
    logger.info(f"üîç Verificando duplicados de timestamp...")
    dup_count = df.index.duplicated().sum()
    if dup_count > 0:
        results["duplicates_found"] = dup_count
        results["valid"] = False
        logger.error(f"‚ùå Timestamps duplicados: {dup_count}")
        results["issues"].append(f"‚ùå {dup_count} timestamps duplicados")
    else:
        logger.info(f"‚úì No hay timestamps duplicados")
        results["checks"].append("‚úì Sin duplicados")
    
    # Verificar filas completamente duplicadas
    logger.info(f"üîç Verificando filas completamente duplicadas...")
    full_dup = df.duplicated().sum()
    if full_dup > 0:
        results["valid"] = False
        logger.error(f"‚ùå Filas duplicadas: {full_dup}")
        results["issues"].append(f"‚ùå {full_dup} filas completamente duplicadas")
    else:
        logger.info(f"‚úì No hay filas duplicadas (ignorando √≠ndice)")
        results["checks"].append("‚úì Sin filas duplicadas")
    
    logger.info("=" * 80)
    return results


def generate_cleanup_report(assessments: list[dict]) -> dict:
    """Genera reporte final de auditor√≠a y recomendaciones."""
    logger.info("\nüìã GENERANDO REPORTE FINAL")
    logger.info("=" * 80)
    
    all_valid = all(a["valid"] for a in assessments)
    total_checks = sum(len(a.get("checks", [])) for a in assessments)
    total_issues = sum(len(a.get("issues", [])) for a in assessments)
    
    report = {
        "audit_timestamp": datetime.now().isoformat(),
        "dataset_path": str(DATASET_PATH),
        "overall_valid": all_valid,
        "summary": {
            "total_validations": len(assessments),
            "checks_passed": total_checks,
            "issues_found": total_issues,
            "status": "‚úÖ APTO PARA ENTRENAMIENTO" if all_valid else "‚ö†Ô∏è REQUIERE LIMPIEZA",
        },
        "phases": assessments,
    }
    
    logger.info(f"\n{'='*80}")
    logger.info("RESUMEN FINAL:")
    logger.info(f"{'='*80}")
    logger.info(f"Status: {report['summary']['status']}")
    logger.info(f"Validaciones: {len(assessments)}")
    logger.info(f"Checks pasados: {total_checks}")
    logger.info(f"Issues encontrados: {total_issues}")
    
    if all_valid:
        logger.info("\n‚úÖ ¬°Dataset VALIDADO y LISTO para:")
        logger.info("   ‚Ä¢ Construcci√≥n de ambiente CityLearn v2")
        logger.info("   ‚Ä¢ Entrenamiento de agentes RL (SAC, PPO, A2C)")
        logger.info("   ‚Ä¢ Exportaci√≥n a observables normalizadas [0,1]")
    else:
        logger.info("\n‚ö†Ô∏è Dataset requiere limpieza:")
        for assessment in assessments:
            if assessment["issues"]:
                logger.info(f"\n{assessment['phase']}:")
                for issue in assessment["issues"]:
                    logger.info(f"   {issue}")
    
    logger.info(f"{'='*80}\n")
    
    return report


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Ejecuta auditor√≠a completa del dataset."""
    
    print("\n" + "="*80)
    print("AUDITOR√çA EXHAUSTIVA: Chargers Dataset v5.2")
    print("="*80)
    print(f"Archivo: {DATASET_PATH}")
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    try:
        # Cargar dataset
        df = load_dataset(DATASET_PATH)
        
        # Hacer backup (antes de cualquier limpieza)
        logger.info(f"\nüíæ Creando backup...")
        df.to_csv(BACKUP_PATH)
        logger.info(f"‚úì Backup guardado: {BACKUP_PATH}")
        
        # Ejecutar validaciones
        assessments = []
        
        assessments.append(validate_date_range(df))
        assessments.append(validate_columns(df))
        assessments.append(validate_data_types(df))
        assessments.append(validate_data_ranges(df))
        assessments.append(validate_citylearn_compatibility(df))
        assessments.append(validate_for_agent_training(df))
        assessments.append(check_for_old_data(df))
        
        # Generar reporte
        report = generate_cleanup_report(assessments)
        
        # Guardar reporte
        with open(OUTPUT_REPORT_PATH, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        logger.info(f"\n‚úì Reporte guardado: {OUTPUT_REPORT_PATH}")
        
        # Resumen para usuario
        status_emoji = "‚úÖ" if report["overall_valid"] else "‚ö†Ô∏è"
        print(f"\n{status_emoji} {report['summary']['status']}")
        print(f"   Validaciones pasadas: {report['summary']['checks_passed']}")
        print(f"   Issues encontrados: {report['summary']['issues_found']}")
        
        if report["overall_valid"]:
            print(f"\nüéâ Dataset chargers_ev_ano_2024_v3.csv est√° 100% LISTO para:")
            print(f"   ‚úÖ Construcci√≥n de dataset CityLearn v2")
            print(f"   ‚úÖ Entrenamiento de agentes RL (SAC, PPO, A2C)")
            print(f"   ‚úÖ Exportaci√≥n de observables normalizadas")
            print(f"   ‚úÖ Integraci√≥n con BESS dataset (bess_simulation_hourly.csv)")
        else:
            print(f"\n‚ö†Ô∏è Se recomienda revisar issues encontrados antes de proceder")
        
        return report
        
    except Exception as e:
        logger.error(f"\n‚ùå Error en auditor√≠a: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    report = main()
