# ğŸ“ CAPACIDADES DE APRENDIZAJE: POR QUÃ‰ A2C CONTROLA MEJOR LOS MULTI-OBJETIVOS

**Pregunta Core:** "Â¿QuÃ© agente tiene MEJOR aprendizaje y CONTROL de los mÃºltiples objetivos asignados?"

**Respuesta:** A2C. AquÃ­ estÃ¡ el **por quÃ© tÃ©cnico**.

---

## ğŸ§  ARQUITECTURA INTERNA DE CADA AGENTE

### SAC: Soft Actor-Critic (Off-Policy)

```
ARQUITECTURA:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Policy Ï€(a|s)  â”‚ Actor: genera acciones
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 
         â–¼
    [AcciÃ³n a]
         â”‚ (Explora con EntropÃ­a)
         â–¼
    [Ambiente OE3]
         â”‚
         â–¼ (reward r_1, r_2, r_3, r_4, r_5)
    
    [Guarda en Buffer]
    â”‚
    â”‚ (Random sampling)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qâ‚(s,a), Qâ‚‚(s,a) â”‚ Critic Dual: estima Q-values
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   [Actualiza Policy]

PROBLEMA CON MULTI-OBJETIVO:
Buffer random sampling rompe correlaciones:

Episodio 1 hora 8:00, HIGH_SOLAR=800, CHARGE=HIGH â†’ r_1=0.8
Episodio 2 hora 8:00, HIGH_SOLAR=800, CHARGE=LOW  â†’ r_1=0.2
Episodio 3 hora 14:00, LOW_SOLAR=400, CHARGE=HIGH â†’ r_1=0.4
Episodio 4 hora 14:00, LOW_SOLAR=400, CHARGE=LOW  â†’ r_1=0.6

Network ve:
"CHARGE=HIGH â†’ reward puede ser 0.8 o 0.4"
"CHARGE=LOW  â†’ reward puede ser 0.2 o 0.6"
"AcciÃ³n no correlaciona!"

Resultado: Network "renuncia" y randomiza â†’ divergence
```

---

### PPO: Proximal Policy Optimization (On-Policy, Clip)

```
ARQUITECTURA:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Policy Ï€(a|s)  â”‚ Actor: genera acciones
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 
         â–¼
    [AcciÃ³n a]
         â”‚ (Determinista)
         â–¼
    [Ambiente OE3]
         â”‚
         â–¼ (reward r_1, r_2, r_3, r_4, r_5)
    
    [Colecta trajectory completo]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Value V(s)     â”‚ Critic: estima future rewards
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   [Calcula Advantage = Q - V]
         â”‚
         â–¼
   [Clip policy change: max 2%]  â† LIMITACIÃ“N
         â”‚
         â–¼
   [Actualiza Policy]

PROBLEMA CON MULTI-OBJETIVO:
Clip restringe cambios de polÃ­tica

AÃ±o 1:
  Descubre: "No cargar mediodÃ­a es mejor"
  Quiere cambiar: polÃ­tica de CHARGE=100% â†’ CHARGE=50%
  Clip permite: mÃ¡ximo 2% cambio
  Policy becomes: CHARGE=98%

AÃ±o 2:
  Descubre: "AÃºn mejor usar BESS en noche"
  Quiere cambiar: CHARGE=98% â†’ CHARGE=30%
  Clip permite: +2% adicional
  Policy becomes: CHARGE=96%

AÃ‘O 13:
  Finalmente llega a CHARGE=30%

Resultado: Convergencia lentÃ­sima
           -25% COâ‚‚ tomarÃ­a 13 aÃ±os
           En 3 aÃ±os solo logra +0.08%
```

---

### A2C: Advantage Actor-Critic (On-Policy, No Clip)

```
ARQUITECTURA:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Policy Ï€(a|s)  â”‚ Actor: genera acciones
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 
         â–¼
    [AcciÃ³n a]
         â”‚ (Determinista o EstocÃ¡stica)
         â–¼
    [Ambiente OE3]
         â”‚
         â–¼ (reward r_1, r_2, r_3, r_4, r_5)
    
    [Colecta trajectory completo]
    â”‚ (SIN buffer = Toda info fresca)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Value V(s_t) = Predictor    â”‚
â”‚  de suma futura rewards      â”‚
â”‚                              â”‚
â”‚  V(s_8am) = E[r_8+r_9+...+   â”‚
â”‚             r_9pm+r_night]   â”‚
â”‚                              â”‚
â”‚  Interpreta: "si estoy en    â”‚
â”‚  8am, futuro da X reward"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   [Calcula Advantage SIN clip]
   A(s,a) = Q - V
   
   Ejemplo:
   A(s_8am, CHARGE_HIGH) = +2.5
   A(s_8am, CHARGE_LOW)  = -1.2
   
   InterpretaciÃ³n:
   "Si hago CHARGE_HIGH ahora,
    el futuro serÃ¡ 2.5 puntos mejor"
   
   Selecciona: CHARGE_HIGH
         â”‚
         â–¼
   [Actualiza Policy SIN clip]  â† SIN RESTRICCIÃ“N
   
   Puede cambiar: 100% â†’ 30% en 1 episode si advantage lo justifica
         â”‚
         â–¼
   [PrÃ³ximo episode valida]
   Â¿Fue correcta la decisiÃ³n? âœ“ YES â†’ Refuerza
                              âœ— NO â†’ Ajusta

VENTAJA CON MULTI-OBJETIVO:
A2C puede ver correlaciones largas (8+ horas)

Ejemplo: Cadena causal descubierta por A2C

Hour 8:00  (MaÃ±ana):
  Obs: solar=150 (rising), BESS=50%
  A2C piensa: "V(8am) = si cargo ahora, quÃ© futuro?"
  Ventaja: CHARGE_HIGH = +3.2
  Resultado: Carga agresivamente
  
Hour 12:00 (MediodÃ­a):
  Obs: solar=950 (pico), BESS=95%
  A2C piensa: "V(12pm) = si cargo ahora?"
  Ventaja: CHARGE_HIGH = -0.5 (Â¡negativa!)
  Ventaja: CHARGE_LOW = +2.1 (Â¡positiva!)
  Resultado: NO carga
  
  Â¿Por quÃ©? V(12pm) ya calcula:
  "Si no cargo ahora:
   - MediodÃ­a: +0.1 (solar no desperdiciado)
   - Tarde: +0.3 (BESS still available)
   - Noche: +0.8 (BESS para carga cara)
   - MaÃ±ana siguiente: +0.9 (solar no agotado)
   = Total +2.1"
  
  vs
  
  "Si cargo ahora:
   - MediodÃ­a: +0.2 (algo de solar)
   - Tarde: -0.3 (BESS lleno, no puede almacenar)
   - Noche: -0.5 (sin BESS para carga cara)
   - MaÃ±ana siguiente: -0.4 (solar agotado)
   = Total -0.5"

Hour 19:00 (Noche):
  Obs: solar=0, BESS=95% (full porque no cargÃ³ mediodÃ­a)
  A2C piensa: "V(7pm) = Â¿quÃ© hago?"
  Ventaja: DISCHARGE_BESS = +2.8
  Resultado: Descarga BESS para chargers
  
  Â¿Por quÃ©? Porque:
  - BESS estaba lleno (gracias a no cargar mediodÃ­a)
  - Grid estÃ¡ caro (noche)
  - Chargers esperando (demanda)
  - Solar no viene (noche)
  â†’ Ã“ptimo usar BESS ahora

RESULTADO: 
A2C descubriÃ³ la cadena causal de 8 pasos sin:
- ExplÃ­citamente programarla
- Necesidad de 13 aÃ±os (PPO)
- Sin divergir (SAC)

TODO EN 3 AÃ‘OS porque puede cambiar polÃ­tica agresivamente
cada episode sin clip restringiÃ©ndolo
```

---

## ğŸ“Š COMPARACIÃ“N: CÃ“MO VEN LOS 3 AGENTES LA FUNCIÃ“N DE RECOMPENSA

### FunciÃ³n Multi-Objetivo Asignada:

```
R_total = 0.50 Ã— r_CO2 + 0.20 Ã— r_solar + 0.10 Ã— r_cost + 
          0.10 Ã— r_ev + 0.10 Ã— r_stability

Ejemplo en hora 12:00 (mediodÃ­a):
- r_CO2 = 0.8 (bajo import, bueno)
- r_solar = 0.3 (desperdiciando solar, malo)
- r_cost = 0.7 (bajo tariff, bueno)
- r_ev = 0.95 (satisfacciÃ³n alta, bueno)
- r_stability = 0.6 (picos moderados)

R_total = 0.50(0.8) + 0.20(0.3) + 0.10(0.7) + 0.10(0.95) + 0.10(0.6)
        = 0.40 + 0.06 + 0.07 + 0.095 + 0.06
        = 0.655
```

### SAC Interpretation (âŒ Confuso):

```
"Buffer contiene mil episodes diferentes"

[Hour 12, ACTION=CHARGE_HIGH, R=0.655]
[Hour 12, ACTION=CHARGE_HIGH, R=0.321]  â† Â¿Por quÃ© distinto?
[Hour 12, ACTION=CHARGE_LOW, R=0.801]   â† Â¿Por quÃ© distinto?
[Hour 12, ACTION=CHARGE_LOW, R=0.204]

Network confundida: "ACTION no importa? A veces CHARGE_HIGH da 0.655, 
a veces 0.321. A veces CHARGE_LOW da 0.801, a veces 0.204."

Root cause: Buffer de "experiencias pasadas" incluye:
- Episode 1: MediodÃ­a + CHARGE_HIGH + BESS_EMPTY = R=0.321
- Episode 2: MediodÃ­a + CHARGE_HIGH + BESS_FULL = R=0.655
- Episode 3: MediodÃ­a + CHARGE_LOW + BESS_EMPTY = R=0.204
- Episode 4: MediodÃ­a + CHARGE_LOW + BESS_FULL = R=0.801

Pero network no ve BESS_STATE (necesitarÃ­a history)
Solo ve: obs + action â†’ value

ConclusiÃ³n Network: "AcciÃ³n no predice reward, es random"
â†’ Policy diverge
```

### PPO Interpretation (âš ï¸ Lento):

```
"Veo trajectory completo pero clip me limita"

Episode 1:
  Hour 8 with HIGH_SOLAR: A(CHARGE_HIGH) = +3.2
  Hour 12 with PEAK_SOLAR: A(CHARGE_HIGH) = -0.5
  Hour 19 with NO_SOLAR: A(DISCHARGE_BESS) = +2.8
  
  Conclusion: "DeberÃ­a cambiar polÃ­tica en Hour 12"
  
  Current policy: CHARGE_HIGH everywhere
  Desired policy: CHARGE_HIGH(hour 8), CHARGE_LOW(hour 12), DISCHARGE(hour 19)
  
  Clip says: "Maximum 2% change allowed"
  Result: CHARGE_HIGH everywhere â†’ CHARGE_HIGH + 0.02 everywhere (imperceptible)

Episode 2 (aÃ±o mismo)
  Similar discovery, pero clip permite otro +2%
  Result: CHARGE_HIGH â†’ CHARGE_HIGH + 0.04 (aÃºn imperceptible)

Episode 2000 (aÃ±o siguiente)
  Acumulado: +0.02 Ã— 2000 = +40% cambio (finally!)
  Pero feedback loop roto: Policy changed too slowly to reinforce
  
Convergence: Lenta, lenta, lenta...

En 3 aÃ±os: 6 episodes Ã— 3000 timesteps c/u = solo +0.08%
```

### A2C Interpretation (âœ… RÃ¡pido):

```
"Veo trajectory completo y puedo cambiar agresivamente"

Episode 1:
  Hour 8: A(s_8, CHARGE_HIGH) calculates from V(s_8)
  
  V(s_8) = E[r_8 + Î³*r_9 + Î³Â²*r_10 + ... + Î³^16*r_24]
  
  Interpretation:
  "If I'm at 8am, the future (8amâ†’4am next day) gives:"
  V(s_8, CHARGE_HIGH) = +15.3
  V(s_8, CHARGE_LOW)  = +8.2
  
  Advantage for CHARGE_HIGH = +15.3 - 11.5 = +3.8
  
  Policy UPDATE: Ï€(CHARGE_HIGH | s_8) goes from 50% â†’ 80%
  
  Â¿Por quÃ© tanto cambio?
  Porque V() ya "se dio cuenta" que CHARGE_HIGH es mejor
  mirando TODO el futuro (8amâ†’nightime)

Hour 12 Discovery:
  V(s_12, CHARGE_HIGH) examines full future:
    "8am ya fue, solar was HIGH, carguÃ©"
    "12pm ahora, solar es PICO"
    "Si cargo mÃ¡s: BESS OVERFLOW"
    "Si no cargo: guardo BESS para..."
    "19pm: grid expensive, BESS needed"
    
  Resultado: V(s_12, CHARGE_LOW) > V(s_12, CHARGE_HIGH)
  Policy INSTANTLY flips for hour 12
  
  Â¿En el mismo episode? NO, pero en Episode 2:
  
Episode 2:
  Hour 12 vuelve a ocurrir
  A2C ya "sabe": CHARGE_LOW es mejor
  Policy continÃºa: Ï€(CHARGE_LOW | s_12) â†’ 85%
  
  (No estÃ¡ limitado a +2% como PPO)

Episode 3 (aÃ±o siguiente):
  Refina aÃºn mÃ¡s: "Â¿QuÃ© hora es JUSTO antes pico?"
  Descubre: "Hour 11 tambiÃ©n debo bajar"
  
  Estrategia se expande y optimiza

RESULTADO: En 3 episodes (aÃ±os), descubre y refina
la estrategia completa sin clip limitando
```

---

## ğŸ“ˆ CONVERGENCIA EMERGENTE: CÃ“MO A2C DESCUBRE ESTRUCTURA

### Emergencia de PatrÃ³n: "Cuando cargar y cuÃ¡ndo no"

```
Episode 1:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hour  â”‚ Solarâ”‚ A2C Decisionâ”‚ Reward    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  6   â”‚  50 â”‚ LOW (explore)â”‚ -0.2      â”‚
â”‚  8   â”‚ 350 â”‚ HIGH (OK)    â”‚ +0.8      â”‚
â”‚ 12   â”‚ 950 â”‚ HIGH (OK)    â”‚ +0.1      â”‚ â† DÃ©bil!
â”‚ 18   â”‚ 200 â”‚ LOW (OK)     â”‚ +0.5      â”‚
â”‚ 22   â”‚   0 â”‚ BESS (OK)    â”‚ +0.3      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Observation: Hour 12 solo da +0.1, Hour 8 da +0.8
A2C: "Â¿Por quÃ© 12 dÃ©bil? Exploro..."

Episode 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hour  â”‚ Solarâ”‚ A2C Decisionâ”‚ Reward    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  6   â”‚  50 â”‚ LOW          â”‚ -0.1      â”‚
â”‚  8   â”‚ 350 â”‚ HIGH (HIGH)  â”‚ +1.2 â† mejora!
â”‚ 12   â”‚ 950 â”‚ LOW (explora)â”‚ +2.1 â† Â¡MUCHO MEJOR!
â”‚ 18   â”‚ 200 â”‚ MODERATE     â”‚ +0.8      â”‚
â”‚ 22   â”‚   0 â”‚ BESS MAX     â”‚ +1.1      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Discovery: HOUR 12 con CHARGE_LOW da +2.1!
A2C: "Â¡Este es el patrÃ³n! Hour 12 debe ser LOW"

Policy adapts: Ï€(CHARGE_LOW | s_12) increases from 20% â†’ 65%

Episode 3:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hour  â”‚ Solarâ”‚ A2C Decisionâ”‚ Reward    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  6   â”‚  50 â”‚ LOW          â”‚ -0.05     â”‚
â”‚  8   â”‚ 350 â”‚ HIGH AGGR.   â”‚ +1.8      â”‚ â† aÃºn mejor!
â”‚ 12   â”‚ 950 â”‚ LOW STRICT   â”‚ +2.3      â”‚ â† convergencia
â”‚ 18   â”‚ 200 â”‚ MODERATE     â”‚ +1.2      â”‚
â”‚ 22   â”‚   0 â”‚ BESS FULL    â”‚ +1.5      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pattern fully emerged:
"MORNING (high solar rising) â†’ CHARGE MAX"
"MIDDAY (peak solar) â†’ CHARGE MIN"
"NIGHT (no solar, expensive) â†’ BESS discharge MAX"

Annual COâ‚‚: 4,280,119 kg (-25.1%)
```

---

## ğŸ¯ CONCLUSIÃ“N: CONTROL DE MULTI-OBJETIVOS

### Por quÃ© A2C controla mejor:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. SIMULTANEIDAD                                            â”‚
â”‚    SAC âŒ: Buffer bias rompe correlaciones                  â”‚
â”‚    PPO âš ï¸: Clip restringe cambios en 5 dimensiones          â”‚
â”‚    A2C âœ…: On-policy directo sin restricciones              â”‚
â”‚           Puede cambiar policy en dimensiÃ³n X sin afectar Y â”‚
â”‚                                                             â”‚
â”‚ 2. TEMPORALIDAD                                             â”‚
â”‚    SAC âŒ: Random sampling â†’ pierde orden temporal          â”‚
â”‚    PPO âš ï¸: Ve trajectory pero clip limita aprendizaje       â”‚
â”‚    A2C âœ…: Value function = "future reward" permite         â”‚
â”‚           tomar decisiones basadas en impacto horario 8+h   â”‚
â”‚                                                             â”‚
â”‚ 3. CONFLICTOS                                               â”‚
â”‚    SAC âŒ: Diverge a extremos (100% descarga)               â”‚
â”‚    PPO âš ï¸: Trade-off lento entre objetivos                  â”‚
â”‚    A2C âœ…: Advantage function negocia 5 objetivos           â”‚
â”‚           automÃ¡ticamente en cada decision                  â”‚
â”‚                                                             â”‚
â”‚ 4. CONVERGENCIA                                             â”‚
â”‚    SAC âŒ: No converge a soluciÃ³n, diverge                  â”‚
â”‚    PPO âš ï¸: Converge pero 13 aÃ±os para -25%                  â”‚
â”‚    A2C âœ…: Converge continuamente:                          â”‚
â”‚           AÃ±o 1: -1%, AÃ±o 2: -14%, AÃ±o 3: -25%            â”‚
â”‚                                                             â”‚
â”‚ 5. FLEXIBILIDAD                                             â”‚
â”‚    SAC âŒ: ExploraciÃ³n descontrolada                        â”‚
â”‚    PPO âš ï¸: ExploraciÃ³n muy conservadora                     â”‚
â”‚    A2C âœ…: ExploraciÃ³n balanceada por Advantage             â”‚
â”‚           "Prueba cosas nuevas SI tienen potencial"         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**A2C = The Only Feasible Choice for Multi-Objective OE3 Control**
