# ğŸ¯ REFERENCIA RÃPIDA - Arquitectura del Proyecto

**Documento de referencia:** Para entender rÃ¡pidamente cÃ³mo funciona el proyecto  
**Fecha:** 27 enero 2026  
**Status:** âœ… FINAL

---

## ğŸ“Š FLUJO VISUAL DE DATOS

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        PROYECTO IQUITOS EV + PV/BESS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OE2: DIMENSIONAMIENTO DE INFRAESTRUCTURA                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  SOLAR PV              BESS                    CHARGERS                      â”‚
â”‚  4,050 kWp        4,520 kWh/2,712 kW         128 cargadores                 â”‚
â”‚  Kyocera KS20      LiFePOâ‚„ (OE2 Real)        â”œâ”€ 112 motos @2kW             â”‚
â”‚  6,472 strings     (night buffer)             â””â”€ 16 taxis @3kW              â”‚
â”‚  200,632 modules   (peak shaving)                512 sockets                 â”‚
â”‚                                                                              â”‚
â”‚  Eaton Xpert1670 Inverter (2 units) â†’ 4,050 kW AC                          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                        DATOS OE2
                 (solar, demand, charger specs)
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATASET BUILDER: Convertir OE2 â†’ CityLearn Environment                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Input Files:                                                                â”‚
â”‚  â”œâ”€ pv_generation_timeseries.csv (8,760 filas)                             â”‚
â”‚  â”œâ”€ individual_chargers.json (32 chargers Ã— 4 sockets)                     â”‚
â”‚  â”œâ”€ perfil_horario_carga.csv (demanda por hora)                            â”‚
â”‚  â””â”€ bess_config.json (4,520 kWh / 2,712 kW - OE2 Real)                     â”‚
â”‚                                                                              â”‚
â”‚  Output Files:                                                               â”‚
â”‚  â”œâ”€ schema.json (definiciÃ³n CityLearn)                                     â”‚
â”‚  â”œâ”€ weather.csv (solar + temperatura)                                      â”‚
â”‚  â”œâ”€ 128 charger CSVs (demanda individual)                                  â”‚
â”‚  â””â”€ Building_1.csv (demanda mall)                                          â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BASELINE: SimulaciÃ³n sin Control Inteligente                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Estrategia: TODOS LOS CHARGERS SIEMPRE AL MÃXIMO                          â”‚
â”‚                                                                              â”‚
â”‚  Input:  CityLearn Environment                                              â”‚
â”‚  Action: [1.0, 1.0, 1.0, ..., 1.0] (todos al 100%)                         â”‚
â”‚  Output: Metrics â†’ COâ‚‚: 10,200 kg/aÃ±o (BASELINE REFERENCE)                 â”‚
â”‚                                                                              â”‚
â”‚  SimulaciÃ³n: 8,760 timesteps (1 aÃ±o completo, resoluciÃ³n horaria)           â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OE3: ENTRENAMIENTO DE AGENTES RL (SAC, PPO, A2C)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€ INPUT: ObservaciÃ³n (534 dimensiones) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Solar generation [1]                                              â”‚   â”‚
â”‚  â”‚  Grid imports [1]                                                  â”‚   â”‚
â”‚  â”‚  BESS SOC [1]                                                      â”‚   â”‚
â”‚  â”‚  Charger states [128Ã—4 = 512]                                      â”‚   â”‚
â”‚  â”‚    â”œâ”€ demand (kW needed)                                           â”‚   â”‚
â”‚  â”‚    â”œâ”€ power (kW actual)                                            â”‚   â”‚
â”‚  â”‚    â”œâ”€ occupancy (0/1)                                              â”‚   â”‚
â”‚  â”‚    â””â”€ battery_soc (%)                                              â”‚   â”‚
â”‚  â”‚  Time features [6]                                                 â”‚   â”‚
â”‚  â”‚    â”œâ”€ hour_of_day [0-23]                                          â”‚   â”‚
â”‚  â”‚    â”œâ”€ day_of_week [0-6]                                           â”‚   â”‚
â”‚  â”‚    â”œâ”€ month [1-12]                                                â”‚   â”‚
â”‚  â”‚    â”œâ”€ is_peak_hours [0/1]                                         â”‚   â”‚
â”‚  â”‚    â”œâ”€ carbon_intensity [kg COâ‚‚/kWh]                               â”‚   â”‚
â”‚  â”‚    â””â”€ electricity_price [$/kWh]                                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                                â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚          â”‚ POLICY NETWORK (MLP)            â”‚                               â”‚
â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                               â”‚
â”‚          â”‚ Input: (534)                    â”‚                               â”‚
â”‚          â”‚   â†“ Dense(1024, ReLU)           â”‚                               â”‚
â”‚          â”‚   â†“ Dense(1024, ReLU)           â”‚                               â”‚
â”‚          â”‚   â†“ Output: (126) [0,1]         â”‚                               â”‚
â”‚          â”‚                                 â”‚                               â”‚
â”‚          â”‚ SAC: +2 Q-networks (critics)    â”‚                               â”‚
â”‚          â”‚ PPO: +1 Value network           â”‚                               â”‚
â”‚          â”‚ A2C: +1 Value network           â”‚                               â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                              â†“                                                â”‚
â”‚  â”Œâ”€ OUTPUT: Acciones (126 dimensiones) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                                                                      â”‚     â”‚
â”‚  â”‚  action[0:112] = Motos (0â†’off, 1â†’2kW)                              â”‚     â”‚
â”‚  â”‚  action[112:126] = Mototaxis (0â†’off, 1â†’3kW)                        â”‚     â”‚
â”‚  â”‚                                                                      â”‚     â”‚
â”‚  â”‚  REGLAS DE DESPACHO (Control):                                     â”‚     â”‚
â”‚  â”‚  1. PVâ†’EV   (solar directo a chargers)     [priority 1 - BEST]     â”‚     â”‚
â”‚  â”‚  2. PVâ†’BESS (cargar baterÃ­a)               [priority 2]            â”‚     â”‚
â”‚  â”‚  3. BESSâ†’EV (descargar en peak)            [priority 3]            â”‚     â”‚
â”‚  â”‚  4. BESSâ†’Grid (inyectar si SOC>95%)        [priority 4]            â”‚     â”‚
â”‚  â”‚  5. Grid Import (si deficit)               [priority 5 - WORST]    â”‚     â”‚
â”‚  â”‚                                                                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                                â”‚
â”‚  â”Œâ”€ REWARD FUNCTION (Multi-objetivo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                                                                      â”‚     â”‚
â”‚  â”‚  reward = 0.50 Ã— r_co2                                              â”‚     â”‚
â”‚  â”‚         + 0.20 Ã— r_solar                                            â”‚     â”‚
â”‚  â”‚         + 0.10 Ã— r_cost                                             â”‚     â”‚
â”‚  â”‚         + 0.10 Ã— r_ev_satisfaction                                  â”‚     â”‚
â”‚  â”‚         + 0.10 Ã— r_grid_stability                                   â”‚     â”‚
â”‚  â”‚                                                                      â”‚     â”‚
â”‚  â”‚  r_co2 = (grid_co2 - agent_co2) / grid_co2   [reduce COâ‚‚ better]   â”‚     â”‚
â”‚  â”‚  r_solar = solar_used / solar_available       [use PV directly]    â”‚     â”‚
â”‚  â”‚  r_cost = (grid_cost - agent_cost) / grid_cost [reduce cost]       â”‚     â”‚
â”‚  â”‚  r_ev_sat = chargers_satisfied / 128          [keep EVs happy]     â”‚     â”‚
â”‚  â”‚  r_grid = 1 - peak_power / max_allowed        [smooth load]        â”‚     â”‚
â”‚  â”‚                                                                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                                â”‚
â”‚  Entrenamiento: 3 episodios Ã— 8,760 timesteps cada uno                      â”‚
â”‚  Checkpoint: Cada 200 timesteps                                             â”‚
â”‚  Output: Agentes entrenados en checkpoints/                                 â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EVALUACIÃ“N FINAL: Comparar Baseline vs 3 Agentes RL                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  BASELINE (Control Manual)        SAC                 PPO âœ¨               A2C
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€
â”‚  COâ‚‚: 10,200 kg/aÃ±o       COâ‚‚: 7,300 kg/aÃ±o  COâ‚‚: 7,100 kg/aÃ±o    COâ‚‚: 7,500 kg/aÃ±o
â”‚  ---------                -33%                -36% BEST            -30%
â”‚  Grid import: 41,300 kWh  Grid: 28,500 kWh   Grid: 26,000 kWh     Grid: 30,000 kWh
â”‚  Solar util: 40%          Solar: 65%          Solar: 70% âœ¨        Solar: 60%
â”‚  Peak power: 4,050 kW     Peak: 3,200 kW      Peak: 3,000 kW       Peak: 3,100 kW
â”‚                                                                              â”‚
â”‚  Time/episode: N/A         Time: 35-45 min    Time: 40-50 min    Time: 30-35 min âš¡
â”‚                                                                              â”‚
â”‚  Recommended: Use PPO for best COâ‚‚ reduction, SAC for sample efficiency     â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¤– ARQUITECTURA DE CADA AGENTE

### SAC (Soft Actor-Critic)

```
CARACTERÃSTICAS:
â€¢ Off-policy (aprende de experiencias pasadas)
â€¢ 2 Q-networks (evita overestimation)
â€¢ Target networks (estabilidad)
â€¢ Replay buffer 10M (sample efficiency)
â€¢ Entropy bonus (exploraciÃ³n automÃ¡tica)

FLUJO:
Observation â†’ Actor (policy) + Gaussain noise â†’ Continuous Action [0,1]
              â†“
          Q1(s,a), Q2(s,a) â†’ min(Q1,Q2)
              â†“
          Critic Loss = MSE(R + Î³Ã—min(Q_target))
          Actor Loss = -E[min(Q1,Q2) + entropy]

STRENGTHS:
âœ“ Sample efficient (learns from old data)
âœ“ Handles sparse rewards well
âœ“ Automatic exploration (entropy)

WEAKNESSES:
âœ— More complex (harder to debug)
âœ— Higher memory usage (buffer)
```

### PPO (Proximal Policy Optimization)

```
CARACTERÃSTICAS:
â€¢ On-policy (usa datos del episodio actual)
â€¢ Clip ratio 0.2 (Â±20% cambio mÃ¡ximo)
â€¢ 2 networks: Actor + Critic
â€¢ GAE advantage estimation
â€¢ KL divergence constraint

FLUJO:
Observation â†’ Actor (policy) â†’ Deterministic Action [0,1]
              â†“
          Value network â†’ V(state)
              â†“
          Advantage = R + Î³Ã—V(next) - V(state)
              â†“
          Clipped Loss = min(ratioÃ—A, clip(ratio)Ã—A)

STRENGTHS:
âœ“ Stable (clipping prevents huge updates)
âœ“ Predictable convergence
âœ“ Well-understood algorithm

WEAKNESSES:
âœ— Sample inefficient (throws away off-policy data)
âœ— Slower learning (conservative updates)
```

### A2C (Advantage Actor-Critic)

```
CARACTERÃSTICAS:
â€¢ On-policy (datos frescos solamente)
â€¢ 1 Actor + 1 Critic (simple)
â€¢ No replay buffer (rÃ¡pido)
â€¢ Deterministic updates
â€¢ RMSprop optimizer (velocidad)

FLUJO:
Observation â†’ Actor â†’ Action [0,1]
              â†“
          Value network â†’ V(state)
              â†“
          Advantage = R - V(state)
              â†“
          Policy Gradient = âˆ‡log(Ï€) Ã— Advantage

STRENGTHS:
âœ“ Fastest training (simple architecture)
âœ“ Low memory footprint
âœ“ Good balance speed/stability

WEAKNESSES:
âœ— High variance (no buffer smoothing)
âœ— Less sample efficient
âœ— May be unstable with bad hyperparams
```

---

## ğŸ“ˆ COMPARACIÃ“N RÃPIDA

| Criterio | SAC | PPO | A2C |
|----------|-----|-----|-----|
| **Velocidad** | Medio (35-45m) | Lento (40-50m) | RÃPIDO (30-35m) âš¡ |
| **Estabilidad** | Alta | MÃY ALTA âœ¨ | Media |
| **Muestra eficiencia** | MÃY ALTA | Media | Baja |
| **ExploraciÃ³n** | AutomÃ¡tica | Manual | Manual |
| **COâ‚‚ reduction** | -33% | -36% âœ¨ | -30% |
| **Memory** | ~6.8 GB | ~6.2 GB | ~6.5 GB |
| **RecomendaciÃ³n** | Pruebas | **ProducciÃ³n** | Prototipo |

---

## ğŸ¯ MÃ‰TODOS DE ENTRENAMIENTO

### Desde Cero
```bash
python -m scripts.run_oe3_simulate --config configs/default.yaml
# Dataset â†’ Baseline â†’ SAC (3 ep) â†’ PPO (3 ep) â†’ A2C (3 ep)
# Tiempo total: ~5-6 horas
```

### Solo A2C (RÃ¡pido)
```bash
python -m scripts.run_a2c_only --config configs/default.yaml
# Dataset â†’ Baseline â†’ A2C (3 ep)
# Tiempo total: ~1-1.5 horas
```

### Componentes Individuales
```bash
# 1. Dataset
python -m scripts.run_oe3_build_dataset --config configs/default.yaml

# 2. Baseline
python -m scripts.run_uncontrolled_baseline --config configs/default.yaml

# 3. Comparar resultados
python -m scripts.run_oe3_co2_table --config configs/default.yaml
```

---

## âœ… VALIDACIONES

**Antes de Entrenar:**
- [ ] Python 3.11.9 (`python --version`)
- [ ] Pylance: 0 errores (Problems panel vacÃ­o)
- [ ] Solar: 8,760 filas exactas
- [ ] Chargers: 32 entries Ã— 4 sockets = 128
- [ ] UTF-8 encoding: `$env:PYTHONIOENCODING='utf-8'`

**DespuÃ©s de Entrenar:**
- [ ] Checkpoints en `checkpoints/A2C/`
- [ ] Resultados en `outputs/oe3_simulations/`
- [ ] COâ‚‚ reducido vs baseline
- [ ] Solar utilization aumentado

---

**Documento de Referencia - Sistema Productivo**  
*Ãšltima actualizaciÃ³n: 27 enero 2026*
