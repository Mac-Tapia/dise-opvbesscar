# OE3 Structure - Visual Maps & Dependency Graphs

---

## 1. Current File Structure (Before Cleanup)

```
src/iquitos_citylearn/oe3/
â”‚
â”œâ”€â”€ ğŸŸ¢ ACTIVE PRODUCTION FILES
â”‚   â”œâ”€â”€ rewards.py                          [529 lines] â† All agents depend on this
â”‚   â”œâ”€â”€ co2_table.py                        [469 lines] â† Main evaluation output
â”‚   â”œâ”€â”€ dataset_builder.py                  [863 lines] â† Creates CityLearn schema
â”‚   â”œâ”€â”€ simulate.py                         [935 lines] â† Central orchestrator
â”‚   â”œâ”€â”€ progress.py                         [50 lines]  â† Training utilities
â”‚   â”œâ”€â”€ enriched_observables.py             [180 lines] â† Observable wrapper
â”‚   â”œâ”€â”€ dispatch_priorities.py              [265 lines] â† BESS dispatch logic
â”‚   â”œâ”€â”€ tier2_v2_config.py                  [50 lines]  â† Config dataclass
â”‚   â”‚
â”‚   â””â”€â”€ agents/                             [7 implementations]
â”‚       â”œâ”€â”€ __init__.py                     â† Central exports
â”‚       â”œâ”€â”€ sac.py                          [1,200 lines] â† SAC RL agent
â”‚       â”œâ”€â”€ ppo_sb3.py                      [900 lines]  â† PPO RL agent
â”‚       â”œâ”€â”€ a2c_sb3.py                      [750 lines]  â† A2C RL agent
â”‚       â”œâ”€â”€ rbc.py                          [350 lines]  â† Rule-based control
â”‚       â”œâ”€â”€ uncontrolled.py                 [100 lines]  â† No control baseline
â”‚       â”œâ”€â”€ no_control.py                   [100 lines]  â† No control variant
â”‚       â”œâ”€â”€ agent_utils.py                  [200 lines]  â† Utilities
â”‚       â”œâ”€â”€ validate_training_env.py        [120 lines]  â† Environment validation
â”‚       â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ ğŸŸ¡ SECONDARY / BACKUP FILES (Should Archive)
â”‚   â”œâ”€â”€ rewards_improved_v2.py              [410 lines] â† v2 iteration
â”‚   â”œâ”€â”€ rewards_wrapper_v2.py               [180 lines] â† v2 wrapper
â”‚   â””â”€â”€ rewards_dynamic.py                  [80 lines]  â† Dynamic reward (dev)
â”‚
â”œâ”€â”€ âš ï¸  UNUSED / ORPHANED FILES (Should Delete/Merge)
â”‚   â”œâ”€â”€ co2_emissions.py                    [358 lines] â† Unused dataclasses (MERGE)
â”‚   â””â”€â”€ demanda_mall_kwh.py                 [507 lines] â† 100% orphaned (DELETE)
â”‚
â”œâ”€â”€ __init__.py
â””â”€â”€ __pycache__/
```

**Summary**:

- âœ… **7 active core files** (~4,500 lines)
- âœ… **7 agent implementations** (~3,600 lines)
- âš ï¸ **3 secondary files** (~670 lines to archive)
- âŒ **2 unused files** (~865 lines to delete/merge)
- **Total**: ~9,600 lines in OE3 module

---

## 2. Import Dependency Graph (Current)

```
ENTRY POINTS (Scripts)
â”‚
â”œâ”€ run_oe3_build_dataset.py â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    â”‚
â”œâ”€ run_oe3_simulate.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â†’ dataset_builder.py
â”‚                                    â”‚
â”œâ”€ run_oe3_co2_table.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚
â””â”€ train_agents_serial.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â†“

            dataset_builder.py
                    â”‚
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Creates CityLearn    â”‚
        â”‚  Schema (JSON)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“

            simulate.py
        (Central Orchestrator)
            â”‚        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                             â”‚
    â†“                             â†“

agents/__init__.py          rewards.py
    â”‚                           â”‚
    â”œâ”€â†’ sac.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”œâ”€â†’ ppo_sb3.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”œâ”€â†’ a2c_sb3.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â”œâ”€â†’ rbc.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”œâ”€â†’ uncontrolled.py â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”œâ”€â†’ no_control.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
        (Training Loop with
        Multi-Objective Reward)
                    â”‚
                    â†“

        outputs/oe3/simulations/
        simulation_summary.json
                    â”‚
                    â†“

            co2_table.py
        (Evaluate all agents)
            â”‚        â”‚        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”˜        â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚                   â”‚
    â†“                â†“                   â†“

OUTPUTS:
â”œâ”€ COMPARACION_BASELINE_VS_RL.txt
â”œâ”€ co2_breakdown_annual.csv
â”œâ”€ agent_comparison.csv
â””â”€ control_comparison_summary.csv


UNUSED IMPORTS:
â””â”€ co2_emissions.py âŒ (imported but classes never used)
```

---

## 3. Data Flow: OE2 â†’ OE3 â†’ Training â†’ Results

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    INPUT LAYER (OE2 Artifacts)                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  data/interim/oe2/
  â”œâ”€â”€ solar/pv_generation_timeseries.csv      [8,760 hourly kW AC values]
  â”‚   â””â”€ Eaton Xpert1670 spec: 2 inverters, 31 modules/string, 6,472 strings
  â”‚
  â”œâ”€â”€ chargers/individual_chargers.json       [32 chargers Ã— 4 sockets]
  â”‚   â””â”€ 112 motos @2kW + 16 mototaxis @3kW = 272 kW installed
  â”‚
  â”œâ”€â”€ chargers/perfil_horario_carga.csv       [24-hour per-charger profile]
  â”‚   â””â”€ 3,061 vehicles/day, 92% utilization
  â”‚
  â””â”€â”€ bess/bess_config.json                   [2 MWh / 1.2 MW BESS]
      â””â”€ Fixed capacity, DoD 80%, eff 95%


          â†“ â†“ â†“ DATASET BUILDER â†“ â†“ â†“
          (src/iquitos_citylearn/oe3/dataset_builder.py)


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              PROCESSING LAYER (CityLearn v2 Schema)                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  data/processed/citylearnv2_dataset/
  â”‚
  â”œâ”€â”€ schema.json                             [Building definition]
  â”‚   â”œâ”€ 1 building (Mall Iquitos)
  â”‚   â”œâ”€ 128 controllable charger outlets
  â”‚   â”œâ”€ PV system (4,050 kWp)
  â”‚   â”œâ”€ BESS system (2 MWh / 1.2 MW)
  â”‚   â””â”€ Grid connection (import/export)
  â”‚
  â”œâ”€â”€ climate_zones/default_climate_zone/
  â”‚   â”œâ”€ weather.csv                  [PVGIS TMY, 8,760 rows]
  â”‚   â”œâ”€ carbon_intensity.csv         [0.4521 kg COâ‚‚/kWh - Iquitos thermal]
  â”‚   â””â”€ pricing.csv                  [0.20 USD/kWh tariff]
  â”‚
  â””â”€â”€ buildings/Iquitos_EV_Mall_PV_BESS/
      â”œâ”€ energy_simulation.csv        [PV + chargers + building load]
      â”œâ”€ charger_simulation_0.csv     [Charger 1 profile, 8,760 rows]
      â”œâ”€ charger_simulation_1.csv     [Charger 2 profile, 8,760 rows]
      â”œâ”€ ...
      â””â”€ charger_simulation_127.csv   [Charger 128 profile, 8,760 rows]


          â†“ â†“ â†“ AGENTS & ENVIRONMENT â†“ â†“ â†“
          (src/iquitos_citylearn/oe3/simulate.py)


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  TRAINING LAYER (RL Agents)                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  CityLearnEnv(schema)
  â”‚
  â”œâ”€ Observation Space: 534 dimensions (flattened)
  â”‚  â”œâ”€ Building energy (solar, demand, grid)  [4 values]
  â”‚  â”œâ”€ Charger states (power, occupancy, SOC) [128Ã—3 = 384 values]
  â”‚  â””â”€ Time features (hour, month, dow)       [4 values]
  â”‚
  â””â”€ Action Space: 126 continuous [0, 1]
     â””â”€ Charger power setpoints (126 of 128 controllable)

          â”‚ â†“
          â”‚ Training Loop (per-timestep):
          â”‚  1. Observe env state
          â”‚  2. Agent.predict(obs) â†’ action
          â”‚  3. env.step(action)
          â”‚  4. Compute Multi-Objective Reward:
          â”‚     r_total = 0.50Â·r_COâ‚‚ + 0.20Â·r_solar + 0.10Â·r_cost + ...
          â”‚  5. Repeat 8,760 timesteps (1 year)
          â”‚
          â””â”€ Trained Agents:
             â”œâ”€ SAC (off-policy)
             â”œâ”€ PPO (on-policy)
             â”œâ”€ A2C (on-policy, simple)
             â””â”€ Baselines (Uncontrolled, RBC, NoControl)


          â†“ â†“ â†“ EVALUATION & RESULTS â†“ â†“ â†“
          (src/iquitos_citylearn/oe3/co2_table.py)


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              OUTPUT LAYER (Results & Comparisons)                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  outputs/oe3/simulations/
  â”œâ”€ simulation_summary.json          [All agents' metrics]
  â”‚  â”œâ”€ pv_bess_results
  â”‚  â”‚  â”œâ”€ SAC: {COâ‚‚, kWh, rewards, ...}
  â”‚  â”‚  â”œâ”€ PPO: {COâ‚‚, kWh, rewards, ...}
  â”‚  â”‚  â””â”€ A2C: {COâ‚‚, kWh, rewards, ...}
  â”‚  â””â”€ pv_bess_uncontrolled: {baseline metrics}
  â”‚
  â””â”€ *_results.json                  [Per-agent detailed results]

  analyses/oe3/training/
  â””â”€ checkpoints/
     â”œâ”€ SAC/*.zip                     [Trained models]
     â”œâ”€ PPO/*.zip
     â””â”€ A2C/*.zip

  analyses/oe3/
  â”œâ”€ COMPARACION_BASELINE_VS_RL.txt   [COâ‚‚ comparison table]
  â”œâ”€ co2_breakdown_annual.csv         [Emissions by scenario]
  â”œâ”€ agent_comparison.csv             [Multiobjetivo metrics]
  â””â”€ control_comparison_summary.csv   [Control strategies]


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      FINAL OUTPUTS                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Key Metrics for Each Agent:
  â”œâ”€ COâ‚‚ emissions (kg/year)
  â”œâ”€ EV charging (kWh/year)
  â”œâ”€ Grid import (kWh/year)
  â”œâ”€ Solar generation (kWh/year)
  â”œâ”€ Self-consumption rate (%)
  â”œâ”€ Multi-objective reward components (5 metrics)
  â””â”€ Cost (USD/year)

  Comparison: Baseline vs SAC vs PPO vs A2C
  â””â”€ Ranking: [Best COâ‚‚ reduction] â†’ Recommended agent
```

---

## 4. Reward System Architecture

```
Multi-Objective Reward Function
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input per timestep:
  obs, actions, env state, carbon_intensity

                    â†“

MultiObjectiveWeights (Dataclass)
â”œâ”€ co2: 0.50                     â† PRIMARY objective
â”œâ”€ solar: 0.20                   â† SECONDARY objective
â”œâ”€ cost: 0.10                    â† TERTIARY objective
â”œâ”€ ev_satisfaction: 0.10         â† BASELINE
â””â”€ grid_stability: 0.10          â† BASELINE


                    â†“

MultiObjectiveReward.compute()
(Function in rewards.py)

    â”œâ”€ Component 1: r_COâ‚‚ = -grid_import_kwh Ã— 0.4521
    â”‚  â””â”€ Penalizes thermal grid imports
    â”‚
    â”œâ”€ Component 2: r_solar = pv_used_directly / (pv_generated + 0.1)
    â”‚  â””â”€ Rewards PV self-consumption
    â”‚
    â”œâ”€ Component 3: r_cost = -grid_import_kwh Ã— 0.20 (USD/kWh)
    â”‚  â””â”€ Penalizes electricity cost
    â”‚
    â”œâ”€ Component 4: r_ev = -max(0, charger_demand - charger_power)
    â”‚  â””â”€ Penalizes unmet EV charging demand
    â”‚
    â””â”€ Component 5: r_grid = -max(0, peak_power - threshold)
       â””â”€ Penalizes grid demand peaks

                    â†“

Weighted Sum:
    r_total = 0.50Â·r_COâ‚‚ + 0.20Â·r_solar + 0.10Â·r_cost + ...

                    â†“

Output: Single scalar reward per timestep
â””â”€ Agents optimize total cumulative reward over 8,760 timesteps


VERSION STATUS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

v1 ACTIVE (rewards.py):
â”œâ”€ MultiObjectiveWeights [co2, solar, cost, ev_satisfaction, grid_stability]
â”œâ”€ IquitosContext [grid_carbon_intensity, tariff, charger_count]
â”œâ”€ MultiObjectiveReward [compute() method]
â””â”€ CityLearnMultiObjectiveWrapper [Gymnasium wrapper]
   â””â”€ Used in: simulate.py (MAIN PIPELINE)

v2 ARCHIVED (rewards_improved_v2.py):
â”œâ”€ ImprovedWeights [co2, solar, cost, ev_satisfaction, grid_stability + peak_import_penalty]
â”œâ”€ IquitosContextV2 [adds grid_stability_threshold]
â”œâ”€ ImprovedMultiObjectiveReward [compute_detailed() method]
â””â”€ ImprovedRewardWrapper [Alternative Gymnasium wrapper]
   â””â”€ Used in: rewards_wrapper_v2.py (NOT IN PIPELINE)

DYNAMIC (rewards_dynamic.py):
â”œâ”€ DynamicReward [Hour-based sinusoidal gradients]
â””â”€ Used in: train_ppo_dynamic.py (DEV SCRIPT ONLY)
```

---

## 5. Agent Dependency Chain

```
AGENT FACTORY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

src/iquitos_citylearn/oe3/agents/__init__.py
â”‚
â”œâ”€â†’ make_sac(env, config) â†’ SACAgent
â”‚   â””â”€ src/iquitos_citylearn/oe3/agents/sac.py
â”‚      â”œâ”€ Implements: learn(), predict(), load(), save()
â”‚      â”œâ”€ Depends on: stable_baselines3.SAC
â”‚      â”œâ”€ Uses: progress.py (training logging)
â”‚      â””â”€ Requires: rewards.py (reward function)
â”‚
â”œâ”€â†’ make_ppo(env, config) â†’ PPOAgent
â”‚   â””â”€ src/iquitos_citylearn/oe3/agents/ppo_sb3.py
â”‚      â”œâ”€ Implements: learn(), predict(), load(), save()
â”‚      â”œâ”€ Depends on: stable_baselines3.PPO
â”‚      â”œâ”€ Uses: progress.py (training logging)
â”‚      â””â”€ Requires: rewards.py (reward function)
â”‚
â”œâ”€â†’ make_a2c(env, config) â†’ A2CAgent
â”‚   â””â”€ src/iquitos_citylearn/oe3/agents/a2c_sb3.py
â”‚      â”œâ”€ Implements: learn(), predict(), load(), save()
â”‚      â”œâ”€ Depends on: stable_baselines3.A2C
â”‚      â”œâ”€ Uses: progress.py (training logging)
â”‚      â””â”€ Requires: rewards.py (reward function)
â”‚
â”œâ”€â†’ make_basic_ev_rbc(env, config) â†’ BasicRBCAgent
â”‚   â””â”€ src/iquitos_citylearn/oe3/agents/rbc.py
â”‚      â”œâ”€ Implements: predict() [deterministic control]
â”‚      â””â”€ Rule: Charge if solar > demand, discharge at peak hours
â”‚
â”œâ”€â†’ UncontrolledChargingAgent
â”‚   â””â”€ src/iquitos_citylearn/oe3/agents/uncontrolled.py
â”‚      â”œâ”€ Implements: predict() [maximum power setpoint]
â”‚      â””â”€ Always: action = [1.0, 1.0, ..., 1.0] (all chargers at max)
â”‚
â”œâ”€â†’ make_no_control(env) â†’ NoControlAgent
â”‚   â””â”€ src/iquitos_citylearn/oe3/agents/no_control.py
â”‚      â”œâ”€ Implements: predict() [zero power]
â”‚      â””â”€ Always: action = [0.0, 0.0, ..., 0.0] (no charging)
â”‚
â”œâ”€ Utilities: agent_utils.py
â”‚  â”œâ”€ validate_env_spaces(env)
â”‚  â”œâ”€ ListToArrayWrapper (CityLearn list â†’ numpy array)
â”‚  â”œâ”€ flatten_action(), unflatten_action()
â”‚  â””â”€ normalize_observations(), clip_observations()
â”‚
â””â”€ Validation: validate_training_env.py
   â”œâ”€ check_dataset()
   â”œâ”€ check_agents()
   â”œâ”€ check_rewards()
   â””â”€ check_gpu()


REWARDS DEPENDENCY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

rewards.py
â”‚
â”œâ”€ MultiObjectiveWeights dataclass
â”‚  â”œâ”€ Instantiated by: agent configs (SAC, PPO, A2C)
â”‚  â””â”€ Used in: simulate.py, agents training loops
â”‚
â”œâ”€ IquitosContext dataclass
â”‚  â”œâ”€ Instantiated by: CityLearnMultiObjectiveWrapper
â”‚  â””â”€ Provides: grid carbon intensity, tariff, charger count
â”‚
â”œâ”€ MultiObjectiveReward class
â”‚  â”œâ”€ Instantiated by: CityLearnMultiObjectiveWrapper
â”‚  â”œâ”€ Method: compute(obs, actions, info) â†’ float reward
â”‚  â””â”€ Used in: Training loop (called 8,760 times per episode)
â”‚
â”œâ”€ CityLearnMultiObjectiveWrapper class
â”‚  â”œâ”€ Wraps: CityLearnEnv (Gymnasium wrapper)
â”‚  â”œâ”€ Override: step() method to apply custom reward
â”‚  â””â”€ Used in: simulate.py
â”‚     ```python
â”‚     env = CityLearnEnv(schema)
â”‚     weights = MultiObjectiveWeights(...)
â”‚     reward_fn = MultiObjectiveReward(weights, context)
â”‚     env = CityLearnMultiObjectiveWrapper(env, reward_fn)
â”‚     ```
â”‚
â””â”€ create_iquitos_reward_weights() function
   â”œâ”€ Factory: creates weights from config dict
   â””â”€ Used in: verification scripts, config loading


TRAINING LOOP EXECUTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

simulate.py::simulate()
â”‚
â”œâ”€ Load schema and create CityLearnEnv
â”œâ”€ Wrap with CityLearnMultiObjectiveWrapper
â”‚
â”œâ”€ Select agent: SAC / PPO / A2C / RBC / Uncontrolled
â”‚  â””â”€ Create agent: make_sac() / make_ppo() / make_a2c() / ...
â”‚
â”œâ”€ Training loop (per episode):
â”‚  â”‚
â”‚  â”œâ”€ obs, info = env.reset()
â”‚  â”‚
â”‚  â”œâ”€ For t in range(8760):  # 1 year
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ action = agent.predict(obs)  [Agent decides charger power]
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ obs, reward, terminated, truncated, info = env.step(action)
â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ env executes action
â”‚  â”‚  â”‚  â”œâ”€ CityLearn simulates physics
â”‚  â”‚  â”‚  â””â”€ MultiObjectiveReward.compute() â†’ reward [CUSTOM REWARD]
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ agent.learn(obs, action, reward, next_obs, done)
â”‚  â”‚  â”‚  â””â”€ Update agent policy
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Track metrics: COâ‚‚, EV kWh, grid import, solar gen, etc.
â”‚  â”‚
â”‚  â””â”€ Save checkpoint: agent_final.zip
â”‚
â””â”€ Return SimulationResult with COâ‚‚, kWh, rewards, etc.
```

---

## 6. File Status Matrix (Before & After Cleanup)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        BEFORE CLEANUP (Current State)                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

File                          Lines   Status      Used By              Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
rewards.py                    529     âœ… ACTIVE   agents/__init__.py   KEEP
co2_table.py                  469     âœ… ACTIVE   run_oe3_co2_table    KEEP
dataset_builder.py            863     âœ… ACTIVE   run_oe3_build_*      KEEP
simulate.py                   935     âœ… ACTIVE   run_oe3_simulate     KEEP
progress.py                   50      âœ… ACTIVE   agents/*.py          KEEP
enriched_observables.py       180     âœ… ACTIVE   [?] Need review      KEEP*
dispatch_priorities.py        265     âœ… ACTIVE   rewards.py           KEEP
tier2_v2_config.py            50      âœ… ACTIVE   config mgmt          KEEP

agents/__init__.py            100     âœ… ACTIVE   entry point          KEEP
agents/sac.py                 1200    âœ… ACTIVE   simulate.py          KEEP
agents/ppo_sb3.py             900     âœ… ACTIVE   simulate.py          KEEP
agents/a2c_sb3.py             750     âœ… ACTIVE   simulate.py          KEEP
agents/rbc.py                 350     âœ… ACTIVE   simulate.py          KEEP
agents/uncontrolled.py        100     âœ… ACTIVE   simulate.py          KEEP
agents/no_control.py          100     âœ… ACTIVE   simulate.py          KEEP
agents/agent_utils.py         200     âœ… ACTIVE   agents/*.py          KEEP
agents/validate_training_env  120     âœ… ACTIVE   verification         KEEP

rewards_improved_v2.py        410     âš ï¸  BACKUP  rewards_wrapper_v2   ARCHIVE
rewards_wrapper_v2.py         180     âš ï¸  BACKUP  [NOT USED]           ARCHIVE
rewards_dynamic.py            80      âš ï¸  DEV     train_ppo_dynamic    ARCHIVE

co2_emissions.py              358     âŒ UNUSED   co2_table (import)   MERGE
demanda_mall_kwh.py           507     âŒ ORPHAN   [ZERO imports]       DELETE

                            â”€â”€â”€â”€â”€â”€
TOTAL ACTIVE CODE:         ~5,100 lines
TOTAL BACKUP CODE:            670 lines  â† Can be archived
TOTAL UNUSED CODE:            865 lines  â† Can be deleted/merged


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        AFTER CLEANUP (Recommended State)                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

File                          Lines   Status      Location             Impact
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
rewards.py                    529     âœ… ACTIVE   oe3/                 NONE
co2_table.py                  827     âœ… ACTIVE   oe3/                 Merged
dataset_builder.py            863     âœ… ACTIVE   oe3/                 NONE
simulate.py                   935     âœ… ACTIVE   oe3/                 NONE
progress.py                   50      âœ… ACTIVE   oe3/                 NONE
enriched_observables.py       180     âœ… ACTIVE   oe3/                 NONE
dispatch_priorities.py        265     âœ… ACTIVE   oe3/                 NONE
tier2_v2_config.py            50      âœ… ACTIVE   oe3/                 NONE

agents/                       3600    âœ… ACTIVE   oe3/agents/          NONE

rewards_improved_v2.py        410     ğŸ”¶ ARCHIVE experimental/         Reference
rewards_wrapper_v2.py         180     ğŸ”¶ ARCHIVE experimental/         Reference
rewards_dynamic.py            80      ğŸ”¶ ARCHIVE experimental/         Reference

[DELETED co2_emissions.py]     â€”       â€”          â€”                    â€”
[DELETED demanda_mall_kwh.py]  â€”       â€”          â€”                    â€”

                            â”€â”€â”€â”€â”€â”€
TOTAL ACTIVE CODE:         ~5,100 lines
TOTAL ARCHIVED CODE:          670 lines  â† Separate experimental/ folder
TOTAL DELETED CODE:           865 lines  â† Removed from repo

RESULT: Cleaner, easier to maintain, no functional changes to production
```

---

## 7. Risk Assessment Heat Map

```
CLEANUP OPERATIONS RISK ASSESSMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Operation                          Risk Level   Rollback Time   Impact
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. DELETE demanda_mall_kwh.py      ğŸŸ¢ NONE      1 minute        Zero
2. MERGE co2_emissions â†’ co2_table ğŸŸ¡ LOW       2 minutes       Minor (test)
3. ARCHIVE rewards_improved_v2.py  ğŸŸ¢ NONE      1 minute        Zero
4. ARCHIVE rewards_wrapper_v2.py   ğŸŸ¢ NONE      1 minute        Zero
5. ARCHIVE rewards_dynamic.py      ğŸŸ¡ LOW       1 minute        Minor (dev)
6. CREATE documentation            ğŸŸ¢ NONE      1 minute        Zero
7. RUN VERIFICATION TESTS          ğŸŸ¡ LOW       5 minutes       Catch issues

TOTAL CLEANUP TIME:                ~35 minutes
TOTAL ROLLBACK TIME IF NEEDED:    ~15 minutes
CONFIDENCE LEVEL:                 95% (Very Low Risk)
```

---

**Visual analysis complete!** Use these diagrams to understand module structure, dependencies, and data flow.
