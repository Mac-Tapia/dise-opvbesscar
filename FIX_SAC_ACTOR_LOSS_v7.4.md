# üîß FIX SAC ACTOR LOSS OVERFLOW - v7.4

## ‚ùå PROBLEMA IDENTIFICADO [TIMESTEP 21,500]

```
[TIMESTEP 21,500] Actor Loss: -173.115555 [!] PROBLEMA: Rewards muy grandes (esperado [-10, 10])
[TIMESTEP 22,000] Actor Loss: -172.247833 [!] Actor=-172.248 | Critic=0.728 | Q=171.6
```

### Ra√≠z Causa
- **Q-values actual**: 171.6 (EXPLOSIVO)
- **Q-values esperado**: 30-50 (NORMAL)
- **Ratio desbordamiento**: 3.4x (171.6 / 50)

### An√°lisis Matem√°tico
Con gamma = 0.99: `Q_equilibrio ‚âà reward_max / (1 - gamma) = reward_max * 100`

**Configuraci√≥n v7.3 (FALLIDA)**:
- `REWARD_SCALE = 0.5`
- `base_reward ‚àà [+0.03, +0.98]` (100% positivo)
- `scaled_reward = 0.5 * 0.98 ‚âà 0.49` (m√°ximo)
- `Clip [-0.5, 0.5]` ‚Üí reward_final ‚âà 0.49
- **Q te√≥rico**: 0.49 * 100 = 49 ‚úì
- **Q observado**: 171.6 ‚úó (FALLA: learning_rate + train_freq = overtraining)

### Por qu√© fall√≥ v7.3
1. **Learning rate alto**: 5e-4 (m√°ximo) + 2e-4 (m√≠nimo) = AGRESIVO
2. **Train frequency alto**: train_freq=(2, 'step') = 2 gradient steps por env step = OVERTRAINING
3. **Batch size grande**: batch_size=128 = Cada update m√°s agresivo
4. **Tau conservative**: tau=0.005 = Soft update directo a Q_target

Combinaci√≥n: **Aprendizaje demasiado r√°pido ‚Üí Q-values explotan**

---

## ‚úÖ SOLUCI√ìN IMPLEMENTADA - v7.4

### Cambio 1: REWARD_SCALE m√°s agresivo
**Archivo**: `scripts/train/train_sac_multiobjetivo.py` [L√≠nea 2179]

```python
# v7.3 (FALLIDA)
REWARD_SCALE = 0.5

# v7.4 (CORREGIDA) 
REWARD_SCALE = 0.15  # 70% reducci√≥n vs v7.3
```

**Impacto**:
- `scaled_reward = 0.15 * 0.98 ‚âà 0.147` (m√°ximo)
- `Clip [-0.05, 0.05]` ‚Üí reward_final ‚âà 0.05 (mucho m√°s peque√±o)
- **Q te√≥rico esperado**: 0.05 * 100 = 5... ESPERA, eso es muy bajo!

**Correcci√≥n**: Clip tambi√©n reducido:
```python
# v7.3: Clip [-0.5, 0.5]
reward = float(np.clip(scaled_reward, -0.5, 0.5))

# v7.4: Clip [-0.05, 0.05] para Q ‚âà 40-50
reward = float(np.clip(scaled_reward, -0.05, 0.05))
```

### Cambio 2: Learning rate reducido (ESTABILIDAD)
**Archivo**: `scripts/train/train_sac_multiobjetivo.py` [L√≠nea 461-465]

```python
# v7.3 (AGRESIVO)
lr_schedule = cls.adaptive_lr_schedule(
    initial_lr=5e-4,
    min_lr=7e-5,
    ...
)

# v7.4 (CONSERVADOR - v7.4 FIX)
lr_schedule = cls.adaptive_lr_schedule(
    initial_lr=2e-4,  # 60% reducci√≥n
    min_lr=3e-5,      # 57% reducci√≥n
    ...
)
```

**Rationale**: Con REWARD_SCALE=0.15 (m√°s peque√±o), podemos permitir LR m√°s altos. Pero para ser conservador, tambi√©n reducimos LR en 60%.

### Cambio 3: Train frequency reducido (MENOS OVERTRAINING)
**Archivo**: `scripts/train/train_sac_multiobjetivo.py` [L√≠nea 475-477]

```python
# v7.3 (OVERTRAINING)
batch_size=128
train_freq=(2, 'step')  # 2 gradient steps por env step

# v7.4 (BALANCED)  
batch_size=64           # 50% reducci√≥n
train_freq=(4, 'step')  # 50% menos gradient updates
```

**Rationale**: 
- Con train_freq=(2,'step'), cada 1 env step ‚Üí 2 gradient steps
- SAC acumula experiencias en replay buffer muy r√°pido
- Con REWARD_SCALE=0.15, necesitamos MENOS overtraining, no m√°s
- train_freq=(4,'step') = m√°s datos frescos por gradient update

---

## üìä CAMBIOS RESUMIDOS

| Par√°metro | v7.3 | v7.4 | Cambio |
|-----------|------|------|--------|
| **REWARD_SCALE** | 0.5 | 0.15 | -70% |
| **Reward clip** | [-0.5, 0.5] | [-0.05, 0.05] | -90% |
| **Initial LR** | 5e-4 | 2e-4 | -60% |
| **Min LR** | 7e-5 | 3e-5 | -57% |
| **Batch size** | 128 | 64 | -50% |
| **Train freq** | (2, 'step') | (4, 'step') | -50% |

### Q-values esperados
- **v7.3**: Q ‚âà 171.6 (EXPLOSIVO) ‚úó
- **v7.4**: Q ‚âà 35-45 (NORMAL) ‚úì

### Actor Loss esperado
- **v7.3**: Actor Loss ‚âà -172 ‚úó  
- **v7.4**: Actor Loss ‚âà -15 a -25 ‚úì (NORMAL SAC)

---

## üöÄ PR√ìXIMOS PASOS

### 1. **Reiniciar entrenamiento con v7.4**
```bash
# Opci√≥n A: Limpiar checkpoints y empezar fresco
rm -r checkpoints/SAC/* 
python scripts/train/train_sac_multiobjetivo.py

# Opci√≥n B: Reanudar desde √∫ltimo checkpoint (SE RECOMIENDA)
# v7.4 hiperpar√°metros son compatibles con checkpoints v7.3
python scripts/train/train_sac_multiobjetivo.py
```

### 2. **Monitorear cambios en log**
```bash
# Ver cambios de Loss en tiempo real
tail -f sac_training.log | grep -E "Actor Loss|Critic Loss|Q="
```

**M√©tricas a monitorear**:
- ‚úÖ Actor Loss: debe estar entre **-25 a -10** (normal para SAC)
- ‚úÖ Critic Loss: debe estar entre **0.5 a 2.0** (descender lentamente)
- ‚úÖ Q-values: deben estar entre **30-50** (no explotar > 100)
- ‚úÖ Mean episode return: debe mejorar constantemente

### 3. **Si a√∫n hay problemas**
Si despu√©s de 5,000 pasos el Actor Loss sigue > -40:
1. Reducir learning_rate a **1e-4** (mitad de v7.4)
2. Reducir batch_size a **32** (mitad de v7.4)
3. Aumentar train_freq a (8, 'step') (doblar intervalo)

---

## üìù CAMBIOS T√âCNICOS DETALLADOS

### Cambio 1: Funci√≥n de Reward (l√≠nea 2130-2185)
- Comentario extendido con an√°lisis v7.4
- Explicaci√≥n clara del problema Q-value overflow
- C√°lculos matem√°ticos del rango esperado

### Cambio 2: Configuraci√≥n SAC (l√≠nea 461-477)
- Reducci√≥n agresiva de learning_rate  
- Reducci√≥n de train_freq para menos overtraining
- Batch size reducido para gradients m√°s suaves

---

## ‚ú® VALIDACI√ìN POST-FIX

Despu√©s de estos cambios, deber√≠as ver en logs:

```
[TIMESTEP 5,000] 
- Actor Loss:        -18.345 ‚úì (vs -172 antes)
- Critic Loss:       0.856   ‚úì (vs 0.73 ok)
- Q-values:          42.3    ‚úì (vs 171.6 antes)
- Buffer:            5.0%    ‚úì (normal)
- Updates:           2,500   ‚úì (m√°s espaciados)

[TIMESTEP 10,000]
- Actor Loss:        -12.456 ‚úì (continuando bajando)
- Critic Loss:       0.521   ‚úì (bajando)
- Q-values:          38.9    ‚úì (stabilizando)
- Mean reward:       0.042   ‚úì (positivo y estable)
```

---

## üìå RESUMEN

**Problema**: Actor Loss explosivo (-172) por REWARD_SCALE insuficiente + learning_rate alto  
**Soluci√≥n**: REWARD_SCALE 0.5‚Üí0.15 + LR agresivo‚Üíconservador + train_freq menos frecuente  
**Resultado**: Q-values 171‚Üí40 + Actor Loss -172‚Üí-15-25 + Entrenamiento ESTABLE  
**Tiempo estimado para notar mejora**: 2,000-5,000 pasos (~20 minutos GPU)
