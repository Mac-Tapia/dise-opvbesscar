"""
REPORTE DE ENTRENAMIENTO: Estado Actual del Entrenamiento RL
Verificaci√≥n de Avance y Aprendizaje de A2C, SAC, PPO
"""

import json
from pathlib import Path

print("=" * 110)
print("üöÄ REPORTE DE ENTRENAMIENTO: VERIFICACI√ìN DE AVANCE Y APRENDIZAJE")
print("=" * 110)
print()

results_dir = Path("outputs/oe3/simulations")

# Cargar resultados
results = {}
for agent in ["A2C", "SAC", "PPO", "Uncontrolled"]:
    with open(results_dir / f"result_{agent}.json") as f:
        results[agent] = json.load(f)

print("1Ô∏è‚É£ ESTADO DE ENTRENAMIENTO")
print("-" * 110)
print()

config_max_steps = {
    "A2C": 87600,
    "SAC": 100000,
    "PPO": 438000,
}

for agent in ["A2C", "SAC", "PPO"]:
    result = results[agent]
    steps = result["steps"]
    max_steps = config_max_steps[agent]
    years = result["simulated_years"]
    progress = (steps / max_steps) * 100
    
    print(f"ü§ñ {agent}")
    print(f"   Pasos completados:  {steps:>10,} / {max_steps:>10,} ({progress:>5.1f}%)")
    print(f"   A√±os simulados:     {years:>10.2f}")
    
    if progress >= 99:
        print(f"   Estado:             ‚úÖ COMPLETADO")
    elif progress >= 50:
        print(f"   Estado:             üîÑ EN PROGRESO (50%+)")
    elif progress >= 10:
        print(f"   Estado:             ‚è≥ EARLY STAGE (< 50%)")
    else:
        print(f"   Estado:             üö´ APENAS COMENZ√ì")
    print()

print()
print("2Ô∏è‚É£ COMPARACI√ìN: AGENTES ENTRENADOS VS BASELINE")
print("-" * 110)
print()

baseline = results["Uncontrolled"]
baseline_carbon = baseline["carbon_kg"]
baseline_grid = baseline["grid_import_kwh"]
baseline_reward = baseline["reward_total_mean"]

metrics = {
    "A2C": results["A2C"],
    "SAC": results["SAC"],
    "PPO": results["PPO"],
}

print(f"{'Agente':<12} {'CO‚ÇÇ (kg)':<18} {'Reducci√≥n':<15} {'Grid (GWh)':<18} {'Reducci√≥n':<15} {'Recompensa':<12}")
print("-" * 110)

for name, result in metrics.items():
    carbon = result["carbon_kg"]
    grid = result["grid_import_kwh"] / 1e6
    reward = result["reward_total_mean"]
    
    carbon_red = ((baseline_carbon - carbon) / baseline_carbon) * 100
    grid_red = ((baseline_grid - result["grid_import_kwh"]) / baseline_grid) * 100
    
    print(f"{name:<12} {carbon:>15,.0f}  {carbon_red:>12.1f}%   {grid:>15.2f}  {grid_red:>12.1f}%   {reward:>10.4f}")

print()
print(f"{'BASELINE':<12} {baseline_carbon:>15,.0f}  {0:>12.1f}%   {baseline_grid/1e6:>15.2f}  {0:>12.1f}%   {baseline_reward:>10.4f}")

print()
print()
print("3Ô∏è‚É£ AN√ÅLISIS DETALLADO DE A2C (Agente Principal)")
print("-" * 110)
print()

a2c = results["A2C"]
steps_pct = (a2c["steps"] / 87600) * 100

print(f"Progreso de Entrenamiento:")
print(f"  Pasos: {a2c['steps']:>10,} / 87,600 ({steps_pct:>5.1f}%)")
print(f"  √âpoca actual: ~{a2c['steps'] // 8760 + 1} de 10 episodios")
print(f"  Tiempo estimado para completar: ~{(87600 - a2c['steps']) // 8760 * 60:.0f} minutos")
print()

print(f"Desempe√±o Energ√©tico:")
print(f"  Grid Import Total:    {a2c['grid_import_kwh']:>15,.0f} kWh = {a2c['grid_import_kwh']/1e6:>6.2f} GWh")
print(f"  PV Generation:        {a2c['pv_generation_kwh']:>15,.0f} kWh = {a2c['pv_generation_kwh']/1e6:>6.2f} GWh")
print(f"  EV Charging Demand:   {a2c['ev_charging_kwh']:>15,.0f} kWh = {a2c['ev_charging_kwh']/1e3:>6.2f} MWh")
print(f"  Building Load:        {a2c['building_load_kwh']:>15,.0f} kWh = {a2c['building_load_kwh']/1e6:>6.2f} GWh")
print()

print(f"M√©tricas de CO‚ÇÇ y Costo:")
print(f"  Emisiones CO‚ÇÇ:        {a2c['carbon_kg']:>15,.0f} kg")
print(f"  vs Baseline:          {((baseline_carbon - a2c['carbon_kg']) / baseline_carbon * 100):>14.1f}% reducci√≥n")
print(f"  Costo anual (est.):   ${a2c['net_grid_kwh'] * 0.20:>14,.0f}")
print()

print(f"Recompensas Multiobjetivo:")
objectives = [
    ("CO‚ÇÇ Reduction (50%)",     a2c['reward_co2_mean'], 0.50),
    ("Cost Reduction (15%)",    a2c['reward_cost_mean'], 0.15),
    ("Solar Maximize (20%)",    a2c['reward_solar_mean'], 0.20),
    ("EV Satisfaction (10%)",   a2c['reward_ev_mean'], 0.10),
    ("Grid Stability (5%)",     a2c['reward_grid_mean'], 0.05),
]

for obj, reward, weight in objectives:
    weighted = reward * weight
    print(f"  {obj:<25} {reward:>8.4f} ‚Üí {weighted:>8.4f}")

print(f"  {'‚îÄ' * 40}")
print(f"  {'Total Reward':<25} {a2c['reward_total_mean']:>8.4f}")

print()
print()
print("4Ô∏è‚É£ EVALUACI√ìN DE APRENDIZAJE A2C")
print("-" * 110)
print()

print("‚úÖ LO QUE A2C HA APRENDIDO BIEN:")
print()
if a2c["reward_solar_mean"] > 0:
    print(f"  ‚òÄÔ∏è  Autoconsumo Solar ({a2c['reward_solar_mean']:.3f}):")
    print(f"      A2C APRENDE a cargar EVs cuando hay disponibilidad solar")
    print(f"      Estrategia: Maximizar autoconsumo, reducir importaci√≥n de red")
else:
    print(f"  ‚òÄÔ∏è  Autoconsumo Solar ({a2c['reward_solar_mean']:.3f}): ‚ö†Ô∏è D√©bil")

if a2c["reward_ev_mean"] > 0.05:
    print(f"      ")
    print(f"  üîã Satisfacci√≥n de EV ({a2c['reward_ev_mean']:.3f}):")
    print(f"      A2C APRENDE a mantener EVs satisfechos (>90% SOC)")
    print(f"      Gestiona carga para cumplir demanda con disponibilidad solar + BESS")
else:
    print(f"  üîã Satisfacci√≥n de EV ({a2c['reward_ev_mean']:.3f}): ‚ö†Ô∏è D√©bil")

print()
print("‚ö†Ô∏è OBJETIVOS DIF√çCILES (comportamiento esperado):")
print()
if a2c["reward_co2_mean"] < -0.8:
    print(f"  üåç CO‚ÇÇ Reduction ({a2c['reward_co2_mean']:.3f}): ‚ùå IMPOSIBLE")
    print(f"      Raz√≥n: Red t√©rmica Iquitos tiene factor de emisi√≥n 0.4521 kg/kWh")
    print(f"      Mall debe importar ~24.7 GWh/a√±o para carga base (inevitables emisiones CO‚ÇÇ)")
    print(f"      PV solo genera 8 GWh ‚Üí No es suficiente para bajar m√°s.")
    print()

if a2c["reward_grid_mean"] < -0.4:
    print(f"  üìä Grid Stability ({a2c['reward_grid_mean']:.3f}): ‚ö†Ô∏è Dif√≠cil")
    print(f"      Raz√≥n: Demanda del mall es muy alta (peak 570 kWh vs 200 kWh limite)")
    print(f"      BESS solo 2000 kWh no es suficiente para desplazar toda la carga pico")

print()
print()
print("5Ô∏è‚É£ PROGRESO DE APRENDIZAJE A TRAV√âS DE ENTRENAMIENTOS")
print("-" * 110)
print()

print(f"Configuraci√≥n de Entrenamiento:")
print(f"  ‚Ä¢ Episodes configurados: 10")
print(f"  ‚Ä¢ Pasos por episodio: ~8,760 (1 a√±o de datos)")
print(f"  ‚Ä¢ Pasos totales m√°ximo: 87,600")
print(f"  ‚Ä¢ Pasos actuales: {a2c['steps']:,} ({steps_pct:.1f}%)")
print()

if steps_pct < 20:
    stage = "üö´ EARLY STAGE (aprendizaje inicial)"
elif steps_pct < 50:
    stage = "üîÑ CONVERGENCIA INICIAL (pol√≠ticas b√°sicas formadas)"
elif steps_pct < 80:
    stage = "üìà MEJORA (refinamiento de estrategia)"
else:
    stage = "‚úÖ CONVERGENCIA FINAL (pol√≠ticas estables)"

print(f"Etapa de Aprendizaje: {stage}")
print()

print("Comportamiento Esperado por Etapa:")
print(f"  ‚Ä¢ 0-10%: Acciones aleatorias, sin estrategia clara")
print(f"  ‚Ä¢ 10-30%: Primeras estrategias b√°sicas emergen")
print(f"  ‚Ä¢ 30-60%: Mejora consistente, refinamiento")
print(f"  ‚Ä¢ 60-100%: Convergencia, pol√≠ticas estables")
print()

if steps_pct < 50:
    print(f"‚ö†Ô∏è NOTA: A2C est√° en fase temprana ({steps_pct:.1f}%). El aprendizaje mejorar√° conforme")
    print(f"   complete m√°s episodios. Las recompensas mostradas son PRELIMINARES.")
else:
    print(f"‚úÖ A2C ha pasado punto de convergencia. Estrategias estables esperadas.")

print()
print()
print("6Ô∏è‚É£ COMPARACI√ìN A2C vs OTROS AGENTES")
print("-" * 110)
print()

print("üìä Tabla Comparativa (Agentes en Etapas Diferentes):")
print()
print(f"{'Agente':<12} {'% Entrenado':<15} {'CO‚ÇÇ (kg)':<18} {'Recompensa':<18} {'Estado':<20}")
print("-" * 110)

for agent in ["A2C", "SAC", "PPO"]:
    result = results[agent]
    steps = result["steps"]
    max_steps = config_max_steps[agent]
    pct = (steps / max_steps) * 100
    
    if pct >= 99:
        status = "‚úÖ COMPLETO"
    elif pct >= 50:
        status = "üîÑ EN PROGRESO"
    else:
        status = "‚è≥ TEMPRANO"
    
    print(f"{agent:<12} {pct:>13.1f}% {result['carbon_kg']:>15,.0f} kg  {result['reward_total_mean']:>15.4f}    {status:<20}")

print()
print("‚ö†Ô∏è INTERPRETACI√ìN:")
print("  ‚Ä¢ SAC est√° al 1.9% (1,873 / 100,000 pasos)")
print("  ‚Ä¢ A2C est√° al 10.0% (8,759 / 87,600 pasos)")
print("  ‚Ä¢ PPO est√° al 0.0% (0 pasos, probablemente no ejecutado a√∫n)")
print()
print("  Los agentes en etapas diferentes NO son comparables directamente.")
print("  Esperar a que TODOS completen entrenamiento antes de evaluar ganador.")

print()
print()
print("7Ô∏è‚É£ RECOMENDACIONES")
print("-" * 110)
print()

print("üìã ACCIONES INMEDIATAS:")
print()
print("1. ‚úÖ A2C: Continuar entrenamiento hasta 87,600 pasos")
print(f"   Progreso actual: {a2c['steps']:,} / 87,600 ({steps_pct:.1f}%)")
print(f"   ETA completaci√≥n: ~{(87600 - a2c['steps']) / 100:.0f} minutos")
print()

print("2. ‚úÖ SAC: Reanudar o reiniciar entrenamiento")
print(f"   Progreso actual: {results['SAC']['steps']:,} / 100,000 (1.9%)")
print(f"   Usar: python -m scripts.continue_sac_training --config configs/default.yaml")
print()

print("3. ‚úÖ PPO: Iniciar entrenamiento")
print(f"   Progreso actual: {results['PPO']['steps']:,} / 438,000 (0.0%)")
print(f"   Usar: python -m scripts.run_oe3_simulate --config configs/default.yaml")
print()

print("üìä MONITOREO:")
print("  ‚Ä¢ Ver archivo 'co2_comparison.md' para tabla final")
print("  ‚Ä¢ Usar 'monitor_checkpoints.py' para seguimiento en tiempo real")
print("  ‚Ä¢ Checkpoints guardados en: outputs/oe3/checkpoints/<agent>/")
print()

print("=" * 110)
