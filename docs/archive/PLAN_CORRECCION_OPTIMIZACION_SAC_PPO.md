# ğŸ”§ PLAN DE CORRECCIÃ“N Y OPTIMIZACIÃ“N: SAC & PPO PRE-REENTRENAMIENTO

**Objetivo:** Diagnosticar, corregir y optimizar SAC y PPO antes de re-entrenamiento para hacer comparaciÃ³n JUSTA vs A2C

**Principio:** Los problemas detectados (+4.7% SAC, +0.08% PPO) son configuracionales, NO inherentes al algoritmo

---

## ğŸ” DIAGNÃ“STICO: PROBLEMAS IDENTIFICADOS

### SAC - Problema: Buffer Divergence (+4.7% peor)

#### Causa RaÃ­z Identificada:
```
âŒ PROBLEMA 1: Replay Buffer Size Insuficiente
   ConfiguraciÃ³n actual: buffer_size = 10,000
   Consecuencia: En 8,760 timesteps/episode Ã— 3 episodes
                = 26,280 total steps
                Buffer se llena rÃ¡pido
                Proporciones: viejo:nuevo = 60:40
                â†’ Experimencia old contamina learning

âŒ PROBLEMA 2: Learning Rate Muy Alto (2e-4)
   Consecuencia: Updates oscilan, no convergen
                Network "olvida" buenos patrones rÃ¡pido
                ExploraciÃ³n descontrolada

âŒ PROBLEMA 3: Entropy Coefficient Muy Bajo (0.2)
   Consecuencia: Insuficiente exploraciÃ³n
                Policy converge a locales subÃ³ptimos
                No descubre reglas de despacho complejas

âŒ PROBLEMA 4: No hay Prioritized Experience Replay (PER)
   Consecuencia: Todas las experiencias tienen peso igual
                Malas decisiones se repiten igual que buenas
                No hay focus en important transitions

âŒ PROBLEMA 5: Tau (polyak average) Muy Bajo (0.001)
   Consecuencia: Target networks actualizan muy rÃ¡pido
                Divergencia en multi-objetivo
                Loss oscillations
```

---

### PPO - Problema: Clip Too Restrictive (+0.08% neutral)

#### Causa RaÃ­z Identificada:
```
âŒ PROBLEMA 1: Clip Range Muy PequeÃ±o (0.2 = 20% mÃ¡x)
   ConfiguraciÃ³n actual: clip_range = 0.2
   Consecuencia: Policy change limitada a 20% por update
                En 3 episodes = acumulado ~60% mÃ¡ximo
                No es suficiente para estrategias radicales
                (ej: no cargar mediodÃ­a requiere 50%+ cambio)

âŒ PROBLEMA 2: N_steps Muy PequeÃ±o (2048 = 2.3 dÃ­as)
   ConfiguraciÃ³n actual: n_steps = 2048
   Consecuencia: Trajectory horizon < 1 semana
                No ve patrones solares semanales
                No conecta decisiones hora-8am con beneficio noche

âŒ PROBLEMA 3: Batch Size Insuficiente (64)
   ConfiguraciÃ³n actual: batch_size = 64
   Consecuencia: Gradients calculados sobre datos pequeÃ±os
                High variance en updates
                No hay suficiente data para convergencia

âŒ PROBLEMA 4: Learning Rate Muy Alto (3e-4)
   Consecuencia: Updates grandes + clip restricciÃ³n
                = learning paralizado
                Combo: quiere cambiar 50%, clip permite 20%
                       learning_rate hace los 20% demasiado abruptos

âŒ PROBLEMA 5: Entropy Coef No Configurado (default 0.0)
   Consecuencia: Sin exploraciÃ³n incentivada
                Policy converge a punto medio (neutral)
                No intenta estrategias diferentes

âŒ PROBLEMA 6: Normalize Advantage NO activado
   Consecuencia: Advantage values en escala incorrecta
                Algunas updates muy grandes, otras muy pequeÃ±as
                Inconsistent learning signal
```

---

## âœ… CORRECCIONES PROPUESTAS

### SAC - ConfiguraciÃ³n Optimizada

```python
# ANTES (Problema: Divergencia)
sac_config = {
    'buffer_size': 10_000,           # âŒ Insuficiente
    'learning_rate': 2e-4,           # âŒ Muy alto
    'tau': 0.001,                    # âŒ Muy bajo
    'ent_coef': 0.2,                 # âŒ Muy bajo
    'use_per': False,                # âŒ Sin Prioritized Replay
}

# DESPUÃ‰S (SoluciÃ³n: Estable)
sac_config_optimized = {
    # BUFFER: Aumentar capacity + agregar PER
    'buffer_size': 100_000,          # âœ… 10x mÃ¡s, suficiente para 3 episodes
    'use_prioritized_replay': True,  # âœ… NUEVO: Focus en important transitions
    'per_alpha': 0.6,                # âœ… Prioritization exponent
    'per_beta': 0.4,                 # âœ… Importance sampling exponent
    'per_epsilon': 1e-6,             # âœ… Min priority epsilon
    
    # LEARNING RATE: Reducir + usar scheduler
    'learning_rate': 5e-5,           # âœ… 4x menor (2e-4 â†’ 5e-5)
    'lr_schedule': 'linear_decay',   # âœ… NUEVO: Decay over episodes
    'lr_final': 1e-5,                # âœ… NUEVO: Final LR after decay
    
    # TARGET NETWORK: MÃ¡s estable
    'tau': 0.01,                     # âœ… 10x mayor (0.001 â†’ 0.01)
    'target_update_interval': 2,     # âœ… NUEVO: Update every 2 steps (not every step)
    
    # EXPLORATION: Mejor balanceado
    'ent_coef': 'auto',              # âœ… NUEVO: Auto-tune entropy
    'ent_coef_init': 0.5,            # âœ… NUEVO: Higher initial (0.2 â†’ 0.5)
    'ent_coef_lr': 1e-4,             # âœ… NUEVO: Learn entropy coefficient
    
    # NETWORK STABILITY
    'max_grad_norm': 1.0,            # âœ… NUEVO: Gradient clipping
    'net_arch': [512, 512],          # âœ… Larger networks for complex space (126 actions)
    'batch_size': 256,               # âœ… Larger batches for stability
}

# JUSTIFICACIÃ“N:
# 1. Buffer 100K: Con 26K steps/entrenamiento, tienes ~3.8x coverage
#    Ratio old:new â‰ˆ 30:70 (mejor mezcla)
# 2. PER: Enfoca en malas decisiones (cuando violarÃ­a prioridades)
# 3. LR 5e-5: Updates mÃ¡s pequeÃ±as, menos oscilaciÃ³n
# 4. Tau 0.01: Target network cambia mÃ¡s gradualmente
# 5. Ent auto: Explora cuando necesario, explota cuando encontrÃ³ buen patrÃ³n
```

---

### PPO - ConfiguraciÃ³n Optimizada

```python
# ANTES (Problema: Neutral, sin progreso)
ppo_config = {
    'clip_range': 0.2,               # âŒ Muy restrictivo
    'n_steps': 2048,                 # âŒ Muy pequeÃ±o
    'batch_size': 64,                # âŒ Muy pequeÃ±o
    'learning_rate': 3e-4,           # âŒ Muy alto para clip+batch combo
    'ent_coef': 0.0,                 # âŒ Sin exploraciÃ³n
    'normalize_advantage': False,    # âŒ Sin normalizaciÃ³n
}

# DESPUÃ‰S (SoluciÃ³n: Agresiva pero estable)
ppo_config_optimized = {
    # CLIP: Permitir cambios mÃ¡s grandes (pero seguros)
    'clip_range': 0.5,               # âœ… 2.5x mayor (0.2 â†’ 0.5)
                                     # Permite ~50% cambio policy/episode
    'clip_range_vf': 0.5,            # âœ… NUEVO: Value function also clipped
    
    # TRAJECTORY: Ver patrones mÃ¡s largos
    'n_steps': 8760,                 # âœ… UNA SEMANA COMPLETA (no 2.3 dÃ­as)
                                     # Full episode = full causal chains visible
    'gae_lambda': 0.98,              # âœ… High lambda para long-term advantages
    
    # BATCHES: MÃ¡s data, mejor gradients
    'batch_size': 256,               # âœ… 4x mayor (64 â†’ 256)
    'n_epochs': 10,                  # âœ… NUEVO: Multiple passes over data
    
    # LEARNING RATE: Reducir pero con decay
    'learning_rate': 1e-4,           # âœ… 3x menor (3e-4 â†’ 1e-4)
    'lr_schedule': 'linear_decay',   # âœ… NUEVO: Decay over episodes
    'max_grad_norm': 1.0,            # âœ… NUEVO: Gradient clipping
    
    # EXPLORATION: Incentivizar descubrimiento
    'ent_coef': 0.01,                # âœ… NUEVO: Small entropy bonus
    'target_kl': 0.02,               # âœ… NUEVO: KL divergence limit (safety)
    
    # ADVANTAGES: Normalizar para consistencia
    'normalize_advantage': True,     # âœ… NUEVO: Normalize within minibatches
    'use_sde': True,                 # âœ… NUEVO: State-Dependent Exploration
    'sde_sample_freq': -1,           # âœ… NUEVO: Sample every step
}

# JUSTIFICACIÃ“N:
# 1. Clip 0.5: Permite cambios ~50% (vs 20% antes)
#    + n_steps 8760: Policy change acumulada ~250% posible
#    = Suficiente para estrategias radicales
# 2. n_steps 8760: Full episode = ve conexiÃ³n 8amâ†’noche
# 3. Batch 256: Gradients mÃ¡s suaves, menos variance
# 4. LR 1e-4 + decay: Updates consistentes que disminuyen
# 5. Ent 0.01: PequeÃ±o bonus para explorar sin divergir
# 6. Normalize: Advantage values en escala [-1, 1] consistente
```

---

## ğŸ“‹ CAMBIOS A REALIZAR ANTES DE RE-ENTRENAMIENTO

### SAC - Checklist de Cambios

```
Archivo: src/iquitos_citylearn/oe3/agents/sac.py

CAMBIOS REQUERIDOS:

â˜ 1. Increase buffer_size
    LÃ­nea actual: buffer_size = 10_000
    Nueva: buffer_size = 100_000
    RazÃ³n: MÃ¡s capacity = menos contamination
    
â˜ 2. Add Prioritized Experience Replay
    LÃ­nea actual: (no existe)
    Nueva: 
        prioritized_replay_kwargs = {
            'alpha': 0.6,
            'beta': 0.4,
            'epsilon': 1e-6,
        }
    RazÃ³n: Focus en important transitions
    
â˜ 3. Reduce learning_rate
    LÃ­nea actual: learning_rate = 2e-4
    Nueva: learning_rate = 5e-5 (con decay schedule)
    RazÃ³n: Mejor convergence, menos oscilaciÃ³n
    
â˜ 4. Increase tau
    LÃ­nea actual: tau = 0.001
    Nueva: tau = 0.01
    RazÃ³n: MÃ¡s estable target networks
    
â˜ 5. Auto-tune entropy
    LÃ­nea actual: ent_coef = 0.2
    Nueva: ent_coef = 'auto', ent_coef_init = 0.5
    RazÃ³n: ExploraciÃ³n adaptiva
    
â˜ 6. Add gradient clipping
    LÃ­nea actual: (no existe)
    Nueva: max_grad_norm = 1.0
    RazÃ³n: Prevenir divergencia
    
â˜ 7. Increase network architecture
    LÃ­nea actual: net_arch = [256, 256]
    Nueva: net_arch = [512, 512]
    RazÃ³n: Mayor capacidad para 126 acciones
    
â˜ 8. Increase batch_size
    LÃ­nea actual: batch_size = 64
    Nueva: batch_size = 256
    RazÃ³n: Mejor gradients, less variance
```

### PPO - Checklist de Cambios

```
Archivo: src/iquitos_citylearn/oe3/agents/ppo_sb3.py

CAMBIOS REQUERIDOS:

â˜ 1. Increase clip_range
    LÃ­nea actual: clip_range = 0.2
    Nueva: clip_range = 0.5
    RazÃ³n: Permitir cambios policy mÃ¡s grandes
    
â˜ 2. Set full episode n_steps
    LÃ­nea actual: n_steps = 2048
    Nueva: n_steps = 8760
    RazÃ³n: Ver patrones solares completos (dÃ­a completo)
    
â˜ 3. Increase batch_size
    LÃ­nea actual: batch_size = 64
    Nueva: batch_size = 256
    RazÃ³n: Mejor gradient estimation
    
â˜ 4. Add multiple epochs
    LÃ­nea actual: n_epochs = 3
    Nueva: n_epochs = 10
    RazÃ³n: MÃºltiples passes para convergencia
    
â˜ 5. Reduce learning_rate
    LÃ­nea actual: learning_rate = 3e-4
    Nueva: learning_rate = 1e-4 (con decay)
    RazÃ³n: Menos oscilaciÃ³n + decay schedule
    
â˜ 6. Add gradient clipping
    LÃ­nea actual: (no existe)
    Nueva: max_grad_norm = 1.0
    RazÃ³n: Estabilidad
    
â˜ 7. Add entropy bonus
    LÃ­nea actual: ent_coef = 0.0
    Nueva: ent_coef = 0.01
    RazÃ³n: Incentivizar exploraciÃ³n
    
â˜ 8. Enable advantage normalization
    LÃ­nea actual: normalize_advantage = False
    Nueva: normalize_advantage = True
    RazÃ³n: Consistency en learning signal
    
â˜ 9. Add State-Dependent Exploration
    LÃ­nea actual: (no existe)
    Nueva: use_sde = True, sde_sample_freq = -1
    RazÃ³n: ExploraciÃ³n mÃ¡s informada
    
â˜ 10. Add KL divergence safety limit
    LÃ­nea actual: (no existe)
    Nueva: target_kl = 0.02
    RazÃ³n: Prevenir policy changes demasiado radicales
```

---

## ğŸ”„ PROCESO DE RE-ENTRENAMIENTO (Orden CrÃ­tico)

### Fase 1: PreparaciÃ³n (ANTES de train)

```
PASO 1: Backup cÃ³digo actual
  $ git commit -m "Backup: Pre-optimization SAC/PPO"
  $ git branch pre-optimization

PASO 2: Hacer TODOS los cambios de cÃ³digo
  â˜ Update SAC config (8 cambios)
  â˜ Update PPO config (10 cambios)
  â˜ Update A2C baseline (sin cambios, es referencia)
  
PASO 3: Validar cambios sintÃ¡cticos
  $ python -m py_compile src/iquitos_citylearn/oe3/agents/sac.py
  $ python -m py_compile src/iquitos_citylearn/oe3/agents/ppo_sb3.py
  $ pylint src/iquitos_citylearn/oe3/agents/
  
PASO 4: Commit cambios
  $ git add -A
  $ git commit -m "Config: Optimize SAC/PPO pre-training
  
  SAC changes:
  - buffer_size 10Kâ†’100K
  - PER enabled
  - LR 2e-4â†’5e-5 (decay)
  - tau 0.001â†’0.01
  - ent_coef auto
  - max_grad_norm 1.0
  - net_arch [256,256]â†’[512,512]
  - batch_size 64â†’256
  
  PPO changes:
  - clip_range 0.2â†’0.5
  - n_steps 2048â†’8760 (FULL EPISODE)
  - batch_size 64â†’256
  - n_epochs 3â†’10
  - LR 3e-4â†’1e-4 (decay)
  - max_grad_norm 1.0
  - ent_coef 0.0â†’0.01
  - normalize_advantage True
  - use_sde True
  - target_kl 0.02"

PASO 5: Crear documento de cambios
  âœ… Archivo: CAMBIOS_PREPROCESAMIENTO_SAC_PPO.md
     (listar cada cambio con justificaciÃ³n)
```

### Fase 2: Re-entrenamiento (Training Loop)

```
PASO 6: Build fresh dataset
  $ python -m scripts.run_oe3_build_dataset --config configs/default.yaml
  
PASO 7: Run baseline (sin cambios, para comparaciÃ³n)
  $ python -m scripts.run_uncontrolled_baseline --config configs/default.yaml
  
PASO 8: Train SAC (NUEVO - optimizado)
  $ python -m scripts.run_oe3_train_agent --agent SAC --episodes 3 --config configs/default.yaml
  Esperar: ~30 min (GPU RTX 4060)
  
PASO 9: Train PPO (NUEVO - optimizado)
  $ python -m scripts.run_oe3_train_agent --agent PPO --episodes 3 --config configs/default.yaml
  Esperar: ~20 min
  
PASO 10: Train A2C (SIN CAMBIOS - referencia)
  $ python -m scripts.run_oe3_train_agent --agent A2C --episodes 3 --config configs/default.yaml
  Esperar: ~25 min
  
PASO 11: ComparaciÃ³n tres agentes
  $ python -m scripts.run_oe3_co2_table --config configs/default.yaml
```

### Fase 3: ValidaciÃ³n y DocumentaciÃ³n

```
PASO 12: Capture resultados
  Archivo: outputs/oe3_simulations/
    - SAC_results_optimized.json
    - PPO_results_optimized.json
    - A2C_results_reference.json

PASO 13: Comparar resultados
  SAC (ANTES):  +4.7%  COâ‚‚
  SAC (DESPUÃ‰S): ???%  COâ‚‚  (deberÃ­a ser -10% a -15%)
  
  PPO (ANTES):  +0.08% COâ‚‚
  PPO (DESPUÃ‰S): ???%  COâ‚‚  (deberÃ­a ser -15% a -20%)
  
  A2C (REFERENCIA): -25.1% COâ‚‚  (sin cambios)

PASO 14: Documentar hallazgos
  Crear: RESULTADOS_REENTRENAMIENTO_SAC_PPO.md
    - MÃ©tricas antes/despuÃ©s
    - GrÃ¡ficas convergencia
    - AnÃ¡lisis de mejora
    - Razones por quÃ© mejoraron

PASO 15: Final commit
  $ git commit -m "Results: SAC/PPO Optimized Re-training Results
  
  SAC optimized:
  - COâ‚‚: [X]% (vs +4.7% antes)
  - Grid import: [Y] kWh
  - Convergence: [Z] episodes
  
  PPO optimized:
  - COâ‚‚: [X]% (vs +0.08% antes)
  - Grid import: [Y] kWh
  - Convergence: [Z] episodes
  
  A2C reference (sin cambios):
  - COâ‚‚: -25.1%
  - Grid import: 9,467,195 kWh"
```

---

## ğŸ“Š MÃ‰TRICAS DE COMPARACIÃ“N JUSTA

DespuÃ©s de re-entrenamiento con configuraciones optimizadas:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ MÃ‰TRICA                    SAC ANTES  SAC OPT   PPO ANTES PPO OPT  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ COâ‚‚ ReducciÃ³n              +4.7%     ??? âœ“     +0.08%    ??? âœ“     â•‘
â•‘ (Objetivo: <-20%)          âŒ        âœ“?        âŒ        âœ“?       â•‘
â•‘                                                                     â•‘
â•‘ Grid Import (kWh/aÃ±o)      +597k    -200k?     +10k      -500k?   â•‘
â•‘ (Objetivo: < 10M)          âŒ        âœ“?        âœ“âš ï¸       âœ“âœ“      â•‘
â•‘                                                                     â•‘
â•‘ EVs sin Grid (%)           75%      85%?       93%       95%?     â•‘
â•‘ (Objetivo: >90%)           âŒ        âœ“?        âœ“         âœ“âœ“       â•‘
â•‘                                                                     â•‘
â•‘ Training Convergence       oscillate converge? flat     accelerate?â•‘
â•‘ (Objetivo: smooth)         âŒ        âœ“?        âš ï¸        âœ“âœ“       â•‘
â•‘                                                                     â•‘
â•‘ Buffer Divergence (SAC)    SÃ        MENOS?    N/A      N/A       â•‘
â•‘ (Objetivo: NO)             âŒ        âœ“?        N/A      N/A       â•‘
â•‘                                                                     â•‘
â•‘ Policy Clip Restriction    N/A      N/A       SÃ­        MENOS?    â•‘
â•‘ (Objetivo: flexible)       N/A      N/A       âŒ        âœ“?       â•‘
â•‘                                                                     â•‘
â•‘ Multi-Objective Balance    Falso    Mejor?    Conservative Better? â•‘
â•‘ (Objetivo: 5 objetivos OK) âŒ        âœ“?        âš ï¸        âœ“âœ“       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPECTATIVAS REALISTAS:
  SAC Optimizado: DeberÃ­a llegar a -10% a -15% COâ‚‚
    - PER + Buffer mÃ¡s grande = mejor stability
    - Auto-entropy = exploraciÃ³n balanceada
    - Pero: Off-policy inherentemente tiene limitaciones
  
  PPO Optimizado: DeberÃ­a llegar a -15% a -20% COâ‚‚
    - Clip 0.5 + n_steps 8760 = suficiente flexibilidad
    - Batch 256 + normalize = gradients consistentes
    - Pero: AÃºn puede estar conservador vs A2C
  
  A2C Referencia: Mantiene -25.1% (es el baseline)
```

---

## âœ… VERIFICACIÃ“N ANTES DE ENTRENAR

### Checklist Final (CRÃTICO):

```
â˜ CÃ“DIGO:
  â˜ SAC config: 8/8 cambios implementados
  â˜ PPO config: 10/10 cambios implementados
  â˜ Archivos compilan sin errores (pylint 0)
  â˜ Imports correctos
  â˜ No hay deprecated functions

â˜ CONFIGURACIÃ“N:
  â˜ configs/default.yaml actualizado (si necesario)
  â˜ Dataset limpio (backup viejo antes de rebuild)
  â˜ Checkpoints vaciados (para clean start)
  â˜ GPU disponible y funcionando

â˜ DOCUMENTACIÃ“N:
  â˜ Documento de cambios creado (CAMBIOS_PREENTRENAMIENTO_SAC_PPO.md)
  â˜ JustificaciÃ³n de cada cambio escrita
  â˜ Expected results documentados
  â˜ ComparaciÃ³n baseline documentada

â˜ GIT:
  â˜ Branch creado: git checkout -b oe3-sac-ppo-optimization
  â˜ Cambios committeados
  â˜ Backup pre-optimization guardado
  â˜ Remote actualizado

â˜ MONITOREO:
  â˜ Script de monitoreo listo
  â˜ Logs guardados con timestamp
  â˜ Checkpoints monitoreados
  â˜ Early stopping configurado (si falla)
```

---

## ğŸ¯ CONCLUSIÃ“N: CRITERIO DE Ã‰XITO

DespuÃ©s de re-entrenamiento con configuraciones optimizadas:

```
âœ… SAC es exitoso si:
   - COâ‚‚: De +4.7% â†’ es decir â‰¥ -10%
   - EVs sin grid: De 75% â†’ â‰¥ 85%
   - Convergencia: smooth vs oscillating
   - ExplicaciÃ³n: "Buffer divergence corregida"

âœ… PPO es exitoso si:
   - COâ‚‚: De +0.08% â†’ â‰¥ -15%
   - EVs sin grid: De 93% â†’ â‰¥ 95%
   - Convergencia: acelerada vs flat
   - ExplicaciÃ³n: "Clip permitiÃ³ estrategias complejas"

âœ… A2C mantiene referencia:
   - COâ‚‚: -25.1% (sin cambios)
   - EVs sin grid: 95%
   - Convergencia: continua
   - ExplicaciÃ³n: "Confirma que A2C es Ã³ptimo"

ENTONCES: ComparaciÃ³n JUSTA se puede hacer
           porque todos tienen configuraciones Ã³ptimas
```

Este plan asegura que NO descartes SAC/PPO por problemas tÃ©cnicos, sino que los CORRIJAS y luego los compares equitativamente con A2C.
