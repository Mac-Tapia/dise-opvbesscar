# üìä SESSION SUMMARY: SAC Learning Rate & Reward Engineering

**Sesi√≥n**: 2026-01-18 (18:58 - 19:20)
**Commits**: 4 (488bb413, 1bc4ff9c, 3d41ca7f, 1bc4ff9c)
**Status**: üü¢ TWO CRITICAL BUGS FIXED - SAC Ready to Learn

---

## Executive Summary

**Problema**: SAC no estaba aprendiendo (reward plano 0.56 por 500+ pasos).

**Causa Ra√≠z**: DOS bugs independientes:

1. **Learning rate capped a 3e-05** (100x demasiado bajo)
2. **Reward mal escalada** (baselines irrealistas, pesos desbalanceados)

**Soluci√≥n**:

- ‚úÖ Removida limitaci√≥n de LR (ahora 0.001)
- ‚úÖ Rebalanceados pesos multiobjetivo (CO‚ÇÇ 0.50, Solar 0.20)
- ‚úÖ Baselines CO‚ÇÇ realistas (130/250 vs 500)
- ‚úÖ SOC penalty normalizada
- ‚úÖ Entrop√≠a SAC reducida

**Impacto**: SAC ahora tiene **claros gradientes** para aprender. Esperado
mejora en pr√≥ximo entrenamiento: r_co2 de -0.05 a +0.30+.

---

## Problema 1: Learning Rate (CR√çTICO)

### Identificaci√≥n

<!-- markdownlint-disable MD013 -->
```text
SAC paso 500: lr=3.00e-05 (mostrado en logs)
Config YAML: learning_rate: 0.001
Factor: 33.3x m√°s bajo
Impacto: Gradientes diminutos, convergencia IMPOSIBLE
```text
<!-- markdownlint-enable MD013 -->

### Root Cause

**Archivo**: [src/iquitos_citylearn/oe3/agents/sac.py][ref]

[ref]: src/iquitos_citylearn/oe3/agents/sac.py#L661

<!-- markdownlint-disable MD013 -->
```python
# ‚ùå ANTES
stable_lr = mi...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

**Por qu√©**: C√≥digo antiguo de "estabilidad conservadora" que NUNCA se removi√≥.

### Soluci√≥n

<!-- markdownlint-disable MD013 -->
```python
# ‚úÖ DESPU√âS
stable_lr = self.config.learning_rate        # Usar config: 0.001
stable_batch = self.config.batch_size        # Usar config: 32,768
```text
<!-- markdownlint-enable MD013 -->

### Commit

- `488bb413`: Fix SAC learning rate & batch size caps

### Validaci√≥n

<!-- markdownlint-disable MD013 -->
```text
Antes:  step 500, lr=3.00e-05, reward_avg=0.5600 (PLANO)
Ahora:  step 500, lr=1.00e-...
```

[Ver c√≥digo completo en GitHub]python
# ‚ùå ANTES: co2_baseline = 500.0 (arbitrario)
# Resultado: reward casi siempre 1 - 2*(100/500) = 0.6+ (sin variaci√≥n)

# ‚úÖ AHORA:
co2_baseline_offpeak = 130.0   # Mall 100kW + Chargers 30kW = 130 kWh/hora
co2_baseline_peak = 250.0      # Mall 150kW + Chargers 100kW = 250 kWh/hora
```text
<!-- markdownlint-enable MD013 -->

**Impacto**: Reward ahora var√≠a en rango [-1, +1] completo.

#### 3. SOC Penalty Normalizada

<!-- markdownlint-disable MD013 -->
```python
# ‚ùå ANTES: soc_penalty sumada directamente, sin peso
# Resultado: SOC penalty = CO‚ÇÇ penalty en magnitude

# ‚úÖ AHORA: (2)
reward = (
    0.50 * r_co2 +
    0.10 * soc_penalty +  # Ponderado expl√≠citamente
    ...
)
```text
<!...
```

[Ver c√≥digo completo en GitHub]python
# ‚ùå ANTES: ent_coef="auto", target_entropy=-126.0 (alta exploraci√≥n)
# AHORA:   ent_coef=0.01 (fijo), target_entropy=-50.0 (baja exploraci√≥n)
```text
<!-- markdownlint-enable MD013 -->

**Beneficio**: Con rewards bien escaladas, SAC puede EXPLOTAR buenas acciones
en lugar de explorar ruido.

---

## Commits History

<!-- markdownlint-disable MD013 -->
```bash
1bc4ff9c - Documentation: TIER1 fixes summary and audit results
<details>
<summary>3d41ca7f - TIER 1: Reward rebalance (CO2=0.50, Solar=0.20), CO2 baselines fix (1...</summary>

3d41ca7f - TIER 1: Reward rebalance (CO2=0.50, Solar=0.20), CO2 baselines fix (130/250 vs 500), SOC penalty normalized, entr...

</details>
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

---

## Archivos Modificados

### Core Changes

- [src/iquitos_citylearn/oe3/rewards.py](src/iquitos_citylearn/oe3/rewards.py)
  - Lines 30-45: Pesos (0.50, 0.20, 0.10, 0.10, 0.10)
  - Lines 152-165: Baselines (130/250)
  - Lines 215-235: SOC penalty ponderada

- [src/iquitos_citylearn/oe3/agents/sac.py][url1]
  - Lines 136-138: Entrop√≠a (0.01, -50)
  - Lines 659-668: LR & batch no capped

### Documentation

- [SAC_LEARNING_RATE_FIX_REPORT.md](SAC_LEARNING_RATE_FIX_REPORT.md)
- [AUDIT_REWARDS_OBSERVABLES_HYPERPARAMS.md][url2]
- [TIER1_FIXES_SUMMARY.md](TIER1_FIXES_SUMMARY.md)

---

## Expected Improvements

### TIER 1 (NOW APPLIED)

- ‚úÖ Learning rate cap removed (3e-05 ‚Üí 0.001)
- ‚úÖ Reward baselines realistic (500 ‚Üí 130/250)
- ‚úÖ Weight balance corrected (0.45 ‚Üí 0.50 CO‚ÇÇ)
- ‚úÖ Entropy reduced (auto ‚Üí 0.01)

**Esperado**: r_co2 mejore de -0.05 a +0.30 por step 500.

### TIER 2 (PENDING TIER 1 VALIDATION)

- ‚è≥ Reward normalization (rolling mean/std)
- ‚è≥ Observable augmentation (is_peak, pv_available, queues)
- ‚è≥ Batch size tuning (32768 ‚Üí 4096)

---

## Next Actions

1. **Monitor Current Training** (terminal b0dc12af)
   - Esperar SAC phase (baseline deber√≠a completar ~19:35)
   - Monitorear r_co2 trend (debe subir, no plano)

2. **Validate TIER 1** (~20:00)
   - Si r_co2 sube: TIER 1 successful, proceed to TIER 2
   - Si r_co2 plano: hay otro bug, debug necesario

3. **Document Results** (despu√©s)
   - Crear comparativa SAC (old vs new weights)
   - Metrics: r_co2, grid_import, bess_soc trends

---

## Technical Details

### Why Learning Rate Was Capped

Historical issue from early SAC implementation (>6 months old). Code
comment:

<!-- markdownlint-disable MD013 -->
```python
# Learning rate M√ÅS conservador para estabilidad  ‚Üê OUTDATED
stable_lr = min(self.config.learning_rate, 3e-5)
```text
<!-- markdownlint-enable MD013 -->

**Lesson**: Always review hardcoded `min()` and `max()` in critical RL loops.

### Why CO‚ÇÇ Baseline Was Wrong

Default value (500.0) was based on earlier **testing** with higher loads.
Production Iquitos:

- Typical mall demand: ~100 kW baseline
...
```

[Ver c√≥digo completo en GitHub]text
r_co2 contribution: 0.50 * (-0.5) = -0.25
r_soc contribution: 1.0 * (-0.5) = -0.50  ‚Üê 2x HEAVIER!
```text
<!-- markdownlint-enable MD013 -->

---

## Rollback Plan (If Needed)

If TIER 1 causes issues:

<!-- markdownlint-disable MD013 -->
```bash
git revert 488bb413  # Revert LR fix
git revert 3d41ca7f  # Revert rewards
# Return to commit 84a62ae9
```text
<!-- markdownlint-enable MD013 -->

But **strongly expect TIER 1 to improve**, not regress.

---

## Conclusion

**Two blocking bugs fixed**:

1. Learning rate 100x too low
2. Reward signal not scaled properly

**Next 4-5 hours**: SAC should show **clear learning trend** (r_co2 increasing
from -0.05 to +0.30+).

**If successful**: Proceed to TIER 2 (observables, batch tuning, normalization).

---

**Status**: üü¢ READY FOR TIER 1 VALIDATION
**ETA SAC Phase Start**: ~19:35 (waiting for baseline to complete ~17 min)
**Expected Result Time**: ~23:50 (4-5 hours total)

[url1]: src/iquitos_citylearn/oe3/agents/sac.py
[url2]: AUDIT_REWARDS_OBSERVABLES_HYPERPARAMS.md