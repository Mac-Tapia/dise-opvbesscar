# ğŸ” ANÃLISIS DE DUPLICACIÃ“N Y PLAN DE CONSOLIDACIÃ“N

**Fecha**: 2026-02-14  
**Estado**: AnÃ¡lisis llevado a cabo

---

## 1. SITUACIÃ“N ACTUAL

### Estructura Duplicada
```
VIEJO: src/citylearnv2/dataset_builder/
â”œâ”€ dataset_builder.py (2,701 LOC) - GRAN builder monolÃ­tico
â”‚  â”œâ”€ Carga datos OE2
â”‚  â”œâ”€ Valida datasets
â”‚  â”œâ”€ Generaliza columnas  
â”‚  â”œâ”€ Crea environment CityLearn
â”‚  â””â”€ Tracking COâ‚‚ directo + indirecto
â”œâ”€ data_loader.py (53 LOC) - Solo re-exports de dataset_builder
â”œâ”€ rewards.py - Funciones de recompensa (USADO)
â”œâ”€ progress.py - Progreso/grÃ¡ficos (USADO)
â”œâ”€ metrics_extractor.py - MÃ©tricas de episodios (USADO)
â”œâ”€ transition_manager.py - GestiÃ³n transiciones (USADO)
â”œâ”€ fixed_schedule.py - Baseline agent (USADO)
â””â”€ __init__.py - Exports

NUEVO: src/dataset_builder_citylearn/
â”œâ”€ enrich_chargers.py - Enriquecimiento CHARGERS
â”œâ”€ integrate_datasets.py - IntegraciÃ³n SOLAR
â”œâ”€ analyze_datasets.py - AnÃ¡lisis
â”œâ”€ catalog_datasets.py - CatÃ¡logo de metadatos (350+ LOC)
â”œâ”€ main_build_citylearn.py - Orquestador
â”œâ”€ __init__.py - Exports catÃ¡logo
â””â”€ README.md
```

### PropÃ³sitos Diferentes
```
VIEJO (src/citylearnv2/dataset_builder/):
â”œâ”€ PropÃ³sito: Cargar datos OE2 y crear CityLearn environment
â”œâ”€ Entrada: CSV raw de OE2
â”œâ”€ Salida: Environment listo para agentes RL
â””â”€ Usuarios: SAC, PPO, A2C agents

NUEVO (src/dataset_builder_citylearn/):
â”œâ”€ PropÃ³sito: Enriquecer y catalogar datos OE2
â”œâ”€ Entrada: CSV raw de OE2
â”œâ”€ Salida: CSV enriquecidos + catÃ¡logo de metadatos
â””â”€ Usuarios: OE2 documentation, future OE3 integrations
```

---

## 2. ANÃLISIS DE DEPENDENCIAS ACTIVAS

### Â¿QuÃ© se importa del VIEJO en cÃ³digo activo?
âœ… **USADO en agentes**:
- `rewards.py` â†’ en `agents/__init__.py`, `agents/rbc.py`
- `progress.py` â†’ en `agents/sac.py`, `agents/ppo_sb3.py`, `agents/a2c_sb3.py`
- `metrics_extractor.py` â†’ dynamic import en agents (sac, ppo, a2c)
- `transition_manager.py` â†’ re-export en `agents/transition_manager.py`
- `fixed_schedule.py` â†’ re-export en `agents/fixed_schedule.py`

âœ… **USADO en baselines**:
- `build_citylearn_dataset` â†’ en `src/baseline/example_agent_training_with_baseline.py`

â“ **AMBIGUO** (posiblemente obsoleto):
- `dataset_builder.py` (2,701 LOC) - MonolÃ­tico, Â¿se usa directamente?
- `data_loader.py` (53 LOC) - Solo re-exports, posible duplicaciÃ³n

### Â¿QuÃ© se importa del NUEVO?
âŒ **NO USADO todavÃ­a**:
- `catalog_datasets.py` - Creado recientemente
- `main_build_citylearn.py` - Orquestador nuevo
- NingÃºn agente lo usa aÃºn

---

## 3. PROBLEMAS IDENTIFICADOS

| # | Problema | Impacto | Severidad |
|---|----------|--------|-----------|
| 1 | Dos builders: `dataset_builder.py` (VIEJO) vs `main_build_citylearn.py` (NUEVO) | ConfusiÃ³n sobre cuÃ¡l usar | ğŸ”´ Alta |
| 2 | No hay una Ãºnica "source of truth" de datasets enriquecidos | Riesgo de desincronizaciÃ³n | ğŸ”´ Alta |
| 3 | `dataset_builder.py` tiene 2,701 LOC (monolÃ­tico) | DifÃ­cil mantener y debuggear | ğŸŸ¡ Media |
| 4 | No estÃ¡ claro quÃ© datasets genera VIEJO vs NUEVO | Pipeline confuso | ğŸŸ¡ Media |
| 5 | CatÃ¡logo NUEVO no estÃ¡ integrado en pipeline de agentes | Metadatos no usados | ğŸŸ  Baja |

---

## 4. PLAN DE CONSOLIDACIÃ“N

### FASE 1: Unificar construcciÃ³n de datasets
**Objetivo**: Una Ãºnica ruta de construcciÃ³n, desde datos raw a datasets listos

```
Paso 1a: Expandir NUEVO builder
â”œâ”€ Mover lÃ³gica de carga OE2 de VIEJO â†’ NUEVO
â”œâ”€ Mover lÃ³gica de validaciÃ³n de VIEJO â†’ NUEVO
â”œâ”€ Mover constantes OSINERGMIN/COâ‚‚ a NUEVO
â””â”€ Resultado: NUEVO puede hacer TODO lo que VIEJO hace para construcciÃ³n

Paso 1b: Actualizar VIEJO data_loader
â”œâ”€ data_loader.py ahora lee desde catÃ¡logo (NUEVO)
â”œâ”€ En lugar de hardcodear rutas, usa get_dataset(id)
â”œâ”€ Los constantes COâ‚‚/tarifa vienen del catÃ¡logo
â””â”€ Resultado: VIEJO solo orquesta, NUEVO proporciona datos

Paso 1c: Reducir VIEJO dataset_builder.py
â”œâ”€ Eliminar lÃ³gica de enriquecimiento (ahora en NUEVO)
â”œâ”€ Eliminar lÃ³gica de validaciÃ³n OE2 (ahora en NUEVO)
â”œâ”€ Mantener solo: ConstrucciÃ³n de CityLearn environment
â”œâ”€ Nuevo tamaÃ±o estimado: ~800 LOC (70% reducciÃ³n)
â””â”€ Resultado: dataset_builder.py mÃ¡s enfocado
```

### FASE 2: Integrar catÃ¡logo en pipeline
**Objetivo**: Los agentes usan datos desde catÃ¡logo

```
Paso 2a: Actualizar agents/__init__.py
â”œâ”€ En lugar de importar rewards de VIEJO
â”œâ”€ Importar rewards DEL VIEJO pero con factores del catÃ¡logo
â””â”€ Resultado: Agentes usan factores centralizados

Paso 2b: Actualizar baseline
â”œâ”€ En lugar de llamar build_citylearn_dataset(raw paths)
â”œâ”€ Llamar build_citylearn_dataset(catalog_paths)
â””â”€ Resultado: Baselines tambiÃ©n usan datos enriquecidos

Paso 2c: DocumentaciÃ³n del flujo
â”œâ”€ Crear PIPELINE.md con diagrama claro
â”œâ”€ Paso 1: python -m src.dataset_builder_citylearn.main_build_citylearn
â”œâ”€ Paso 2: python -m src.agents.sac (carga desde catÃ¡logo)
â””â”€ Resultado: Usuarios entienden quÃ© ejecutar y en quÃ© orden
```

### FASE 3: Limpieza
**Objetivo**: Eliminar cÃ³digo duplicado

```
Paso 3a: Identificar archivos descartables
â”œâ”€ Si dataset_builder.py reducido < 500 LOC: mantener en VIEJO
â”œâ”€ Si > 500 LOC: considerar mover a NUEVO
â””â”€ Resultado: Claridad sobre quÃ© goes where

Paso 3b: Eliminar duplicados
â”œâ”€ Â¿enrich_chargers.py y lÃ³gica en VIEJO son iguales?
â”œâ”€ Â¿integrate_datasets.py duplica VIEJO? 
â””â”€ Resultado: Una sola implementaciÃ³n por feature

Paso 3c: Consolidar exports
â”œâ”€ NUEVO.__init__.py: Exporta todo lo necesario (catÃ¡logo, builder, etc)
â”œâ”€ VIEJO.__init__.py: Importa desde NUEVO donde sea relevante
â””â”€ Resultado: Clear API surface
```

---

## 5. PLAN DE EJECUCIÃ“N RECOMENDADO

### OpciÃ³n A: RÃ¡pida (MÃ­nimo riesgo)
```
1. Dejar VIEJO como estÃ¡ (funciona con agentes)
2. NUEVO genera datos enriquecidos + catÃ¡logo
3. Crear documento de flujo: "Siempre ejecuta NUEVO builder primero"
4. Agentes continÃºan usando VIEJO (sin cambios)
5. Tiempo: 1 hora
6. Riesgo: Bajo (sin cambios a cÃ³digo activo)
```

### OpciÃ³n B: Moderada (Unificar data_loader)
```
1. Actualizar VIEJO.data_loader para leer del catÃ¡logo (NUEVO)
2. Mantener todo lo demÃ¡s igual
3. Resultado: CatÃ¡logo es source of truth
4. Tiempo: 3-4 horas
5. Riesgo: Medio (cambio a data_loader, usado por todos)
```

### OpciÃ³n C: Completa (ReestructuraciÃ³n)
```
1. Mover lÃ³gica OE2 de VIEJO â†’ NUEVO
2. Reducir dataset_builder.py a solo CityLearn construction
3. Unificar imports en todos los agents
4. Eliminar duplicados
5. Tiempo: 2-3 dÃ­as
6. Riesgo: Alto (refactor grande)
```

---

## 6. MI RECOMENDACIÃ“N

**OpciÃ³n B (Moderada)** - Mejor balance:

1. **Bajo riesgo**: Solo cambio en `data_loader.py` lineal
2. **Alto impacto**: CatÃ¡logo es source of truth
3. **Mantenible**: VIEJO sigue siendo "utilidades", NUEVO es "datos"
4. **Documentable**: Flujo claro: NUEVO genera â†’ VIEJO consume
5. **Tiempo razonable**: 3-4 horas de trabajo

### Pasos especÃ­ficos:
1. Actualizar `src/citylearnv2/dataset_builder/data_loader.py`:
   - En lugar de cargar paths HARDCODED
   - Usar `get_dataset()` del catÃ¡logo NUEVO
   - Leer constantes COâ‚‚/tarifa del catÃ¡logo

2. Crear `docs/DATA_PIPELINE_FLOW.md`:
   - Diagrama: NUEVO â†’ (dataset enriquecidos) â†’ VIEJO â†’ (agents)
   - Comandos: quÃ© ejecutar y en quÃ© orden
   - Validaciones: verificar integridad

3. Actualizar docstrings en:
   - `agents/__init__.py`
   - `baseline/`
   - README.md

4. Test end-to-end:
   - Ejecutar NUEVO builder
   - Entrenar SAC
   - Verificar que usa datos enriquecidos

---

## 7. DETALLES DE IMPLEMENTACIÃ“N (OpciÃ³n B)

### Cambio en `data_loader.py`

**ANTES**:
```python
DEFAULT_SOLAR_PATH = Path("data/oe2/Generacionsolar/pv_generation_citylearn2024.csv")
DEFAULT_CHARGERS_PATH = Path("data/oe2/chargers/chargers_ev_ano_2024_v3.csv")

def load_solar_data(path=None):
    path = path or DEFAULT_SOLAR_PATH
    return pd.read_csv(path)
```

**DESPUÃ‰S**:
```python
from src.dataset_builder_citylearn.catalog_datasets import get_dataset

def load_solar_data(path=None):
    if path:
        return pd.read_csv(path)
    # Usar catÃ¡logo como default
    solar_catalog = get_dataset("SOLAR_v2")
    return pd.read_csv(solar_catalog.path)

def load_chargers_data(path=None):
    if path:
        return pd.read_csv(path)
    # Usar catÃ¡logo como default
    chargers_catalog = get_dataset("CHARGERS_v2")
    return pd.read_csv(chargers_catalog.path)
```

### Beneficios:
- CatÃ¡logo es source of truth
- Paths resueltos automÃ¡ticamente
- Columnas verificadas por catÃ¡logo
- FÃ¡cil debuggear si hay problemas

---

## 8. DECISIÃ“N REQUERIDA

**Â¿CuÃ¡l opciÃ³n prefieres?**

A. **RÃ¡pida** (NUEVO genera, VIEJO igual)  
B. **Moderada** (VIEJO usa catÃ¡logo del NUEVO) â† RECOMENDADA  
C. **Completa** (reestructuraciÃ³n grande)

---

**PrÃ³ximo paso**: Una vez decidido, implemento el plan.
