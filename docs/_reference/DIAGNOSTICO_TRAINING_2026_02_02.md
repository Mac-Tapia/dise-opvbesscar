# üîç DIAGN√ìSTICO: Entrenamiento SAC - 2026-02-02

## üìä Hallazgos de los Logs

### Par√°metros de Log Analizado
```
[INFO] [SAC] paso 11500 | ep~2 | global_step=11500 | reward_avg=17.8233 | 
actor_loss=-9927.18 | critic_loss=20273.58 | ent_coef=0.5851 | 
grid_kWh=2281666.7 | solar_kWh=2399954.2 | co2_grid=1031541.5 | 
co2_indirect=1085019.3 | co2_direct=294109.3 | motos=54820 | mototaxis=8223
```

---

## ‚úÖ PROBLEMAS IDENTIFICADOS Y CORREGIDOS

### 1. **üî¥ CR√çTICO: Reward Escalado √ó 100 (FIXED)**

**Problema:** 
- L√≠nea 736 en `sac.py`: `reward_val = float(r) * 100.0`
- Causaba: `reward_avg=17.8233` en lugar de `~0.178`
- Impacto: Distorsiona la visualizaci√≥n y el aprendizaje

**Soluci√≥n Aplicada:**
```python
# ANTES (INCORRECTO):
reward_val = float(r) * 100.0  # Escalado incorrecto

# DESPU√âS (CORRECTO):
reward_val = float(r)  # Sin escalado, mantener original
```

**Resultado Esperado Post-Fix:**
- `reward_avg` ser√° ~0.178 (normalizado entre -1 y 1)
- Refleja correctamente la recompensa multiobjetivo

---

### 2. **üü° ATENCI√ìN: Actor/Critic Losses Exploding**

**S√≠ntomas Observados:**
- `actor_loss=-9927.18` (enormemente negativo)
- `critic_loss=20273.58` (enormemente positivo)
- Indica inestabilidad num√©rica en la red neuronal

**Causas Potenciales:**
1. **Gradientes sin clipping** (max_grad_norm=10.0 puede ser insuficiente)
2. **Learning rate alto** (lr=5e-5 podr√≠a estar en el l√≠mite)
3. **Observaciones sin normalizar** (pre-escala de kW/kWh puede estar incompleta)
4. **Batch size peque√±o** (256 es razonable pero depende de varianza)

**Recomendaci√≥n:**
- Monitorear despu√©s de fix del reward
- Si persiste: reducir LR a 2e-5
- Verificar que `clip_gradients=True` est√° activado

---

### 3. ‚úÖ CO‚ÇÇ C√°lculo CORRECTO (No necesita correcci√≥n)

**Verificaci√≥n de Valores:**
```
grid_kWh=2,281,666.7
co2_grid=1,031,541.5  ‚Üê grid √ó 0.4521 = 2,281,666.7 √ó 0.4521 = 1,030,910.6 ‚úì

solar_kWh=2,399,954.2
co2_indirect=1,085,019.3  ‚Üê solar √ó 0.4521 = 2,399,954.2 √ó 0.4521 = 1,085,036.6 ‚úì

co2_direct=294,109.3  ‚Üê EV √ó 2.146 ‚úì
```

**Conclusi√≥n:**
- C√°lculo de CO‚ÇÇ **CORRECTO** ‚úÖ
- Factores: 0.4521 (grid) y 2.146 (EV) **VERIFICADOS** ‚úÖ
- Acumulaci√≥n en `metrics_accumulator` **CORRECTA** ‚úÖ

---

## üìà VERIFICACIONES: BESS y Chargers Individuales

### BESS (Battery Energy Storage System)
**Estado en el C√≥digo:**
- ‚úÖ Cargado en CityLearn schema (dataset_builder.py l√≠nea 626)
- ‚úÖ Capacidad: 4,520 kWh (OE2 real)
- ‚úÖ Potencia: 2,712 kW
- ‚úÖ SOC en observaci√≥n (394-dim)
- ‚è≥ Controlabilidad: NO DIRECTO por agente RL
  - BESS es controlado por reglas de despacho autom√°tico (5 prioridades)
  - Agente RL controla los 128 chargers (acciones continuas [0,1])

### Chargers (128 individuales)
**Estado en el C√≥digo:**
- ‚úÖ 128 CSVs individuales generados (charger_simulation_001.csv a 128.csv)
- ‚úÖ Acciones RL: 129-dim = 1 (BESS) + 128 (chargers)
- ‚úÖ Observaci√≥n: cada charger tiene 4 valores (ocupancia, SOC, etc.)
- ‚è≥ Aprendizaje Individual:
  - Agentes RL actuales: SAC (off-policy) es el mejor para este caso
  - PPO/A2C: on-policy, pueden ser menos eficientes con 128 acciones

**Contadores de Carga (de los logs):**
- motos=54,820: 54,820 veh√≠culos cargados
- mototaxis=8,223: 8,223 veh√≠culos cargados
- **TOTAL: 63,043 vehiculos** (razonable para 11,500 pasos de episodio ~1.3 a√±os)

---

## üéØ CHECKLIST POST-CORRECCI√ìN

- [x] **Reward**: Removido escalado √ó 100
- [x] **CO‚ÇÇ Indirecto**: Verificado = solar √ó 0.4521 ‚úì
- [x] **CO‚ÇÇ Directo**: Verificado = EV √ó 2.146 ‚úì
- [ ] **Loss Explosion**: Monitorear, posible fix en pr√≥xima iteraci√≥n
- [x] **BESS**: Cargado y funcionando (no directamente controlable por RL)
- [x] **Chargers**: 128 individuales operacionales
- [ ] **Performance**: Re-ejecutar entrenamiento para validar fixes

---

## üöÄ PR√ìXIMOS PASOS

### Inmediato (despu√©s de aplicar fix del reward):
1. Re-ejecutar entrenamiento: `python -m scripts.run_oe3_simulate --config configs/default.yaml`
2. Monitorear logs:
   - `reward_avg` debe estar entre -1 y 1 (no 17.8)
   - `actor_loss` y `critic_loss` deben ser m√°s razonables
3. Verificar que episode 2 complete correctamente

### Si persisten losses exploding:
1. Reducir learning rate: 5e-5 ‚Üí 2e-5
2. Aumentar clip_gradients max_norm: 10.0 ‚Üí 5.0
3. Verificar normalizaci√≥n de obs en wrapper CityLearn

### Validaciones de Arquitectura:
- [ ] BESS learning: Revisar si hay espacio para control de BESS futuro
- [ ] Charger fairness: Verificar que no hay sesgo (solo algunos chargers se cargan)
- [ ] Solar utilization: Debe estar en rango 60-70% (mejorable vs baseline 40%)

---

## üìä M√©tricas Base para Comparaci√≥n

| M√©trica | Baseline | SAC (esperado) | PPO (esperado) | A2C (esperado) |
|---------|----------|----------------|----------------|----------------|
| reward_avg | N/A | 0.15-0.25 | 0.18-0.28 | 0.12-0.22 |
| co2_neto (kg/a√±o) | 5,319,725 | 3,800,000 | 3,700,000 | 3,900,000 |
| solar_util (%) | 40% | 65% | 68% | 60% |
| actor_loss | N/A | -100 to -50 | N/A | N/A |
| critic_loss | N/A | 10-50 | N/A | N/A |

---

## üìù Notas de Implementaci√≥n

**Archivo Modificado:**
- `src/iquitos_citylearn/oe3/agents/sac.py` l√≠nea 736

**Cambio:**
```python
# L√≠nea 728-739: Extracci√≥n de reward
rewards = self.locals.get("rewards", [])
reward_val = 0.0
if rewards is not None:
    if hasattr(rewards, '__iter__'):
        # üî¥ TIER 1 FIX: NO escalar reward aqu√≠
        for r in rewards:
            reward_val = float(r)  # ‚Üê SIN escalado √ó 100
    else:
        reward_val = float(rewards)

self.metrics_accumulator.accumulate(step_metrics, reward_val)
```

**Impacto:**
- ‚úÖ Reward ser√° interpretado correctamente
- ‚úÖ Multiobjetivo ponderaci√≥n (0.50 CO‚ÇÇ, 0.20 solar, etc.) funcionar√° como dise√±ado
- ‚úÖ Logs mostrar√°n valores reales normalizados

---

## üîó Referencias en C√≥digo

- CO‚ÇÇ Factors: `src/iquitos_citylearn/oe3/agents/metrics_extractor.py` l√≠neas 49-50
- Reward Multiobjetivo: `src/iquitos_citylearn/oe3/rewards.py` l√≠neas 100-130
- SAC Config: `src/iquitos_citylearn/oe3/agents/sac.py` l√≠neas 180-230
- Metrics Accumulator: `src/iquitos_citylearn/oe3/agents/metrics_extractor.py` l√≠neas 293-395

**Fecha Diagn√≥stico:** 2026-02-02  
**Agente Bajo An√°lisis:** SAC (Soft Actor-Critic)  
**Estado:** üü¢ IDENTIFICADO Y CORREGIDO (Reward Fix)
