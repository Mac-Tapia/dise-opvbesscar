# Configuraciones Ã“ptimas de Agentes - JustificaciÃ³n CientÃ­fica

## Referencia de Papers

- **SAC**: Christodoulou et al. (2018) - "Soft Actor-Critic Algorithms and Applications"
- **PPO**: Schulman et al. (2017) - "Proximal Policy Optimization Algorithms"
- **A2C**: Mnih et al. (2016) - "Asynchronous Methods for Deep Reinforcement Learning"

---

## 1. SAC (Soft Actor-Critic) - Off-Policy, Sample-Efficient

### ConfiguraciÃ³n Actual
```yaml
learning_rate_actor: 0.001
learning_rate_critic: 0.002
tau: 0.01
buffer_size: 10000000
batch_size: 1024
entropy_coef_init: 0.2
entropy_coef_learned: true
target_entropy_scale: 1.5
train_freq: 1
gradient_steps: 2048
```

### JustificaciÃ³n CientÃ­fica

#### 1.1 Learning Rates (Actor: 0.001 | Critic: 0.002)
**Paper**: Christodoulou et al. (2018), Section 4.1
- SAC mantiene dos redes: actor (policy) y critic (value)
- **Critic LR > Actor LR**: El crÃ­tico necesita aprender primero el landscape de recompensas
- 0.002 para crÃ­tico â†’ Convergencia rÃ¡pida del estimador de valor
- 0.001 para actor â†’ PolÃ­tica mÃ¡s estable (actualizaciÃ³n gradual)
- **Ratio 2:1** mejora la estabilidad convergencia en problemas de dimensiÃ³n alta (534-dim obs aquÃ­)

#### 1.2 Entropy Coefficient (0.2, aprendible)
**Paper**: Christodoulou et al. (2018), Section 3
- SAC aÃ±ade **regularizaciÃ³n de entropÃ­a** para exploraciÃ³n automÃ¡tica
- H(Ï€) mide la aleatoriedad de la polÃ­tica: maximizar H fuerza exploraciÃ³n
- **ent_coef_learned: true** â†’ Lagrange multiplier aprendible
- 0.2 inicial â†’ Balance exploraciÃ³n-explotaciÃ³n desde inicio
- target_entropy_scale: 1.5 â†’ Temperatura adaptativa para recompensas RL multi-objetivo
- **Para control energÃ©tico**: Busca la polÃ­tica mÃ¡s "suave" (smooth), menos agresiva

#### 1.3 Soft Update (tau: 0.01)
**Paper**: Christodoulou et al. (2018), eq. (7)
- Target networks se actualizan con: Î¸' = Ï„Â·Î¸ + (1-Ï„)Â·Î¸'
- tau=0.01 significa: 1% del modelo actual + 99% histÃ³rico
- **Muy conservador**: Evita overestimation de Q-values (crÃ­tico problema en off-policy)
- Para 8,760 timesteps: gradual learning = convergencia estable en largo plazo

#### 1.4 Replay Buffer (10M experiencias)
**Paper**: Christodoulou et al. (2018), Section 4
- SAC es **off-policy**: reutiliza datos de exploraciones pasadas
- 10M = ~1142 episodios (8,760 steps cada uno) Ã— 1.14 = capacidad para recorrer datos
- Permite mini-batches de 1024 durante mÃºltiples epochs sin repeating data

#### 1.5 Batch Size (1024)
**Paper**: Stable Baselines3 documentation + Christodoulou et al.
- Grande (1024) para estabilidad numÃ©rica con gradientes de valor Q
- Reduce varianza de estimador de Q-value
- GPU-friendly en CUDA (512+ es Ã³ptimo)

#### 1.6 Gradient Steps (2048 por rollout)
**Paper**: Christodoulou et al., off-policy advantage
- 2048 pasos de gradiente por actuaciÃ³n env = mÃ¡ximo uso de datos del buffer
- Compensa que solo hay 1 paso env por entrenamiento (train_freq=1)

---

## 2. PPO (Proximal Policy Optimization) - On-Policy, Estable

### ConfiguraciÃ³n Actual
```yaml
learning_rate: 0.0003
n_steps: 4096
batch_size: 512
n_epochs: 25
entropy_coef: 0.001
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
clip_range_vf: 0.2
max_grad_norm: 0.5
target_kl: 0.003
use_amp: true
```

### JustificaciÃ³n CientÃ­fica

#### 2.1 Learning Rate (0.0003, muy bajo)
**Paper**: Schulman et al. (2017), Section 5.2
- PPO usa **on-policy** â†’ cada step es "fresco" (no reutilizado)
- LR bajo compensa que hay menos datos (solo 1 trajectory = 8,760 steps)
- 0.0003 vs 0.001 (SAC): PPO es mÃ¡s sensible a overtraining porque no tiene replay buffer
- Formula de confianza: "Stay close to old policy" â†’ LR pequeÃ±o garantiza eso

#### 2.2 N-Steps (4096 rollout)
**Paper**: Schulman et al. (2017), GAE section
- MÃ­nimo: 1024 (batch tamaÃ±o estÃ¡ndar)
- 4096 = 4.6 episodes (8760 / 4096 â‰ˆ 2 sub-episodes)
- **Advantage**: Estimador de ventaja mÃ¡s estable (menos bias)
- **Tradeoff**: Espera mÃ¡s pasos antes de actualizar (pero es on-policy, necesario)

#### 2.3 GAE Lambda (0.95)
**Paper**: Schulman et al. (2015) - "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
- Î» âˆˆ [0, 1] controla bias-variance en estimador de ventaja
- 0.95 = **muy estable** (cercano a 1.0)
- Combina: Discounted sum of rewards + Critic baseline
- Para energÃ­a: Necesita estabilidad porque rewards tienen mÃºltiples objetivos (COâ‚‚, solar, cost, EV, grid)

#### 2.4 Clipping (0.2 both actor + value)
**Paper**: Schulman et al. (2017), eq. (7) y (8)
- "Proximal" = Stay near old policy
- clip_range=0.2 â†’ Nueva polÃ­tica puede alejarse mÃ¡ximo 20% en probabilidad
- clip_range_vf=0.2 â†’ CrÃ­tico tambiÃ©n clipeado (estabilidad)
- Previene actualizaciones agresivas que destrozan exploraciÃ³n

#### 2.5 Max Grad Norm (0.5)
**Paper**: Stable Baselines3 + RL best practices
- Clip gradientes a norm â‰¤ 0.5
- Previene "gradient explosion" en redes profundas
- 0.5 es conservador: permite aprendizaje sin inestabilidad

#### 2.6 Target KL (0.003)
**Paper**: Schulman et al., adaptive clipping
- KL divergence = medida de "cuÃ¡n diferente" es nueva vs vieja polÃ­tica
- target_kl=0.003 â†’ Si KL > 0.003, para actualizar (early stopping)
- Garantiza "trust region" en sentido prÃ¡ctico
- Permite entrenamientos mÃ¡s largos sin divergencia

#### 2.7 Epochs (25)
**Paper**: Schulman et al., multiple passes
- 25 passes sobre mismo 4096-step batch
- MÃ¡ximo aprovechamiento del dato (on-policy, no se puede reutilizar)
- Formula: 25 Ã— 512 batch / 4096 = 3.125 datasets worth of training

---

## 3. A2C (Advantage Actor-Critic) - On-Policy, Simple

### ConfiguraciÃ³n Actual
```yaml
learning_rate: 0.002
n_steps: 16
batch_size: 1024
entropy_coef: 0.02
gamma: 0.99
vf_coef: 0.5
max_grad_norm: 1.0
gae_lambda: 0.9
use_rms_prop: true
normalize_advantage: true
```

### JustificaciÃ³n CientÃ­fica

#### 3.1 Learning Rate (0.002, moderado)
**Paper**: Mnih et al. (2016), "Asynchronous Methods for Deep RL"
- A2C es **"A" = Asynchronous**, but en stable-baselines3 = sincrÃ³nico
- 0.002 es mÃ¡s alto que PPO (0.0003) porque:
  - A2C usa advantage function (mÃ¡s estable que raw rewards)
  - Sin trust region (cliping): puede permitirse LR mayor
  - Menos datos reutilizado â†’ necesita LR > para convergencia

#### 3.2 N-Steps (16, VERY SMALL)
**Paper**: Mnih et al. (2016), Section 4
- A2C hace updates frecuentes (every 16 steps)
- trade-off: bias alto (short bootstrap) vs variance bajo (rÃ¡pidos updates)
- 16 = "shallow" advantage estimation
- **Ventaja para energÃ­a**: Reacciona rÃ¡pido a cambios (solar â†“, clouds, EV arrivals)
- 8760 / 16 = 547 updates/episodio vs PPO: 8760/4096 = 2 updates

#### 3.3 Entropy Coefficient (0.02, alto)
**Paper**: Mnih et al., exploration bonus
- 0.02 es 20Ã— mayor que PPO (0.001)
- A2C necesita mÃ¡s exploraciÃ³n porque:
  - No tiene trust region (PPO clip)
  - Sin replay buffer (SAC)
  - Actualizaciones on-policy + frequent = tiende a converger rÃ¡pido a mÃ­nimo local
- **Mayor entropÃ­a** = explora mÃ¡s, tarda en convergir pero menos stuck

#### 3.4 Value Function Coefficient (0.5)
**Paper**: Mnih et al. (2016), eq. (3)
- Loss = Policy loss + 0.5 Ã— Value loss
- 0.5 = balance perfecto
  - < 0.5: CrÃ­tico bajo-entrenado (mala baseline)
  - > 0.5: CrÃ­tico sobre-entrenado (destabiliza actor)

#### 3.5 Max Grad Norm (1.0, mÃ¡s permisivo)
**Paper**: Mnih et al., implementation details
- A2C es mÃ¡s "robusto" que PPO (no usa clipping)
- 1.0 vs PPO's 0.5 = permite gradientes mayores
- AÃºn previene explosiÃ³n (norm > 1 se clipea)
- Necesario para que se movimiento en high-dim space (534-dim obs)

#### 3.6 RMS Prop (true) vs Adam
**Paper**: Mnih et al. (2016), optimization
- Argumento original: RMS Prop converge mÃ¡s rÃ¡pido para on-policy
- Adam = adaptivo per-param (mÃ¡s lento para A2C)
- RMS Prop = global momentum (mÃ¡s directo)

---

## 4. Comparativa de Hyperparameters

| Parameter | SAC | PPO | A2C | JustificaciÃ³n |
|-----------|-----|-----|-----|---------------|
| **LR (Actor)** | 0.001 | 0.0003 | 0.002 | SAC off-policyâ†’LR alto; PPO trust regionâ†’LR bajo; A2C simpleâ†’LR medio |
| **N-Steps** | No aplica (replay) | 4096 | 16 | SAC reutiliza buffer; PPO necesita largo rollout (GAE); A2C actualiza frecuente |
| **Entropy** | 0.2 learned | 0.001 | 0.02 | SAC auto-ajusta; PPO minimiza (trust); A2C explora agresivo |
| **Buffer Size** | 10M | None | None | SAC off-policy requiere buffer; ON-policy no reutilizan |
| **Batch Size** | 1024 | 512 | 1024 | SAC/A2C: batches grandes (GPU); PPO: menor (stability) |
| **Gamma** | 0.99 | 0.99 | 0.99 | Todos: discounting a largo plazo (8,760 pasos = 1 aÃ±o) |
| **Clip/Trust** | Soft update (Ï„) | Clip (0.2) | None | SAC gradual; PPO strict; A2C directo |

---

## 5. Recomendaciones para MÃ¡ximo Potencial

### 5.1 SAC - Optimizaciones Sugeridas

**Para acelerar convergencia (sin perder estabilidad):**
```yaml
sac:
  learning_rate: 0.001      # âœ“ Ã“ptimo actual
  learning_rate_critic: 0.0025  # â†‘ Aumentar 25%
  tau: 0.005                # â†“ Bajar (soft update mÃ¡s rÃ¡pido)
  buffer_size: 20000000     # â†‘ Duplicar (mÃ¡s experiencias)
  ent_coef_init: 0.1        # â†“ Bajar entropÃ­a inicial (menos random)
```

**JustificaciÃ³n**:
- Mayor buffer + crÃ­tico LR = mejor Q-value estimation
- Menor tau = updates mÃ¡s rÃ¡pidas (pero con datos mÃ¡s estables)
- Menor entropÃ­a inicial â†’ Mejor explotaciÃ³n early (aÃºn aprendible con ent_coef_learned)

### 5.2 PPO - Optimizaciones Sugeridas

**Para multi-objective RL:**
```yaml
ppo:
  learning_rate: 0.0005     # â†‘ Aumentar 67%
  n_steps: 8192             # â†‘ Duplicar (mÃ¡s datos/epoch)
  n_epochs: 20              # â†“ Bajar 20% (menos overfitting)
  entropy_coef: 0.002       # â†‘ Duplicar exploraciÃ³n
  gae_lambda: 0.98          # â†‘ Subir (mÃ¡s estabilidad)
```

**JustificaciÃ³n**:
- Multi-objetivo (5 rewards) â†’ necesita exploraciÃ³n + estabilidad
- 8192 steps = 1 full episode, mejor GAE estimation
- 20 epochs vs 25 = menos riesgo de overfitting
- gae_lambda 0.98 = casi retorno sin descuento (mÃ¡s datos)

### 5.3 A2C - Optimizaciones Sugeridas

**Para reacciÃ³n rÃ¡pida a cambios ambientales:**
```yaml
a2c:
  learning_rate: 0.003      # â†‘ Aumentar 50% (A2C robusto)
  n_steps: 8                # â†“ Bajar 50% (updates mÃ¡s frecuentes)
  entropy_coef: 0.03        # â†‘ Aumentar exploraciÃ³n
  gae_lambda: 0.92          # â†‘ Aumentar lambda
  use_rms_prop: true        # âœ“ Mantener
```

**JustificaciÃ³n**:
- A2C: cambios rÃ¡pidos (solar, weather) â†’ updates frecuentes
- 8 steps = ~1 min de tiempo real (reacciÃ³n casi instantÃ¡nea)
- Mayor LR + entropÃ­a = explora bien en high-dim space
- RMS Prop es ideal para A2C on-policy

---

## 6. Tabla de ApplicaciÃ³n Ã“ptima Sugerida

```yaml
oe3:
  evaluation:
    
    # SAC: Para exploraciÃ³n exhaustiva + datos off-policy
    sac:
      learning_rate_actor: 0.001
      learning_rate_critic: 0.0025      # CHANGE â†‘
      tau: 0.005                         # CHANGE â†“
      buffer_size: 20000000              # CHANGE â†‘
      entropy_coef_init: 0.1             # CHANGE â†“
      entropy_coef_learned: true
      batch_size: 1024
      gradient_steps: 2048
      episodes: 5                        # CHANGE: â†‘ de 3
    
    # PPO: Para estabilidad + multi-objetivo
    ppo:
      learning_rate: 0.0005              # CHANGE â†‘
      n_steps: 8192                      # CHANGE â†‘
      n_epochs: 20                       # CHANGE â†“
      entropy_coef: 0.002                # CHANGE â†‘
      gae_lambda: 0.98                   # CHANGE â†‘
      batch_size: 512
      episodes: 5                        # CHANGE: â†‘ de 3
    
    # A2C: Para reacciÃ³n rÃ¡pida
    a2c:
      learning_rate: 0.003               # CHANGE â†‘
      n_steps: 8                         # CHANGE â†“
      entropy_coef: 0.03                 # CHANGE â†‘
      gae_lambda: 0.92                   # CHANGE â†‘
      batch_size: 1024
      use_rms_prop: true
      episodes: 5                        # CHANGE: â†‘ de 3
```

---

## 7. ImplementaciÃ³n Escalonada

### Fase 1: Validar Baselines (actual config)
- Entrenar 3 episodios cada agente
- Recolectar rewards, COâ‚‚, solar utilization
- **Output**: benchmark numbers

### Fase 2: SAC Optimizado
- Aplicar cambios SAC (tauâ†“, bufferâ†‘, critic_lrâ†‘)
- Entrenar 5 episodios
- Comparar vs Fase 1

### Fase 3: PPO Optimizado
- Aplicar cambios PPO (lrâ†‘, n_stepsâ†‘, entropyâ†‘)
- Entrenar 5 episodios
- Comparar vs Fase 1

### Fase 4: A2C Optimizado
- Aplicar cambios A2C (lrâ†‘, n_stepsâ†“, entropyâ†‘)
- Entrenar 5 episodios
- Comparar vs Fase 1

### Fase 5: Ensemble Comparison
- Ejecutar todos 3 agentes optimizados en paralelo
- Generar tabla comparativa de rendimiento

---

## Referencias CientÃ­ficas Completas

1. **Christodoulou et al. (2018)**
   - "Soft Actor-Critic Algorithms and Applications"
   - arXiv:1812.05905
   - Key: Entropy regularization, off-policy learning, temperature scaling

2. **Schulman et al. (2017)**
   - "Proximal Policy Optimization Algorithms"
   - arXiv:1707.06347
   - Key: Trust region via clipping, GAE, on-policy stability

3. **Mnih et al. (2016)**
   - "Asynchronous Methods for Deep Reinforcement Learning"
   - ICML 2016
   - Key: A3C foundation, frequent updates, RMS Prop optimization

4. **Schulman et al. (2015)**
   - "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
   - ICML 2016
   - Key: GAE formula, bias-variance tradeoff in advantage estimation

5. **Stable Baselines3 Documentation**
   - https://stable-baselines3.readthedocs.io/
   - Key: Implementation details, hyperparameter guides, reproducibility

---

## PrÃ³ximos Pasos

1. âœ… Documentar justificaciones (este archivo)
2. ðŸ”„ Crear config_optimized.yaml con cambios sugeridos
3. ðŸ”„ Implementar Fase 1: Entrenar con config actual
4. ðŸ”„ Implementar Fase 2-5: Optimizaciones graduales
5. ðŸ”„ Generar tabla de comparaciÃ³n de resultados

**Estado**: Listo para implementar optimizaciones âœ“
