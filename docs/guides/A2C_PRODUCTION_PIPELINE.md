# ğŸš€ A2C Production Pipeline Guide

## Iquitos EV/Solar/BESS Optimization - Advantage Actor-Critic

---

## ğŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [Arquitectura A2C](#arquitectura-a2c)
3. [Comparativa SAC/PPO/A2C](#comparativa-sacppoa2c)
4. [HiperparÃ¡metros](#hiperparÃ¡metros)
5. [Uso del Pipeline](#uso-del-pipeline)
6. [Flujo de Entrenamiento](#flujo-de-entrenamiento)
7. [Troubleshooting](#troubleshooting)
8. [MÃ©tricas COâ‚‚](#mÃ©tricas-coâ‚‚)

---

## ğŸ¯ DescripciÃ³n General

El pipeline A2C estÃ¡ diseÃ±ado para optimizar el sistema de carga EV con solar y BESS en Iquitos, utilizando el algoritmo **Advantage Actor-Critic**.

### Â¿Por quÃ© A2C?

| CaracterÃ­stica | A2C | PPO | SAC |
|----------------|-----|-----|-----|
| **Velocidad (wall-clock)** | â˜…â˜…â˜…â˜…â˜… MÃ¡s rÃ¡pido | â˜…â˜…â˜…â˜†â˜† Medio | â˜…â˜…â˜†â˜†â˜† MÃ¡s lento |
| **Sample Efficiency** | â˜…â˜…â˜†â˜†â˜† Menor | â˜…â˜…â˜…â˜…â˜† Alta | â˜…â˜…â˜…â˜…â˜… Mejor |
| **Estabilidad** | â˜…â˜…â˜…â˜†â˜† Media | â˜…â˜…â˜…â˜…â˜… Muy alta | â˜…â˜…â˜…â˜…â˜† Alta |
| **Memoria GPU** | â˜…â˜…â˜…â˜…â˜… Baja | â˜…â˜…â˜…â˜…â˜† Media | â˜…â˜…â˜…â˜†â˜† Alta |

**A2C es ideal cuando:**
- âš¡ El tiempo de entrenamiento es crÃ­tico
- ğŸ’» Hardware limitado (< 8GB VRAM)
- ğŸ§ª IteraciÃ³n rÃ¡pida de experimentos
- ğŸ“Š Pruebas de concepto iniciales

---

## ğŸ—ï¸ Arquitectura A2C

### Componentes Principales

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     A2C AGENT ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Environment â”‚â—„â”€â”€â”€â”€â”‚     CityLearn Wrapper               â”‚   â”‚
â”‚  â”‚ (CityLearn) â”‚     â”‚  â€¢ Observation Normalization        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚  â€¢ Reward Scaling (Ã—0.1)            â”‚   â”‚
â”‚         â”‚            â”‚  â€¢ Action Smoothing                 â”‚   â”‚
â”‚         â–¼            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   A2C NETWORK                           â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚
â”‚  â”‚   â”‚    ACTOR     â”‚         â”‚    CRITIC    â”‚            â”‚   â”‚
â”‚  â”‚   â”‚   (Policy)   â”‚         â”‚   (Value)    â”‚            â”‚   â”‚
â”‚  â”‚   â”‚              â”‚         â”‚              â”‚            â”‚   â”‚
â”‚  â”‚   â”‚  obs (394)   â”‚         â”‚  obs (394)   â”‚            â”‚   â”‚
â”‚  â”‚   â”‚     â†“        â”‚         â”‚     â†“        â”‚            â”‚   â”‚
â”‚  â”‚   â”‚  FC(256)     â”‚         â”‚  FC(256)     â”‚            â”‚   â”‚
â”‚  â”‚   â”‚  ReLU        â”‚         â”‚  ReLU        â”‚            â”‚   â”‚
â”‚  â”‚   â”‚     â†“        â”‚         â”‚     â†“        â”‚            â”‚   â”‚
â”‚  â”‚   â”‚  FC(256)     â”‚         â”‚  FC(256)     â”‚            â”‚   â”‚
â”‚  â”‚   â”‚  ReLU        â”‚         â”‚  ReLU        â”‚            â”‚   â”‚
â”‚  â”‚   â”‚     â†“        â”‚         â”‚     â†“        â”‚            â”‚   â”‚
â”‚  â”‚   â”‚ actions(129) â”‚         â”‚  value(1)    â”‚            â”‚   â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚   ADVANTAGE = Reward + Î³Â·V(s') - V(s)                  â”‚   â”‚
â”‚  â”‚   LOSS = Actor_Loss + vf_coefÂ·Critic_Loss              â”‚   â”‚
â”‚  â”‚        - ent_coefÂ·Entropy                               â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Observaciones (394 dimensiones)

| Grupo | Dimensiones | DescripciÃ³n |
|-------|-------------|-------------|
| Solar/Grid | 8 | PV generation, grid metrics |
| BESS | 4 | SOC, power, capacity |
| Chargers (Ã—128) | 128Ã—3 = 384 | Estado de cada cargador |
| Tiempo | 3 | hour, day_of_week, month |
| Extras | 5 | Agregaciones y features |

### Acciones (129 dimensiones)

| AcciÃ³n | DimensiÃ³n | Rango | DescripciÃ³n |
|--------|-----------|-------|-------------|
| BESS setpoint | 1 | [0, 1] | Control de carga/descarga |
| Charger 1-112 | 112 | [0, 1] | Setpoints motos |
| Charger 113-128 | 16 | [0, 1] | Setpoints mototaxis |

---

## âš”ï¸ Comparativa SAC/PPO/A2C

### Tiempos de Entrenamiento Estimados (RTX 4060)

| Timesteps | A2C | PPO | SAC |
|-----------|-----|-----|-----|
| 50,000 | ~3 min | ~5 min | ~8 min |
| 100,000 | ~6 min | ~12 min | ~20 min |
| 500,000 | ~30 min | ~1 hora | ~2 horas |
| 1,000,000 | ~1 hora | ~2 horas | ~4 horas |

### CaracterÃ­sticas de cada Algoritmo

#### A2C (Advantage Actor-Critic)
- âœ… **Ventajas:**
  - Entrenamiento mÃ¡s rÃ¡pido
  - Menor consumo de memoria
  - Actualizaciones sÃ­ncronas (predecibles)
- âŒ **Desventajas:**
  - Alta varianza en gradientes
  - Menos sample-efficient
  - Puede converger a Ã³ptimos locales

#### PPO (Proximal Policy Optimization)
- âœ… **Ventajas:**
  - Muy estable (clipped objective)
  - Buen balance eficiencia/velocidad
  - KL-adaptive para estabilidad extra
- âŒ **Desventajas:**
  - MÃ¡s lento que A2C
  - MÃ¡s hiperparÃ¡metros que ajustar

#### SAC (Soft Actor-Critic)
- âœ… **Ventajas:**
  - Mejor exploration (entropy maximization)
  - Off-policy (replay buffer)
  - State-of-the-art en muchos benchmarks
- âŒ **Desventajas:**
  - MÃ¡s lento
  - Mayor consumo de memoria
  - MÃ¡s complejo de debuggear

---

## âš™ï¸ HiperparÃ¡metros

### ConfiguraciÃ³n Optimizada A2C (RTX 4060)

```python
# HiperparÃ¡metros A2C - Optimizados para Iquitos EV/Solar/BESS
A2CConfig(
    train_steps=500_000,        # Total timesteps
    n_steps=2048,               # Rollout buffer (similar a PPO)
    learning_rate=1e-4,         # Con linear decay
    lr_schedule="linear",       # Decay hacia 0

    # GAE (Generalized Advantage Estimation)
    gamma=0.99,                 # Discount factor
    gae_lambda=0.95,            # Î» para GAE (reduce varianza)

    # Entropy & Value Function
    ent_coef=0.01,              # Coef. entropy (exploration)
    ent_coef_schedule="linear", # Decay: 0.01 â†’ 0.001
    ent_coef_final=0.001,
    vf_coef=0.5,                # Coef. value function loss
    max_grad_norm=0.5,          # Gradient clipping

    # Network Architecture
    hidden_sizes=(256, 256),    # 2 capas hidden

    # Normalization (CRÃTICO para estabilidad)
    normalize_observations=True,
    normalize_rewards=True,
    reward_scale=0.1,
    clip_obs=10.0,
)
```

### ExplicaciÃ³n de ParÃ¡metros Clave

| ParÃ¡metro | Valor | Por quÃ© |
|-----------|-------|---------|
| `n_steps` | 2048 | Buffer suficiente para capturar patrones diarios |
| `gae_lambda` | 0.95 | Balance bias-varianza en estimaciÃ³n de ventajas |
| `ent_coef` | 0.01â†’0.001 | Exploration inicial, explotaciÃ³n al final |
| `vf_coef` | 0.5 | Peso relativo del critic vs actor |
| `reward_scale` | 0.1 | Escala rewards a rango manejable |

---

## ğŸ® Uso del Pipeline

### Comandos BÃ¡sicos

```bash
# Entrenamiento estÃ¡ndar (500k timesteps, ~30 min)
python -m scripts.train_a2c_production

# Entrenamiento rÃ¡pido para testing (50k, ~3 min)
python -m scripts.train_a2c_production --timesteps 50000

# Entrenamiento extendido (1M timesteps, ~1 hora)
python -m scripts.train_a2c_production --timesteps 1000000

# Continuar desde checkpoint
python -m scripts.train_a2c_production --resume

# Solo evaluaciÃ³n (sin entrenar)
python -m scripts.train_a2c_production --eval-only

# Con configuraciÃ³n personalizada
python -m scripts.train_a2c_production --config configs/custom.yaml
```

### Opciones CLI

| OpciÃ³n | Default | DescripciÃ³n |
|--------|---------|-------------|
| `--config` | `configs/default.yaml` | Archivo de configuraciÃ³n |
| `--timesteps` | `500000` | Total de pasos de entrenamiento |
| `--resume` | `False` | Continuar desde Ãºltimo checkpoint |
| `--eval-only` | `False` | Solo evaluar, no entrenar |

---

## ğŸ“ˆ Flujo de Entrenamiento

### Pipeline Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   A2C TRAINING PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1ï¸âƒ£ INICIALIZACIÃ“N                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Cargar configs/default.yaml                          â”‚    â”‚
â”‚  â”‚ â€¢ Detectar GPU (CUDA/MPS/CPU)                          â”‚    â”‚
â”‚  â”‚ â€¢ Validar dataset (128 chargers, 8760 timesteps)       â”‚    â”‚
â”‚  â”‚ â€¢ Configurar logging y directorios                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â†“                                      â”‚
â”‚  2ï¸âƒ£ ENTRENAMIENTO                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ FOR step in range(timesteps):                          â”‚    â”‚
â”‚  â”‚   â”‚                                                     â”‚    â”‚
â”‚  â”‚   â”œâ”€ Collect n_steps experiences (rollout)             â”‚    â”‚
â”‚  â”‚   â”‚   â€¢ Execute actions in CityLearn env               â”‚    â”‚
â”‚  â”‚   â”‚   â€¢ Store (obs, action, reward, done)              â”‚    â”‚
â”‚  â”‚   â”‚                                                     â”‚    â”‚
â”‚  â”‚   â”œâ”€ Compute Advantages (GAE)                          â”‚    â”‚
â”‚  â”‚   â”‚   â€¢ Î´_t = r_t + Î³Â·V(s_{t+1}) - V(s_t)             â”‚    â”‚
â”‚  â”‚   â”‚   â€¢ Ã‚_t = Î£ (Î³Î»)^k Â· Î´_{t+k}                       â”‚    â”‚
â”‚  â”‚   â”‚                                                     â”‚    â”‚
â”‚  â”‚   â”œâ”€ Update Networks                                   â”‚    â”‚
â”‚  â”‚   â”‚   â€¢ Actor: âˆ‡ log Ï€(a|s) Â· Ã‚                        â”‚    â”‚
â”‚  â”‚   â”‚   â€¢ Critic: (V(s) - R_target)Â²                     â”‚    â”‚
â”‚  â”‚   â”‚   â€¢ Entropy: -Ï€ log Ï€                              â”‚    â”‚
â”‚  â”‚   â”‚                                                     â”‚    â”‚
â”‚  â”‚   â””â”€ Checkpoint (cada 1000 steps)                      â”‚    â”‚
â”‚  â”‚       â€¢ Guardar modelo: a2c_step_XXXXX.zip             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â†“                                      â”‚
â”‚  3ï¸âƒ£ EVALUACIÃ“N FINAL                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Ejecutar 1 episodio completo (8760 steps)            â”‚    â”‚
â”‚  â”‚ â€¢ Calcular mÃ©tricas COâ‚‚ (3-component)                  â”‚    â”‚
â”‚  â”‚ â€¢ Generar timeseries CSV                               â”‚    â”‚
â”‚  â”‚ â€¢ Guardar a2c_summary.json                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Archivos Generados

```
checkpoints/
â””â”€â”€ a2c/
    â”œâ”€â”€ a2c_step_001000.zip    # Checkpoint paso 1000
    â”œâ”€â”€ a2c_step_002000.zip    # Checkpoint paso 2000
    â”œâ”€â”€ ...
    â””â”€â”€ a2c_final.zip          # Modelo final

outputs/oe3_simulations/a2c/
â”œâ”€â”€ result_a2c.json            # MÃ©tricas detalladas
â”œâ”€â”€ timeseries_a2c.csv         # Serie temporal horaria
â”œâ”€â”€ trace_a2c.csv              # Trace de entrenamiento
â””â”€â”€ a2c_summary.json           # Resumen ejecutivo
```

---

## ğŸ”§ Troubleshooting

### Problemas Comunes

#### âŒ Alta varianza en rewards

**SÃ­ntomas:** Rewards oscilando fuertemente entre episodios

**SoluciÃ³n:**
```python
# Reducir learning rate
a2c_learning_rate=5e-5  # En lugar de 1e-4

# Aumentar batch size efectivo
a2c_n_steps=4096  # En lugar de 2048
```

#### âŒ Convergencia a Ã³ptimo local

**SÃ­ntomas:** Reward estancado, no mejora

**SoluciÃ³n:**
```python
# Aumentar exploration
a2c_entropy_coef=0.02  # En lugar de 0.01

# Usar schedule mÃ¡s lento
ent_coef_schedule="linear"
ent_coef_final=0.005  # En lugar de 0.001
```

#### âŒ GPU out of memory

**SÃ­ntomas:** CUDA OOM error

**SoluciÃ³n:**
```python
# Reducir n_steps
a2c_n_steps=1024  # En lugar de 2048

# O usar CPU
a2c_device="cpu"
```

#### âŒ Entrenamiento muy lento

**SÃ­ntomas:** Progreso lento, GPU subutilizada

**SoluciÃ³n:**
```bash
# Verificar que PyTorch usa GPU
python -c "import torch; print(torch.cuda.is_available())"

# Aumentar batch size si hay VRAM disponible
a2c_n_steps=4096
```

### Logs de DiagnÃ³stico

```bash
# Ver progreso en tiempo real
tail -f checkpoints/progress/a2c_progress.csv

# Verificar checkpoints
ls -la checkpoints/a2c/

# Inspeccionar Ãºltimo checkpoint
python -c "
from stable_baselines3 import A2C
model = A2C.load('checkpoints/a2c/a2c_final.zip')
print(model.policy)
print(f'Timesteps: {model.num_timesteps}')
"
```

---

## ğŸŒ MÃ©tricas COâ‚‚

### 3-Component Breakdown

El sistema calcula COâ‚‚ con metodologÃ­a de 3 componentes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 COâ‚‚ CALCULATION (3-COMPONENT)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1ï¸âƒ£ COâ‚‚ EMITIDO POR GRID                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  = Grid Import Ã— 0.4521 kg COâ‚‚/kWh                              â”‚
â”‚  (Central tÃ©rmica Iquitos - combustibles fÃ³siles)               â”‚
â”‚                                                                 â”‚
â”‚  2ï¸âƒ£ REDUCCIONES INDIRECTAS (Evita grid import)                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  = (Solar consumido + BESS descargado) Ã— 0.4521                 â”‚
â”‚  (EnergÃ­a que NO viene del grid tÃ©rmico)                        â”‚
â”‚                                                                 â”‚
â”‚  3ï¸âƒ£ REDUCCIONES DIRECTAS (Reemplaza gasolina)                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  = EV total cargada Ã— 2.146 kg COâ‚‚/kWh                          â”‚
â”‚  (EVs evitan vehÃ­culos de combustiÃ³n)                           â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“Š COâ‚‚ NETO = Emitido - Indirectas - Directas                  â”‚
â”‚                                                                 â”‚
â”‚  Si COâ‚‚ NETO < 0 â†’ âœ… SISTEMA CARBONO-NEGATIVO                  â”‚
â”‚  Si COâ‚‚ NETO > 0 â†’ âš ï¸  Sistema carbono-positivo                 â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Objetivos de Entrenamiento

| MÃ©trica | Baseline (sin RL) | Objetivo A2C | Unidad |
|---------|-------------------|--------------|--------|
| COâ‚‚ Emitido Grid | ~640,000 | < 200,000 | kg/aÃ±o |
| Reducciones Indirectas | ~380,000 | > 400,000 | kg/aÃ±o |
| Reducciones Directas | ~509,000 | ~509,000 | kg/aÃ±o |
| COâ‚‚ Neto | +131,000 | < 0 (negativo) | kg/aÃ±o |

### Multi-Objetivo Pesos

```python
# ConfiguraciÃ³n COâ‚‚_FOCUS para A2C
multi_objective_priority = "co2_focus"

# Pesos resultantes:
co2_weight: 0.50         # PRIMARY: Minimizar importaciÃ³n grid
solar_weight: 0.20       # Maximizar autoconsumo solar
cost_weight: 0.15        # Minimizar costo elÃ©ctrico
ev_satisfaction: 0.10    # Satisfacer demanda EV
grid_stability: 0.05     # Estabilidad de red
```

---

## ğŸ“Š Resultados Esperados

### Benchmark A2C (RTX 4060, 500k timesteps)

| MÃ©trica | Valor Esperado |
|---------|----------------|
| Tiempo entrenamiento | ~30 min |
| Steps ejecutados | 500,000 |
| Final reward (mean) | 0.03 - 0.08 |
| COâ‚‚ Neto | -200,000 a -400,000 kg |
| Carbon Negative | âœ… SÃ­ |

### Comparativa Final

| Agente | COâ‚‚ Neto (kg/aÃ±o) | Mejora vs Baseline |
|--------|-------------------|-------------------|
| Baseline (sin control) | +131,000 | - |
| A2C | -200,000 a -300,000 | ~250-350% |
| PPO | -300,000 a -500,000 | ~350-480% |
| SAC | -400,000 a -700,000 | ~400-630% |

---

## ğŸ”— Archivos Relacionados

| Archivo | PropÃ³sito |
|---------|-----------|
| [train_a2c_production.py](../../scripts/train_a2c_production.py) | Script de entrenamiento |
| [a2c_sb3.py](../../src/iquitos_citylearn/oe3/agents/a2c_sb3.py) | ImplementaciÃ³n agente |
| [simulate.py](../../src/iquitos_citylearn/oe3/simulate.py) | FunciÃ³n simulate() |
| [rewards.py](../../src/iquitos_citylearn/oe3/rewards.py) | Multi-objetivo rewards |
| [default.yaml](../../configs/default.yaml) | ConfiguraciÃ³n principal |

---

**Fecha:** 2026-02-04  
**VersiÃ³n:** 1.0.0  
**Autor:** pvbesscar-copilot
