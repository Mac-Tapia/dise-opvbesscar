# üöÄ PPO Production Pipeline Guide

## Descripci√≥n

Pipeline de producci√≥n para entrenar el agente **PPO (Proximal Policy Optimization)** en el proyecto pvbesscar. PPO es un algoritmo **on-policy** que ofrece excelente estabilidad y es ideal para problemas con espacios de acci√≥n continuos.

## Caracter√≠sticas PPO vs SAC

| Caracter√≠stica | PPO | SAC |
|---------------|-----|-----|
| **Tipo** | On-Policy | Off-Policy |
| **Replay Buffer** | No (solo rollouts) | S√≠ (200k transiciones) |
| **Eficiencia de Datos** | Menor | Mayor |
| **Estabilidad** | Alta | Media-Alta |
| **Exploraci√≥n** | Via entropy coef | Via entropy autom√°tica |
| **Mejor Para** | Estabilidad, problemas nuevos | Eficiencia, fine-tuning |

## Uso R√°pido

```bash
# Entrenamiento est√°ndar (100k timesteps ~ 30 min)
python -m scripts.train_ppo_production

# Entrenamiento extendido (500k timesteps ~ 2-3 horas)
python -m scripts.train_ppo_production --timesteps 500000

# Entrenamiento r√°pido para testing (10k timesteps ~ 3 min)
python -m scripts.train_ppo_production --timesteps 10000

# Continuar desde checkpoint
python -m scripts.train_ppo_production --resume

# Solo evaluaci√≥n (sin entrenamiento)
python -m scripts.train_ppo_production --eval-only
```

## Hiperpar√°metros (Optimizados para RTX 4060)

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| `n_steps` | 2,048 | Rollout buffer size |
| `batch_size` | 256 | Mini-batch para SGD |
| `n_epochs` | 10 | √âpocas por rollout |
| `learning_rate` | 1e-4 | Con decay lineal |
| `gamma` | 0.99 | Factor de descuento |
| `gae_lambda` | 0.98 | GAE para advantage |
| `clip_range` | 0.2 | PPO clipping |
| `ent_coef` | 0.01 | Coef. de entrop√≠a (decaying) |
| `vf_coef` | 0.5 | Coef. value function |
| `hidden_sizes` | (256, 256) | Arquitectura de red |

## Tiempos Estimados (RTX 4060)

| Timesteps | Duraci√≥n | Episodios |
|-----------|----------|-----------|
| 10,000 | ~3 min | ~1 |
| 50,000 | ~15 min | ~6 |
| 100,000 | ~30 min | ~11 |
| 500,000 | ~2-3 h | ~57 |

## Estructura de Archivos

```
checkpoints/
‚îî‚îÄ‚îÄ ppo/
    ‚îú‚îÄ‚îÄ ppo_step_1000.zip    # Checkpoint cada 1000 steps
    ‚îú‚îÄ‚îÄ ppo_step_2000.zip
    ‚îî‚îÄ‚îÄ ppo_final.zip        # Checkpoint final

outputs/agents/ppo/
‚îú‚îÄ‚îÄ ppo_summary.json         # Resumen de m√©tricas
‚îú‚îÄ‚îÄ result_ppo.json          # Resultados detallados
‚îú‚îÄ‚îÄ timeseries_ppo.csv       # Serie temporal completa
‚îî‚îÄ‚îÄ trace_ppo.csv            # Traza obs/actions/rewards
```

## M√©tricas Multi-Objetivo

PPO usa la misma funci√≥n de reward multi-objetivo que SAC:

```
reward = 0.50 √ó r_co2      (Minimizar CO‚ÇÇ)
       + 0.20 √ó r_solar    (Maximizar autoconsumo)
       + 0.15 √ó r_cost     (Minimizar costo)
       + 0.10 √ó r_ev       (Satisfacci√≥n EV)
       + 0.05 √ó r_grid     (Estabilidad red)
```

## Monitoreo de Entrenamiento

### M√©tricas de Log (cada 500 steps)

```
[PPO] step 500 | ep~0 | reward_avg=0.25 | policy_loss=-0.012 | 
value_loss=45.2 | entropy_loss=0.85 | approx_kl=0.008 |
explained_var=0.65 | clip_fraction=0.12
```

**Interpretaci√≥n:**
- `reward_avg > 0`: Aprendiendo correctamente
- `approx_kl < 0.02`: Policy estable (no diverge)
- `explained_var > 0.5`: Value function predice bien
- `clip_fraction < 0.3`: Clipping efectivo

### Se√±ales de Problema

| S√≠ntoma | Causa Probable | Soluci√≥n |
|---------|----------------|----------|
| `approx_kl > 0.05` | Policy cambia muy r√°pido | Reducir lr a 5e-5 |
| `explained_var < 0` | VF no predice well | Aumentar vf_coef a 0.7 |
| `entropy_loss ‚Üí 0` | Exploraci√≥n colaps√≥ | Aumentar ent_coef a 0.02 |
| `clip_fraction > 0.5` | Updates muy grandes | Reducir clip_range a 0.1 |

## Diferencias con SAC Pipeline

1. **Timesteps vs Episodes**: PPO usa timesteps (no episodios)
2. **Sin Replay Buffer**: PPO es on-policy
3. **GAE**: PPO usa Generalized Advantage Estimation
4. **KL Divergence**: PPO monitorea KL para estabilidad

## Troubleshooting

### Error: "Dataset incompleto"
```bash
python -m scripts.run_oe3_build_dataset --config configs/default.yaml
```

### GPU Out of Memory
Reduce batch_size en c√≥digo o usa `--timesteps` menor para testing.

### Reward negativo persistente
1. Verificar que dataset tiene 8,760 timesteps
2. Revisar que solar_generation > 0
3. Aumentar exploraci√≥n: incrementar ent_coef

## Comparaci√≥n Post-Entrenamiento

Despu√©s de entrenar SAC, PPO y A2C, genera tabla comparativa:
```bash
python -m scripts.run_oe3_co2_table --config configs/default.yaml
```

## Referencias

- [PPO Paper (Schulman et al. 2017)](https://arxiv.org/abs/1707.06347)
- [Stable-Baselines3 PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)
- [GAE Paper](https://arxiv.org/abs/1506.02438)
