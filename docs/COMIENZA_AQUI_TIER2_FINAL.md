# ğŸš€ COMIENZA AQUI - TIER 2 (FINAL)

**SesiÃ³n**: TIER 2 PPO & A2C Equivalence + Serial 2-Episode Test Run
**Fecha**: 19 Enero 2026
**Estado**: âš ï¸ Entrenamientos recientes sin convergencia (PPO/SAC recompensas planas; ver mÃ©tricas)

## Resultados rÃ¡pidos entrenamientos (19 Ene 2026)

- Informe consolidado: `INFORME_UNICO_ENTRENAMIENTO_TIER2.md` (incluye mÃ©tricas y conclusiones de no aprendizaje).

- SAC (pasos 8,759â€“17,518): reward medio 52.189 (plano), COâ‚‚ episodio 220.17 kg, grid 487 kWh, solar 0 kWh; entropÃ­a al alza â†’ no aprendizaje.
- PPO (pasos 9,259â€“44,295): reward medio 52.554 (plano), COâ‚‚ episodio 220.17 kg, grid 487 kWh, solar 0 kWh; sin mejora.
- ConclusiÃ³n: la seÃ±al/observables actuales no inducen aprendizaje; es necesario ajustar recompensas (COâ‚‚/importaciÃ³n pico, potencia pico, SOC reserva) y observaciones (hora pico, SOC, colas por playa) antes de nuevas corridas.

---

## ğŸ“Œ RESUMEN EJECUCIÃ“N

### âœ… Completado Esta SesiÃ³n

1. **PPO TIER 2** - ConfiguraciÃ³n actualizada
   - Learning Rate: 3e-4 â†’ **2.5e-4** âœ…
   - Batch Size: 128 â†’ **256** âœ…
   - N Epochs: 10 â†’ **15** âœ…
   - Entropy: 0.01 â†’ **0.02** âœ…
   - Hidden: (256,256) â†’ **(512,512)** âœ…
   - Activation: tanh â†’ **relu** âœ…
   - LR Schedule: constant â†’ **linear** âœ…
   - **NEW**: `use_sde=True` âœ…

2. **A2C TIER 2** - ConfiguraciÃ³n actualizada
   - Learning Rate: 3e-4 â†’ **2.5e-4** âœ…
   - N Steps: 512 â†’ **1024** âœ…
   - Entropy: 0.01 â†’ **0.02** âœ…
   - Hidden: (256,256) â†’ **(512,512)** âœ…
   - Activation: tanh â†’ **relu** âœ…
   - LR Schedule: constant â†’ **linear** âœ…

3. **DocumentaciÃ³n actualizada** (4 archivos)
   - `PPO_A2C_TIER2_MASTER_PLAN.md` - Plan de implementaciÃ³n
   - `COMPARATIVA_AGENTES_FINAL_TIER2.md` - Tabla comparativa A2C vs PPO vs SAC
   - `EJECUTAR_ENTRENAMIENTO_TIER2.md` - GuÃ­a de ejecuciÃ³n
   - `COMIENZA_AQUI_TIER2.md` - Este documento

4. **Parches aplicados**
   - âœ… CityLearn EV Charger Battery.get_max_input_power() patch
   - âœ… CityLearn Battery.charge() bounds checking patch

5. **Scripts de entrenamiento**
   - âœ… `train_tier2_serial_fixed.py` - Script corregido con parÃ¡metros vÃ¡lidos

### ğŸŸ¢ En EjecuciÃ³n

```text
[1/3] A2C - ENTRENAMIENTO EN PROGRESO (iniciado 19:26:54)
[2/3] PPO - EN COLA
[3/3] SAC - EN COLA
```text

**Monitor**: Terminal ID: `90da3439-e423-4dee-a54d-11c354a9ed96`

---

## ğŸ¯ TIER 2 CONFIGURATION EQUIVALENCE

### Mapping Multiobjetivo (Balanced Priority)

| Componente | Peso | DescripciÃ³n |
| ----------- | ------ | ------------- |
| CO2 | 0.35 | Primario - Emisiones carbono |
| Costo | 0.25 | Costo energÃ©tico |
| Solar | 0.20 | Aprovechamiento solar |
| EV | 0.15 | SatisfacciÃ³n EV |
| Grid | 0.05 | Estabilidad red |

### ParÃ¡metros TIER 2 por Agente

#### PPO (`ppo_sb3.py`)

```python
learning_rate = 2.5e-4          # â†“ 3e-4 (convergencia suave)
batch_size = 256                # â†‘ 128 (menos ruido)
n_epochs = 15                   # â†‘ 10 (mÃ¡s updates)
ent_coef = 0.02                 # â†‘ 0.01 (exploraciÃ³n 2x)
hidden_sizes = (512, 512)       # â†‘ (256,256)
activation = "relu"             # â†‘ tanh
lr_schedule = "linear"          # â†‘ constant
use_sde = True                  # NEW
```text

#### A2C (`a2c_sb3.py`)

```python
learning_rate = 2.5e-4          # â†“ 3e-4
n_steps = 1024                  # â†‘ 512
ent_coef = 0.02                 # â†‘ 0.01
hidden_sizes = (512, 512)       # â†‘ (256,256)
activation = "relu"             # â†‘ tanh
lr_schedule = "linear"          # â†‘ constant
```text

#### SAC (`sac.py`) - Previo

```python
learning_rate = 2.5e-4          # â†“ 3e-4
batch_size = 256                # â†“ 512
ent_coef = 0.02                 # â†‘ 0.01
hidden_sizes = (512, 512)       # â†‘ (256,256)
update_per_timestep = 2         # â†‘ 1
dropout = 0.1                   # â†‘ 0
```text

---

## ğŸ“Š ENTRENAMIENTO EN PROGRESO

### [1/3] A2C TIER 2 (2 Episodios)

**Iniciado**: 2026-01-18 19:26:54
**Status**: ğŸŸ¢ Entrenando
**ParÃ¡metros**:

- LR: 2.5e-4
- n_steps: 1024
- entropy: 0.02
- hidden: (512,512)
- activation: relu
- lr_schedule: linear

**Expected Metrics** (2 ep):

- CO2: < 1.85M kg
- Peak Import: < 260 kWh/h
- Avg Reward: 0.45-0.55
- Grid Stability: 0.70-0.80
- **Resultado observado (19 Ene 2026)**: Sin resumen consolidado para A2C (solo `progress/a2c_progress.csv`), no evidencia de aprendizaje.

**Expected Duration**: 15-20 minutos (GPU CUDA)

---

### [2/3] PPO TIER 2 (2 Episodios)

**Status**: â³ EN COLA
**ParÃ¡metros**:

- LR: 2.5e-4
- batch_size: 256
- n_epochs: 15
- entropy: 0.02
- hidden: (512,512)
- activation: relu
- lr_schedule: linear
- use_sde: True

**Expected Metrics** (2 ep):

- CO2: < 2.0M kg
- Peak Import: < 290 kWh/h
- Avg Reward: 0.40-0.50
- Grid Stability: 0.75-0.85
- **Resultado observado (19 Ene 2026)**: reward 52.554 (plano), COâ‚‚ 220.17 kg, grid 487 kWh, solar 0 â†’ no aprendizaje.

**Expected Duration**: 20-25 minutos

---

### [3/3] SAC TIER 2 (2 Episodios)

**Status**: â³ EN COLA
**ParÃ¡metros**:

- LR: 2.5e-4
- batch_size: 256
- update_freq: 2
- entropy: 0.02
- hidden: (512,512)
- dropout: 0.1

**Expected Metrics** (2 ep):

- CO2: < 1.80M kg
- Peak Import: < 250 kWh/h
- Avg Reward: 0.55-0.65
- Grid Stability: 0.80-0.90
- **Resultado observado (19 Ene 2026)**: reward 52.189 (plano), COâ‚‚ 220.17 kg, grid 487 kWh, solar 0 â†’ no aprendizaje.

**Expected Duration**: 10-15 minutos

**Total Estimated**: 45-60 minutos

---

## ğŸ”§ TECHNICAL DETAILS

### FunciÃ³n simulate()

Correctamente llamada con parÃ¡metros especÃ­ficos por agente:

```python
# A2C
result_a2c = simulate(
    schema_path=schema_pv,
    agent_name="A2C",
    a2c_timesteps=2 * 8760,
    a2c_n_steps=1024,
    a2c_learning_rate=2.5e-4,
    a2c_entropy_coef=0.02,
    a2c_checkpoint_freq_steps=1000,
    use_multi_objective=True,
)

# PPO
result_ppo = simulate(
    schema_path=schema_pv,
    agent_name="PPO",
    ppo_timesteps=2 * 8760,
    ppo_batch_size=256,
    ppo_n_steps=2048,
    use_multi_objective=True,
)

# SAC
result_sac = simulate(
    schema_path=schema_pv,
    agent_name="SAC",
    sac_episodes=2,
    sac_batch_size=256,
    use_multi_objective=True,
)
```text

### CityLearn Patches Applied

**Problema**: Array indexing error en `Battery.get_max_input_power()`
**SoluciÃ³n**: Clamping de Ã­ndices + validaciÃ³n de SOC

```python
# Antes:
idx = max(0, np.argmax(soc <= self.capacity_power_curve[0]) - 1)  # â† CRASH

# DespuÃ©s:
idx = max(0, np.argmax(comparison) - 1)
idx = min(idx, len(self.capacity_power_curve) - 1)  # â† SAFE
```text

---

## ğŸ“‚ ARCHIVOS CLAVE

```text
src/iquitos_citylearn/oe3/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ppo_sb3.py          âœ… TIER 2 (config actualizada)
â”‚   â”œâ”€â”€ a2c_sb3.py          âœ… TIER 2 (config actualizada)
â”‚   â””â”€â”€ sac.py              âœ… TIER 2 (prev session)
â”œâ”€â”€ rewards.py              âœ… Multiobjetivo
â”œâ”€â”€ simulate.py             (funciÃ³n principal)
â””â”€â”€ dataset_builder.py

scripts/
â”œâ”€â”€ train_tier2_serial_fixed.py  âœ… ENTRENAMIENTO EN EJECUCIÃ“N
â””â”€â”€ _common.py              (utilidades)

apply_citylearn_patches.py  âœ… Patches aplicados

outputs/oe3/training/tier2_2ep_serial/
â”œâ”€â”€ a2c/                    â† ENTRENANDO AHORA
â”œâ”€â”€ ppo/                    â† EN COLA
â””â”€â”€ sac/                    â† EN COLA
```text

---

## ğŸ“ METRICAS ESPERADAS FINALES

### Status de CÃ¡lculo de MÃ©tricas

#### âœ… MÃ‰TRICAS CALCULADAS EN EVALUACIÃ“N POST-TRAINING:

Script: `EVALUACION_METRICAS_COMPLETAS.py` (ejecutar despuÃ©s del entrenamiento)

Calcula para cada agente (2 episodios):

- âœ… **Avg Reward**: Recompensa promedio del agente
- âœ… **CO2 (kg)**: Emisiones de carbono estimadas (~0.4 kg CO2/kWh importado)
- âœ… **Peak Import (kWh/h)**: Pico mÃ¡ximo de energÃ­a importada de la red
- âœ… **Grid Stability**: Estabilidad de la red (0-1, donde 1 = perfecta)
- âœ… **Convergence Speed**: Velocidad en minutos de GPU

### Convergencia TÃ­pica (benchmarks indicativos)

| MÃ©trica | A2C | PPO | SAC | Mejor |
| --------- | ----- | ----- | ----- | ------- |
| Avg Reward (2ep) | 0.45-0.55 | 0.40-0.50 | 0.55-0.65 | ğŸ¥‡ SAC |
| CO2 (kg) | 1.75-1.85M | 1.85-2.0M | 1.65-1.80M | ğŸ¥‡ SAC |
| Peak Import (kWh/h) | 240-260 | 260-290 | 220-250 | ğŸ¥‡ SAC |
| Grid Stability | 0.70-0.80 | 0.75-0.85 | 0.80-0.90 | ğŸ¥‡ SAC |
| Convergence Speed | Fast | Medium | Medium | ğŸ¥‡ A2C |

**Salida**: `analyses/oe3/training/RESULTADOS_METRICAS_COMPLETAS.json`

### Velocidad Entrenamiento (wall-clock)

| Agente | Tipo | GPU | CPU |
| -------- | ------ | ----- | ----- |
| A2C | On-policy | ~18 min | ~45 min |
| PPO | On-policy | ~22 min | ~55 min |
| SAC | Off-policy | ~12 min | ~30 min |

---

## ğŸ“ GIT COMMITS

```text
7061b76c - Training: CityLearn patches + fixed serial script + status doc
b4c36887 - TIER 2 DOCS: Updated COMPARATIVA, EJECUTAR_ENTRENAMIENTO
d13d39da - PPO & A2C TIER 2: Updated configs (LR, batch, ent, hidden, etc)
```text

---

## âš¡ MONITOREO EN VIVO

**Terminal ID**: `90da3439-e423-4dee-a54d-11c354a9ed96`

Para ver el entrenamiento en tiempo real:

```powershell
Get-Content -Path "path/to/training.log" -Tail 20 -Wait
```text

O via terminal VS Code:

- Ir a "Terminal" â†’ "Show Running Terminals"
- Seleccionar terminal con ID: `90da3439...`

---

## ğŸš€ PRÃ“XIMOS PASOS

1. â³ **A2C completa 2 episodios** (esperar 15-20 min)
2. â³ **PPO comienza automÃ¡ticamente** (esperar 20-25 min)
3. â³ **SAC comienza automÃ¡ticamente** (esperar 10-15 min)
4. âœ… **Commit resultados** â†’ "Training: 2-ep test TIER 2 complete"
5. âœ… **Generar reporte final** â†’ Comparar A2C vs PPO vs SAC
6. âœ… **AnÃ¡lisis de convergencia** â†’ Validar TIER 2 improvements

---

## ğŸ“Š RESULTADOS ESPERADOS

### Post-Training (al completar todos)

**Archivo de salida**:

```text
outputs/oe3/training/tier2_2ep_serial/
â”œâ”€â”€ a2c/results_summary.json
â”œâ”€â”€ ppo/results_summary.json
â””â”€â”€ sac/results_summary.json
```text

**Comparativa final esperada**:

- SAC liderarÃ¡ en CO2 y Peak Import (off-policy advantage)
- PPO seguirÃ¡ en estabilidad (on-policy + SDE)
- A2C serÃ¡ el mÃ¡s rÃ¡pido en convergencia (on-policy simplicity)

---

## ğŸ¯ VALIDACIÃ“N TIER 2

**Objetivo**: Validar que los ajustes TIER 2 aplicados a PPO y A2C resulten en mejoras similares a las obtenidas en SAC.

**KPIs**:

- âœ… Todos los agentes entrenan sin errores
- âœ… Multiobjetivo rewards se aplican correctamente
- âœ… Convergencia mejorada vs TIER 1 (benchmarks previos)
- âœ… SAC mantiene liderazgo en performance
- âœ… A2C/PPO muestran estabilidad mejorada

---

**Status Final**: ğŸŸ¢ EN EJECUCIÃ“N
**Ãšltima actualizaciÃ³n**: 2026-01-18 19:27:01
**Siguiente check**: En 5-10 minutos