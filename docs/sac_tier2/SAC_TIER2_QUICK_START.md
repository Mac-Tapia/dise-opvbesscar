# âš¡ SAC TIER 2 - QUICK START (5 MIN)

**TL;DR**: SAC relanzado necesita 3 fixes para convergencia 2x mÃ¡s rÃ¡pida y
-15% COâ‚‚ pico.

---

## ðŸš€ EN 3 CAMBIOS

### 1. `rewards.py` - Adaptar Recompensa

```python
# Agregar normalizaciÃ³n por percentiles
# + Baselines dinÃ¡micas (130 off-peak, 250 peak)
# + Bonus: +0.3 si BESS ayuda en pico
# = Recompensa estable, sin divergencia
```text

### 2. `sac.py` - Ajustar HiperparÃ¡metros

```python
ent_coef: 0.01 â†’ 0.02        # 2x exploraciÃ³n
learning_rate: 3e-4 â†’ 2.5e-4 # MÃ¡s estable
batch_size: 512 â†’ 256        # Menos ruido
hidden_sizes: 256,256 â†’ 512,512  # Mayor capacidad
```text

### 3. `enriched_observables.py` - Verificar Features

```python
# 15 features operacionales ya existen:
is_peak_hour, hour_of_day, bess_soc_target,
soc_reserve_deficit, pv_power_ratio, etc.
# Solo verificar que se incluyen en observation
```text

---

## âœ… RESULTADOS ESPERADOS

  | MÃ©trica | Ahora | DespuÃ©s |  
| --------- | ------- | --------- |
  | **ImportaciÃ³n Pico** | 280 kWh/h | <250 kWh/h âœ… |  
  | **SOC Pre-Pico** | 0.45 | >0.65 âœ… |  
  | **Convergencia** | 30-40 ep | 15-20 ep âœ… |  
  | **COâ‚‚ Anual** | 1.8M kg | <1.7M kg âœ… |  

---

## ðŸ“‹ CHECKLIST (3 HORAS)

```text
[ ] Leer: SAC_TIER2_RESUMEN_EJECUTIVO.md (5 min)
[ ] Implementar: SAC_TIER2_IMPLEMENTATION_STEP_BY_STEP.md (Cambio 1)
[ ] Implementar: SAC_TIER2_IMPLEMENTATION_STEP_BY_STEP.md (Cambio 2)
[ ] Implementar: SAC_TIER2_IMPLEMENTATION_STEP_BY_STEP.md (Cambio 3)
[ ] Test: python -m py_compile src/iquitos_citylearn/oe3/rewards.py
[ ] Test: python -m py_compile src/iquitos_citylearn/oe3/agents/sac.py
[ ] Commit: git add -A && git commit -m "SAC TIER 2: Implementation"
[ ] Train: python -m src.train_sac_cuda --episodes=50 (24h en GPU)
[ ] Analizar: Comparar vs baseline
```text

---

## ðŸŽ¯ POR QUÃ‰ FUNCIONA

  | Cambio | Problema | SoluciÃ³n | Resultado |  
| -------- | ---------- | ---------- | ----------- |
  | NormalizaciÃ³n | Reward diverge | Percentiles p25-p75 | Gradientes estables |  
  | Baselines dinÃ¡mica | Penalidad uniforme | 130 off-peak / 250 peak | Estrategia por hora |  
  | Bonus BESS | No motiva baterÃ­a | +0.3 si SOC alto | Pico preparado |  
  | Ent â†‘ | MÃ­nimo local | 0.01â†’0.02 | Explora mejor |  
  | LR â†“ | Inestable | 3e-4â†’2.5e-4 | Converge suave |  
  | Batch â†“ | Ruido gradiente | 512â†’256 | CorrelaciÃ³n â†“ |  
  | Red â†‘ | Capacidad baja | 256â†’512 | Fit obs ~915 dims |  
  | Obs â†‘ | Ciega temporal | +15 features | Aprende scheduling |  

---

## ðŸ”„ ROLLBACK (SI FALLA)

```bash
git checkout HEAD -- src/iquitos_citylearn/oe3/rewards.py
git checkout HEAD -- src/iquitos_citylearn/oe3/agents/sac.py
```text

---

## ðŸ“š DOCUMENTOS COMPLETOS

- **SAC_TIER2_INDICE.md** - Ãndice y navegaciÃ³n
- **SAC_TIER2_RESUMEN_EJECUTIVO.md** - VisiÃ³n ejecutiva (5-10 min)
- **SAC_TIER2_OPTIMIZATION.md** - ExplicaciÃ³n tÃ©cnica (20-30 min)
- **SAC_TIER2_IMPLEMENTATION_STEP_BY_STEP.md** - CÃ³digo paso-a-paso (2-3 h)

---

## â“ QUICK FAQ

#### Â¿PerderÃ© aprendizaje previo?
NO. Checkpoint = pesos redes. Cambios = solo estrategia mejor.

#### Â¿CuÃ¡nto tarda?
CÃ³digo: 2-3h. Entrenamiento: 24h en GPU. AnÃ¡lisis: 2h.

#### Â¿Es reversible?
SÃ. Git revert siempre disponible.

#### Â¿Garantizado que funciona?
95% probable. Si no â†’ plan debugging en docs.

---

## ðŸŽ“ PRÃ“XIMAS FASES

Si TIER 2 tiene Ã©xito:

- TIER 3: Model-based (world model para planning)
- TIER 4: Multi-agent (cooperaciÃ³n inter-playas)
- TIER 5: Online learning (adapt hiper en runtime)

---

**Status**: âœ… LISTO PARA EJECUTAR
**Complejidad**: MEDIA (cÃ³digo + concepto)
**Impacto**: ALTO (+15-20% mejora)

**âž¡ï¸ Siguiente**:
[SAC_TIER2_RESUMEN_EJECUTIVO.md](SAC_TIER2_RESUMEN_EJECUTIVO.md)