# SAC TIER 2 OPTIMIZATION - POST-RELANZAMIENTO

**Estado**: PLAN EJECUTABLE (SAC relanzado con LR corregido)
**Fecha**: 2025-02-13
**Objetivo**: Maximizar convergencia SAC despu√©s del relanzamiento LR 3e-4

---

## üìã AN√ÅLISIS SITUACI√ìN ACTUAL

### ‚úÖ ESTADO SAC AHORA

- **Learning Rate**: 3e-4 (corregido de 1e-3)
- **Entrop√≠a**: ent_coef=0.01 fijo (no auto)
- **Target Entropy**: -50.0 (menos exploraci√≥n que -126.0)
- **Batch Size**: 512
- **Buffer**: 100k transitions
- **Episodes**: 50 (m√≠nimo para ~900 obs dims √ó 126 act dims)

### ‚ö†Ô∏è PROBLEMAS IDENTIFICADOS EN TIER 1

1. **Recompensa**: Pesos mal distribuidos, sin normalizaci√≥n adaptativa
2. **Observables**: Faltan flags de pico, SOC din√°mica, colas por playa
3. **Hiperpar√°metros**: ent_coef podr√≠a ser a√∫n mayor; target_entropy podr√≠a
ajustarse

---

## üéØ TIER 2 FIXES - IMPLEMENTACI√ìN INMEDIATA

### A. RECOMPENSA - NORMALIZACI√ìN ADAPTATIVA

**Cambio**: Implementar running statistics y normalizaci√≥n por percentiles

```python
# En src/iquitos_citylearn/oe3/rewards.py: MultiObjectiveReward.__init__

class MultiObjectiveReward:
    def __init__(self, weights=None, context=None, adapt_rewards=True):
        # ... c√≥digo existente ...

        # NEW: Estad√≠sticas adaptativas por componente
        self._component_history = {
            "r_co2": [],
            "r_cost": [],
            "r_solar": [],
            "r_ev": [],
            "r_grid": [],
        }
        self._history_size = 500  # Rolling window
        self._adapt_rewards = adapt_rewards
        self._reward_percentiles = {k: (0.0, 1.0) for k in self._component_history}
```text

**L√≥gica**:

- Guardar √∫ltimo 500 rewards por componente
- Calcular p25 y p75
- Normalizar cada componente al rango [p25, p75] ‚Üí [-1, 1]

---

### B. FUNCI√ìN COMPUTE() - BASELINES DIN√ÅMICAS

**Cambio**: Ajustar baselines seg√∫n hora y estado

```python
def compute(self, grid_import_kwh, grid_export_kwh, solar_generation_kwh,
            ev_charging_kwh, ev_soc_avg, bess_soc, hour, ev_demand_kwh=0.0):

    components = {}
    is_peak = hour in self.context.peak_hours  # [18, 19, 20, 21]

    # ========== CO‚ÇÇ RECOMPENSA (50% del peso) ==========
    co2_kg = grid_import_kwh * self.context.co2_factor_kg_per_kwh

    # BASELINES DIN√ÅMICAS (no fijas)
    co2_baseline_offpeak = 130.0  # kWh/hora t√≠pico off-peak
    co2_baseline_peak = 250.0     # kWh/hora target con BESS en pico

    if is_peak:
        # Penalidad EXPONENCIAL en pico (no lineal)
        # Si importas 250 ‚Üí r_co2 = 1 - 2*(250/250) = -1
        # Si importas 100 ‚Üí r_co2 = 1 - 2*(100/250) = 0.2
        # Si importas 50  ‚Üí r_co2 = 1 - 2*(50/250) = 0.6
        r_co2_raw = 1.0 - 2.0 * min(1.0, grid_import_kwh / co2_baseline_peak)

        # BONUS si batter√≠a contribuy√≥ a bajar importaci√≥n
        bess_contribution = max(0, bess_soc - 0.40)  # SOC > 40% en pico
        r_co2 = r_co2_raw + 0.3 * bess_contribution  # Bonus +0.3 si SOC bien
    else:
        r_co2_raw = 1.0 - 1.0 * min(1.0, grid_import_kwh / co2_baseline_offpeak)
        r_co2 = r_co2_raw  # Sin bonus off-peak

    r_co2 = np.clip(r_co2, -1.0, 1.0)
    components["r_co2"] = r_co2
    components["co2_kg"] = co2_kg

    # ========== ESTABILIDAD GRID (10% ‚Üí aumentar a 15%) ==========
    demand_ratio = grid_import_kwh / self.context.peak_demand_limit_kw

    if is_peak:
        # Penalidad MUY fuerte en pico si superas 200 kW
        if demand_ratio > 1.0:
            r_grid = -1.0  # Violaci√≥n severa
        else:
            r_grid = 1.0 - 3.0 * demand_ratio  # Gradientes m√°s fuertes
    else:
        r_grid = 1.0 - 1.5 * min(1.0, demand_ratio)

    r_grid = np.clip(r_grid, -1.0, 1.0)
    components["r_grid"] = r_grid

    # ... resto de componentes igual ...

    # RECOMPENSA TOTAL - TIER 2: Pesos rebalanceados
    reward = (
        0.50 * r_co2 +      # PRIMARY: minimizar CO‚ÇÇ
        0.15 * r_grid +     # SECUNDARIO: estabilidad (+5%)
        0.20 * r_solar +    # Autoconsumo
        0.10 * r_ev +       # Satisfacci√≥n EV
        0.05 * r_cost       # Costo m√≠nimo
    )

    reward = np.clip(reward, -1.0, 1.0)
    components["reward_total"] = reward
    return reward, components
```text

**Cambios clave**:

1. ‚úÖ Bonus por SOC en pico (anima a cargar bater√≠a)
2. ‚úÖ Penalidad exponencial si superas l√≠mite (violaci√≥n severa = -1.0)
3. ‚úÖ Grid stability peso +5% (0.10 ‚Üí 0.15)

---

### C. OBSERVABLES - ENRIQUECIMIENTO

**Cambio**: Incluir flags operacionales en observation space

**Ubicaci√≥n**: `src/iquitos_citylearn/oe3/enriched_observables.py`

**Observables a a√±adir** (ya existen, solo asegurar inclusi√≥n):

```python
enriched_state = {
    "is_peak_hour": 1 if hour in [18,19,20,21] else 0,           # Flag pico
    "hour_of_day": float(hour),                                    # Hora [0-23]
    "bess_soc_current": bess_soc,                                  # SOC [0-1]
    "bess_soc_target": soc_target_dinamico,                       # SOC objetivo din√°mico
    "bess_soc_reserve_deficit": max(0, soc_target - bess_soc),   # D√©ficit reserva
    "pv_power_available_kw": pv_power,                            # FV disponible
    "pv_power_ratio": pv_power / (ev_power + 0.1),              # Cobertura FV
    "grid_import_kw": grid_import,                                # Importaci√≥n actual
    "ev_power_motos_kw": power_motos,                            # Motos [kW]
    "ev_power_mototaxis_kw": power_mototaxis,                    # Mototaxis [kW]
    "ev_power_fairness_ratio": max_power / min_power,            # Equilibrio playas
}
```text

**Dimensi√≥n**:

- Base (CityLearn): ~900 dims
- - Enriquecimiento: +15 dims
- **Total**: ~915 dims (a√∫n dentro de capacidad)

**Utilidad**:

- ‚úÖ Red aprende a reconocer horas pico autom√°ticamente
- ‚úÖ SOC target din√°mico ‚Üí estrategia preparaci√≥n pre-pico
- ‚úÖ Fairness flag ‚Üí evita sobrecargar una playa
- ‚úÖ PV ratio ‚Üí incentiva uso solar en tiempo real

---

### D. HIPERPAR√ÅMETROS - AJUSTES TIER 2

#### D.1 Entrop√≠a

**Cambio Propuesto**:

```python
ent_coef: float = 0.02        # Aumentar de 0.01 (m√°s exploraci√≥n inicial)
target_entropy: float = -40.0  # Reducir penalidad (de -50.0)
```text

**Justificaci√≥n**:

- `ent_coef=0.02` ‚Üí 2x exploraci√≥n (evita m√≠nimos locales)
- `target_entropy=-40.0` ‚Üí Red puede ser m√°s determin√≠stica (mejor control)
- Rango exploraci√≥n sigue siendo restringido (vs -126.0)

#### D.2 Learning Rates

**Cambio Propuesto**:

```python
# En SACConfig dataclass:
learning_rate: float = 2.5e-4  # Bajar de 3e-4 (m√°s estable)
critic_lr: float = 2.5e-4      # Critic LR
actor_lr: float = 2.5e-4       # Actor LR
alpha_lr: float = 1e-4         # LR para alpha (entrop√≠a)
```text

**Justificaci√≥n**:

- SAC es sensible a LR (mejor convergencia con LR menor)
- 2.5e-4 es sweet spot entre velocidad y estabilidad
- alpha_lr peque√±o (1e-4) ‚Üí ajuste lento de entrop√≠a

#### D.3 Batch & Buffer

**Cambio Propuesto**:

```python
batch_size: int = 256           # Bajar de 512 (menos ruido)
buffer_size: int = 150000       # Aumentar de 100k (m√°s diversidad)
update_per_timestep: int = 2    # 2 updates por step (vs 1)
```text

**Justificaci√≥n**:

- Batch menor ‚Üí gradientes menos ruidosos
- Buffer mayor ‚Üí experiencia m√°s diversa
- 2 updates ‚Üí cr√≠tico entrenado m√°s frecuentemente

#### D.4 Red Neuronal

**Cambio Propuesto**:

```python
hidden_sizes: tuple = (512, 512)  # Aumentar de (256, 256)
hidden_activation: str = "relu"   # Mantener
use_dropout: bool = True          # NUEVO: regularizaci√≥n
dropout_rate: float = 0.1         # 10% regularizaci√≥n
```text

**Justificaci√≥n**:

- Redes 512x512 m√°s expresivas (alto-dimensional obs)
- Dropout evita overfitting en pesos
- SAC combina bien con redes grandes + dropout

---

## üöÄ PLAN DE IMPLEMENTACI√ìN

### Fase 1: C√≥digo (2 horas)

- [ ] **rewards.py**:
  - Agregar `_component_history` y stats adaptativas
  - Modificar `compute()` con baselines din√°micas y bonuses
  - Rebalancear pesos (0.50/0.15/0.20/0.10/0.05)

- [ ] **sac.py**:
  - Actualizar `SACConfig` (ent_coef, learning_rates, batch_size, etc.)
  - Verificar que observables enriquecidos se pasan al modelo

- [ ] **enriched_observables.py**:
  - Asegurar que todos los 15 features se incluyen
  - Testar `get_enriched_state()` con valores reales

### Fase 2: Validaci√≥n (1 hora)

- [ ] Cargar checkpoint actual de SAC
- [ ] Ejecutar 1 episodio de test
- [ ] Verificar:
  - ‚úÖ Observation shape = (915,)
  - ‚úÖ Reward en rango [-1, 1]
  - ‚úÖ Sin NaN/Inf

### Fase 3: Entrenamiento (24 horas en GPU)

- [ ] Entrenar con Fase 1 fixes
- [ ] Monitorear:
  - Reward promedio por hora (especialmente picos)
  - Importaci√≥n grid (target: <250 kWh en pico)
  - SOC pre-pico (target: >60% en horas 16-17)

### Fase 4: An√°lisis (2 horas)

- [ ] Comparar vs baseline (A2C, SAC sin fixes)
- [ ] Generar reporte de mejoras
- [ ] Graficar convergencia

---

## üìä M√âTRICAS √âXITO TIER 2 | M√©trica | Baseline | Target TIER 2 | C√≥mo Medir | | --- | ---------- | --- | ----------- | | **Importaci√≥n Pico (kWh/hora)** | 280-300 | <250 | Promedio horas 18-21 | | **Importaci√≥n Off-Peak (kWh/hora)** | 120-140 | <130 | Promedio horas 0-8 | | **SOC Pre-Pico (16-17h)** | 0.45-0.55 | >0.65 | Promedio horas 16-17 | | **SOC Pico (18-21h)** | 0.20-0.30 | >0.35 | Promedio horas 18-21 | | **CO‚ÇÇ Total A√±o (kg)** | ~1.8e6 | <1.7e6 | Integraci√≥n anual | |**Reward Convergencia**|Lento (~ep 30)|R√°pido (~ep 15)|Episode smoothed| | **Fairness (motos/mototaxis)** | 1.2-1.5 | <1.1 | Ratio m√°x/m√≠n | ---

## üîç DEBUGGING ESPERADO

### Si Reward diverge

- Bajar `ent_coef` a 0.01
- Reducir `learning_rate` a 2e-4

### Si Importaci√≥n sigue alta

- Aumentar peso CO‚ÇÇ de 0.50 a 0.60
- Bajar baseline pico de 250 a 220

### Si SOC se drena

- Aumentar bonus BESS en `r_co2` de 0.3 a 0.5
- Penalizar m√°s si `bess_soc < 0.30` en pico

### Si converge muy lento

- Aumentar `batch_size` a 512
- Aumentar `update_per_timestep` a 3

---

## üìù CHECKLIST EJECUCI√ìN

```text
FASE 1: C√ìDIGO
---
[ ] Crear archivo: SAC_TIER2_IMPLEMENTATION.md (paso a paso)
[ ] Editar rewards.py - componentes adaptativas
[ ] Editar rewards.py - baselines din√°micas
[ ] Editar rewards.py - pesos rebalanceados
[ ] Editar sac.py - SACConfig actualizado
[ ] Editar sac.py - incluir observables enriquecidos
[ ] Compilar/Linter check
[ ] Commit: "SAC TIER 2: Normalizaci√≥n adaptativa + observables enriquecidos"

FASE 2: VALIDACI√ìN
---
[ ] Test reshape observation: esperado (915,)
[ ] Test reward output: rango [-1, 1]
[ ] Test step(): sin NaN/Inf
[ ] Cargar checkpoint SAC existente
[ ] 1 episodio forward pass
[ ] Verificar gradientes (no exploding/vanishing)

FASE 3: ENTRENAMIENTO
---
[ ] Ejecutar: python -m src.train_sac_cuda --episodes=50
[ ] Monitorear GPU: nvidia-smi
[ ] Graficar progreso cada 2 episodios
[ ] Checkpoint cada episodio

FASE 4: AN√ÅLISIS
---
[ ] Generar dashboard de convergencia
[ ] Comparar vs A2C baseline
[ ] Reportar mejoras
[ ] Identificar pr√≥ximos fixes (TIER 3)

```text

---

## üéì REFERENCIAS TE√ìRICAS

### Por qu√© estos cambios funcionan

1. **Normalizaci√≥n Adaptativa** (rewards.py)
   - SAC es algoritmo on-policy (sensible a escala)
   - Normalizar por percentiles evita saturation
   - Permite pesos m√°s simples (no necesita tuning manual)

2. **Baselines Din√°micas** (rewards.py)
   - Baselines fijos ignoran contexto temporal
   - Baselines = target realista por hora
   - Diferencia = (actual - target) = signal RL

3. **Observables Enriquecidos** (enriched_obs.py)
   - Red neuronal explora mejor con **state features**
   - Flags de pico = aprender scheduling
   - SOC din√°mico = aprender reserva
   - Fairness = aprender coordinaci√≥n multi-playa

4. **Hiperpar√°metros TIER 2** (sac.py)
   - SAC necesita alta entrop√≠a (exploraci√≥n)
   - LR menor = convergencia m√°s estable
   - Batch peque√±o = menos correlaci√≥n
   - Redes grandes = capacidad expresiva

---

## üìû CONTACTO PARA ISSUES

Si durante implementaci√≥n encuentras:

- **Valores NaN**: Check normalizaci√≥n dividir por cero
- **Reward siempre negativo**: Ajustar baselines
- **GPU memory**: Reducir batch_size de 256 a 128
- **Convergencia lenta**: Aumentar update_per_timestep

---

**Pr√≥ximas fases**: TIER 3 (model-based predictions), TIER 4 (multi-agent
coordination)
