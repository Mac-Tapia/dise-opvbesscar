# SAC TIER 2 OPTIMIZATION - POST-RELANZAMIENTO

**Estado**: PLAN EJECUTABLE (SAC relanzado con LR corregido)
**Fecha**: 2025-02-13
**Objetivo**: Maximizar convergencia SAC despu√©s del relanzamiento LR 3e-4

---

## üìã AN√ÅLISIS SITUACI√ìN ACTUAL

### ‚úÖ ESTADO SAC AHORA

- **Learning Rate**: 3e-4 (corregido de 1e-3)
- **Entrop√≠a**: ent_coef=0.01 fijo (no auto)
- **Target Entropy**: -50.0 (menos exploraci√≥n que -126.0)
- **Batch Size**: 512
- **Buffer**: 100k transitions
- **Episodes**: 50 (m√≠nimo para ~900 obs dims √ó 126 act dims)

### ‚ö†Ô∏è PROBLEMAS IDENTIFICADOS EN TIER 1

1. **Recompensa**: Pesos mal distribuidos, sin normalizaci√≥n adaptativa
2. **Observables**: Faltan flags de pico, SOC din√°mica, colas por playa
3. **Hiperpar√°metros**: ent_coef podr√≠a ser a√∫n mayor; target_entropy podr√≠a
ajustarse

---

## üéØ TIER 2 FIXES - IMPLEMENTACI√ìN INMEDIATA

### A. RECOMPENSA - NORMALIZACI√ìN ADAPTATIVA

**Cambio**: Implementar running statistics y normalizaci√≥n por percentiles

<!-- markdownlint-disable MD013 -->
```python
# En src/iquitos_citylearn/oe3/rewards.py: MultiObjectiveReward.__init__

class MultiObjectiveReward:
    def __init__(self, weights=None, context=None, adapt_rewards=True):
        # ... c√≥digo existente ...

        # NEW: Estad√≠sticas adaptativas por componente
        self._component_history = {
            "r_co2": [],
            "r_cost": [],
            "r_solar": [],
            "r_ev": [...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

**L√≥gica**:

- Guardar √∫ltimo 500 rewards por componente
- Calcular p25 y p75
- Normalizar cada componente al rango [p25, p75] ‚Üí [-1, 1]

---

### B. FUNCI√ìN COMPUTE() - BASELINES DIN√ÅMICAS

**Cambio**: Ajustar baselines seg√∫n hora y estado

<!-- markdownlint-disable MD013 -->
```python
def compute(self, grid_import_kwh, grid_export_kwh, solar_generation_kwh,
            ev_charging_kwh, ev_soc_avg, bess_soc, hour, ev_demand_kwh=0.0):

    components = {}
    is_peak = hour in self.context.peak_hours  # [18, 19, 20, 21]

    # ========== CO‚ÇÇ RECOMPENSA (50% del peso) ==========
    co2_kg = grid_import_kwh * self.context.co2_factor_kg_per_kwh

    # BASELINES DIN√ÅMICAS (no fijas)...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

**Cambios clave**:

1. ‚úÖ Bonus por SOC en pico (anima a cargar bater√≠a)
2. ‚úÖ Penalidad exponencial si superas l√≠mite (violaci√≥n severa = -1.0)
3. ‚úÖ Grid stability peso +5% (0.10 ‚Üí 0.15)

---

### C. OBSERVABLES - ENRIQUECIMIENTO

**Cambio**: Incluir flags operacionales en observation space

**Ubicaci√≥n**: `src/iquitos_citylearn/oe3/enriched_observables.py`

**Observables a a√±adir** (ya existen, solo asegurar inclusi√≥n):

<!-- markdownlint-disable MD013 -->
```python
enriched_state = {
    "is_peak_hour": 1 if hour in [18,19,20,21] else 0,           # Flag pico
    "hour_of_day": float(hour),                                    # Hora [0-23]
    "bess_soc_current": bess_soc,                                  # SOC [0-1]
    "bess_soc_target": soc_target_dinamico,                       # SOC objetivo din√°mico
    "bess_soc_reserve_deficit": max(0, soc_target - be...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

**Dimensi√≥n**:

- Base (CityLearn): ~900 dims
- - Enriquecimiento: +15 dims
- **Total**: ~915 dims (a√∫n dentro de capacidad)

**Utilidad**:

- ‚úÖ Red aprende a reconocer horas pico autom√°ticamente
- ‚úÖ SOC target din√°mico ‚Üí estrategia preparaci√≥n pre-pico
- ‚úÖ Fairness flag ‚Üí evita sobrecargar una playa
- ‚úÖ PV ratio ‚Üí incentiva uso solar en tiempo real

---

### D. HIPERPAR√ÅMETROS - AJUSTES TIER 2

#### D.1 Entrop√≠a

**Cambio Propuesto**:

<!-- markdownlint-disable MD013 -->
```python
ent_coef: float = 0.02        # Aumentar de 0.01 (m√°s exploraci√≥n inicial)
target_entropy: float = -40.0  # Reducir penalidad (de -50.0)
```text
<!-- markdownlint-enable MD013 -->

**Justificaci√≥n**:

- `ent_coef=0.02` ‚Üí 2x exploraci√≥n (evita m√≠nimos locales)
- `target_entropy=-40.0` ‚Üí Red puede ser m√°s determin√≠stica (mejor control)
- Rango exploraci√≥n sigue siendo restringido (vs -126.0)

#### D...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

**Justificaci√≥n**:

- SAC es sensible a LR (mejor convergencia con LR menor)
- 2.5e-4 es sweet spot entre velocidad y estabilidad
- alpha_lr peque√±o (1e-4) ‚Üí ajuste lento de entrop√≠a

#### D.3 Batch & Buffer

**Cambio Propuesto**:

<!-- markdownlint-disable MD013 -->
```python
batch_size: int = 256           # Bajar de 512 (menos ruido)
buffer_size: int = 150000       # Aumentar de 100k (m√°s diversidad)
update_per_timestep: int = 2    # 2 updates por step (vs 1)
```text
<!-- markdownlint-enable MD013 -->

**Justificaci√≥n**:

- Batch menor ‚Üí gradientes menos ruidosos
- Buffer mayor ‚Üí experiencia m√°s diversa
- 2 updates ‚Üí cr√≠tico entrenado m√°s frecuentemente

#### D.4 Red...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

**Justificaci√≥n**:

- Redes 512x512 m√°s expresivas (alto-dimensional obs)
- Dropout evita overfitting en pesos
- SAC combina bien con redes grandes + dropout

---

## üöÄ PLAN DE IMPLEMENTACI√ìN

### Fase 1: C√≥digo (2 horas)

- [ ] **rewards.py**:
  - Agregar `_component_history` y stats adaptativas
  - Modificar `compute()` con baselines din√°micas y bonuses
  - Rebalancear pesos (0.50/0.15/0.20/0.10/0.05)

- [ ] **sac.py**:
  - Actualizar `SACConfig` (ent_coef, learning_rates, batch_size, etc.)
  - Verificar que observables enriquecidos se pasan al modelo

- [ ] **enriched_observables.py**:
  - Asegurar que todos los 15 features se incluyen
  - Testar `get_enriched_state()` con valores reales

### Fase 2: Validaci√≥n (1 hora)

- [ ] Cargar checkpoint actual de SAC
- [ ] Ejecutar 1 episodio de test
- [ ] Verificar:
  - ‚úÖ Observation shape = (915,)
  - ‚úÖ Reward en rango [-1, 1]
  - ‚úÖ Sin NaN/Inf

### Fase 3: Entrenamiento (24 horas en GPU)

- [ ] Entrenar con Fase 1 fixes
- [ ] Monitorear:
  - Reward promedio por hora (especialmente picos)
  - Importaci√≥n grid (target: <250 kWh en pico)
  - SOC pre-pico (target: >60% en horas 16-17)

### Fase 4: An√°lisis (2 horas)

- [ ] Comparar vs baseline (A2C, SAC sin fixes)
- [ ] Generar reporte de mejoras
- [ ] Graficar convergencia

---

<!-- markdownlint-disable MD013 -->
## üìä M√âTRICAS √âXITO TIER 2 | M√©trica | Baseline | Target TIER 2 | C√≥mo Medir | | --- | ---------- | --- | ----------- | | **Importaci√≥n Pico (kWh/hora)** | 280-300 | <250 | Promedio horas 18-21 | | **Importaci√≥n Off-Peak (kWh/hora)** | 120-140 | <130 | Promedio horas 0-8 | | **SOC Pre-Pico (16-17h)** | 0.45-0.55 | >0.65 | Promedio horas 16-17 | | **SOC Pico (18-21h)** | 0.20-0.30 | >0.35 | Promedio horas 18-21 | | **CO‚ÇÇ Total A√±o (kg)** | ~1.8e6 | <1.7e6 | Integraci√≥n anual | |**Reward Convergencia**|Lento (~ep 30)|R√°pido (~ep 15)|Episode smoothed| | **Fairness (motos/mototaxis)** | 1.2-1.5 | <1.1 | Ratio m√°x/m√≠n | ---

## üîç DEBUGGING ESPERADO

### Si Reward diverge

- Bajar `ent_coef` a 0.01
- Reducir `learning_rate` a 2e-4

### Si Importaci√≥n sigue alta

- Aumentar peso CO‚ÇÇ de 0.50 a 0.60
- Bajar baseline pico de 250 a 220

### Si SOC se drena

- Aumentar bonus BESS en `r_co2` de 0.3 a 0.5
- Penalizar m√°s si `bess_soc < 0.30` en pico

### Si converge muy lento

- Aumentar `batch_size` a 512
- Aumentar `update_per_timestep` a 3

---

## üìù CHECKLIST EJECUCI√ìN

<!-- markdownlint-disable MD013 -->
```text
FASE 1: C√ìDIGO
---
[ ] Crear archivo: SAC_TIER2_IMPLEMENTATION.md (paso a paso)
[ ] Editar rewards.py - componentes adaptativas
[ ] Editar rewards.py - baselines din√°micas
[ ] Editar rewards.py - pesos rebalanceados
[ ] Editar sac.py - SACConfig actualizado
[ ] Editar sac.py - incluir observables enriquecidos
[ ] Compilar/Linter check
[ ] Commit: "SAC TIER 2: Normalizaci√≥n adaptativa + observables...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

---

## üéì REFERENCIAS TE√ìRICAS

### Por qu√© estos cambios funcionan

1. **Normalizaci√≥n Adaptativa** (rewards.py)
   - SAC es algoritmo on-policy (sensible a escala)
   - Normalizar por percentiles evita saturation
   - Permite pesos m√°s simples (no necesita tuning manual)

2. **Baselines Din√°micas** (rewards.py)
   - Baselines fijos ignoran contexto temporal
   - Baselines = target realista por hora
   - Diferencia = (actual - target) = signal RL

3. **Observables Enriquecidos** (enriched_obs.py)
   - Red neuronal explora mejor con **state features**
   - Flags de pico = aprender scheduling
   - SOC din√°mico = aprender reserva
   - Fairness = aprender coordinaci√≥n multi-playa

4. **Hiperpar√°metros TIER 2** (sac.py)
   - SAC necesita alta entrop√≠a (exploraci√≥n)
   - LR menor = convergencia m√°s estable
   - Batch peque√±o = menos correlaci√≥n
   - Redes grandes = capacidad expresiva

---

## üìû CONTACTO PARA ISSUES

Si durante implementaci√≥n encuentras:

- **Valores NaN**: Check normalizaci√≥n dividir por cero
- **Reward siempre negativo**: Ajustar baselines
- **GPU memory**: Reducir batch_size de 256 a 128
- **Convergencia lenta**: Aumentar update_per_timestep

---

**Pr√≥ximas fases**: TIER 3 (model-based predictions), TIER 4 (multi-agent
coordination)
