# IMPLEMENTACI√ìN PASO-A-PASO: SAC TIER 2

**Duraci√≥n estimada**: 2-3 horas
**Archivos a editar**: 3 principales
**Status**: INICIANDO

---

## üìã ARCHIVOS A MODIFICAR

<!-- markdownlint-disable MD013 -->
```text
src/iquitos_citylearn/oe3/
‚îú‚îÄ‚îÄ rewards.py              ‚Üê CAMBIO 1: Normalizaci√≥n + baselines din√°micas
‚îú‚îÄ‚îÄ agents/sac.py           ‚Üê CAMBIO 2: Hiperpar√°metros + LRs
‚îî‚îÄ‚îÄ enriched_observables.py ‚Üê VERIFICAR: Observables incluidos
```text
<!-- markdownlint-enable MD013 -->

---

## CAMBIO 1: rewards.py - NORMALIZACI√ìN ADAPTATIVA

### Paso 1.1: Agregar clase para stats adaptativas

**Ubicaci√≥n**: Desp...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

---

### Paso 1.2: Modificar `__init__` de `MultiObjectiveReward`

**Ubicaci√≥n**: L√≠nea ~113 en rewards.py

**Antes**:

<!-- markdownlint-disable MD013 -->
```python
    def __init__(
        self,
        weights: Optional[MultiObjectiveWeights] = None,
        context: Optional[IquitosContext] = None,
    ):
        if weights is None:
            weights = MultiObjectiveWeights(co2=0.50, cost=0.15, solar=0.20, ev_satisfaction=0.10, grid_stability=0.05)
        self.weights = weights
        self.context = context or IquitosContext()

        self._reward_hi...
```

[Ver c√≥digo completo en GitHub]python
    def __init__(
        self,
        weights: Optional[MultiObjectiveWeights] = None,
        context: Optional[IquitosContext] = None,
        use_adaptive_stats: bool = True,
    ):
        if weights is None:
            # TIER 2 FIX: Pesos rebalanceados
            weights = MultiObjectiveWeights(
                co2=0.50,              # PRIMARY: CO‚ÇÇ minimizaci√≥n
                grid_stability=0.15,   # +5% por importancia pico
                solar=0.20,
                ev_satisfaction=0.10,
                cost=0.05              # REDUCIDO: no es bottleneck
            )
        self.weights = weights
        self.context = context or IquitosContext()

        # NEW: Estad√≠sticas adaptativas
        self._adaptive_stats = AdaptiveRewardStats() \
                if use_adaptive_stats else None

        self._reward_history: List[Dict[str, float]] = []
        self._max_history = 1000
```text
<!-- markdownlint-enable MD013 -->

---

### Paso 1.3: Reemplazar funci√≥n `compute()` COMPLETA

**Ubicaci√≥n**: L√≠nea ~143 - ~280

**Reemplazar por**:

<!-- markdownlint-disable MD013 -->
```python
    def compute(
        self,
        grid_import_kwh: float,
        grid_export_kwh: float,
        solar_generation_kwh: float,
        ev_charging_kwh: float,
        ev_soc_avg: float,
        bess...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

---

## CAMBIO 2: sac.py - HIPERPAR√ÅMETROS TIER 2

### Paso 2.1: Modificar `SACConfig`

**Ubicaci√≥n**: L√≠nea ~176 en sac.py

**Antes**:

<!-- markdownlint-disable MD013 -->
```python
@dataclass
class SACConfig:
    """Configuraci√≥n avanzada para SAC..."""
    episodes: int = 50
    batch_size: int = 512
    buffer_size: int = 100000
    learning_rate: float = 3e-4
    gamma: float = 0.99
    tau: float = 0.005

    ent_coef: float = 0.01
    target_entropy: Optional[float] = -50.0

    hidden_sizes: tuple = (256, 256)
    activation: str = "relu"

    n_steps: int = 1
    grad...
```

[Ver c√≥digo completo en GitHub]python
@dataclass
class SACConfig:
    """Configuraci√≥n SAC TIER 2: Optimizaci√≥n post-relanzamiento.

    TIER 2 FIX APPLIED:
    - Entrop√≠a: 0.02 (‚Üë exploraci√≥n inicial)
    - Target entropy: -40.0 (menos restrictivo)
    - Learning rate: 2.5e-4 (m√°s estable)
    - Batch size: 256 (menos ruido)
    - Buffer: 150k (m√°s diversidad)
    - Hidden: 512x512 (m√°s expresiva)
    - Dropout: 0.1 (regularizaci√≥n)
    """
    # Entrenamiento base
    episodes: int = 50
    batch_size: int = 256              # ‚Üì de 512: menos ruido
    buffer_size: int = 150000          # ‚Üë de 100k: m√°s diversidad
    learning_rate: float = 2.5e-4      # ‚Üì de 3e-4: m√°s estable

    # Learning rates espec√≠ficos
    critic_lr: float = 2.5e-4          # NEW: LR cr√≠tico
    actor_lr: float = 2.5e-4           # NEW: LR actor
    alpha_lr: float = 1e-4             # NEW: LR para alpha (entrop√≠a)

    gamma: float = 0.99                # Discount factor
    tau: float = 0.005                 # Target network update rate

    # Entrop√≠a - TIER 2 FIX
    ent_coef: float = 0.02             # ‚Üë de 0.01: m√°s exploraci√≥n
    target_entropy: Optional[float] = -40.0  # ‚Üì de -50.0: menos penalizante

    # Red neuronal - TIER 2 FIX
    hidden_sizes: tuple = (512, 512)   # ‚Üë de (256,256): mayor capacidad
    activation: str = "relu"
    use_dropout: bool = True           # NEW: regularizaci√≥n
    dropout_rate: float = 0.1          # NEW: 10% dropout

    # Actualizaciones m√∫ltiples
    n_steps: int = 1
    gradient_steps: int = 1
    update_per_timestep: int = 2       # NEW: 2 updates/step (vs 1)

    # GPU/CUDA
    device: str = "auto"
    use_amp: bool = True
```text
<!-- markdownlint-enable MD013 -->

---

### Paso 2.2: Verificar que observables enriquecidos se usan

**Ubicaci√≥n**: L√≠nea ~550-600 en sac.py (en m√©todo de env wrapping)

**Buscar**: c√≥digo que llama a `enriched_observables.EnrichedObservableWrapper`

**Si NO existe**, a√±adir en `setup_env()`:

<!-- markdownlint-disable MD013 -->
```python
# En setup_env() method, despu√©s de crear env:

from ..en...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

---

## CAMBIO 3: enriched_observables.py - VERIFICACI√ìN

### Paso 3.1: Revisar que todos los features se incluyen

**Ubicaci√≥n**: M√©todo `get_enriched_state()`en enriched_observables.py (~l√≠nea
100)

**Verif icar que retorna TODOS estos keys**:

<!-- markdownlint-disable MD013 -->
```python
return {
    "is_peak_hour": is_peak,                        # 1 feature
    "is_valley_hour": is_valley,                    # 1 feature
    "hour_of_day": float(self.hour_of_day),         # 1 feature
    "bess_soc_current": float(bess_soc),            # 1 feature
    "bess_soc_target": float(soc_target),           # 1 feature
    "bess_soc_reserve_deficit": float(soc_reserve_deficit),  # 1 featur...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

---

## ‚úÖ VALIDACI√ìN POST-CAMBIOS

### Test 1: Verificar sintaxis

<!-- markdownlint-disable MD013 -->
```bash
python -m py_compile src/iquitos_citylearn/oe3/rewards.py
python -m py_compile src/iquitos_citylearn/oe3/agents/sac.py
python -m py_compile src/iquitos_citylearn/oe3/enriched_observables.py
```text
<!-- markdownlint-enable MD013 -->

### Test 2: Cargar m√≥dulos

<!-- markdownlint-disable MD013 -->
```python
import sys
sys.path.insert(0, 'd:\\dise√±opvbesscar')

from src.iquitos_citylearn.oe3.rewards...
```

[Ver c√≥digo completo en GitHub]text
<!-- markdownlint-enable MD013 -->

### Test 3: Full env test

<!-- markdownlint-disable MD013 -->
```bash
# Cargar checkpoint SAC actual
# Ejecutar 1 episodio completo
# Verificar: sin NaN, observation shape correcto
```text
<!-- markdownlint-enable MD013 -->

---

## üîÑ ROLLBACK (si algo falla)

Si necesitas revertir:

<!-- markdownlint-disable MD013 -->
```bash
# Volver a versi√≥n anterior
git checkout HEAD -- src/iquitos_citylearn/oe3/rewards.py
git checkout HEAD -- src/iquitos_citylearn/oe3/agents/sac.py

# O si ya committeaste
git revert HEAD~1
```text
<!-- markdownlint-enable MD013 -->

---

## üìä PR√ìXIMOS PASOS

Una vez completes estos 3 cambios:

1. Commit:
`"SAC TIER 2: Normalizaci√≥n adaptativa + observables + hiperpar√°metros"`
2. Entrenar: `python -m src.train_sac_cuda --episodes=50 --device=cuda`
3. Monitorear: Reward converge m√°s r√°pido?
4. Analizar: CO‚ÇÇ y SOC mejoraron?

---

**Duraci√≥n total**: 2-3 horas (c√≥digo + test + debug)
**Riesgo**: BAJO (cambios mostly en rewards, no en core RL)
**Reversibilidad**: ALTA (git revert siempre disponible)