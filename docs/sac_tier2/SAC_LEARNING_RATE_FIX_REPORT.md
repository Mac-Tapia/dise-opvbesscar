# üî¥ CR√çTICO: SAC Learning Rate Cap Bug - IDENTIFICADO Y CORREGIDO

**Fecha**: 2026-01-18 19:05
**Estado**: ‚úÖ FIXED Y RELANZADO
**Impacto**: Bloqueaba completamente el aprendizaje del agente SAC

---

## Problema Detectado

El log de entrenamiento mostr√≥ que SAC **NO estaba aprendiendo**:

```text
Paso 25-500:   reward_avg = 0.5600 ‚Üí 0.5550 (EMPEORANDO)
Learning Rate: lr = 3.00e-05 (¬°100x MENOR que lo configurado!)
```text

**Configurado en YAML**: `learning_rate: 0.001` (0.001)
**Actual en ejecuci√≥n**: `learning_rate: 3.00e-05` (0.00003)
**Factor de degradaci√≥n**: **33.3x m√°s lento** ‚ùå

---

## Ra√≠z del Problema

En [src/iquitos_citylearn/oe3/agents/sac.py][ref], l√≠neas 659-667:

[ref]: src/iquitos_citylearn/oe3/agents/sac.py

```python
# ‚ùå ANTES (BUG)
stable_lr = min(self.config.learning_rate, 3e-5)  # Cap a 3e-5 (muy bajo)
stable_batch = min(self.config.batch_size, 512)   # Cap a 512 (muy bajo)
```text

**Problema**:

- Capaba el learning rate a 3e-5 sin importar la configuraci√≥n
- Capaba batch_size a 512 (config = 32,768 para GPU)
- C√≥digo antiguo de "estabilidad conservadora" que nunca se removi√≥

---

## Soluci√≥n Aplicada

‚úÖ Removida la limitaci√≥n de learning rate
‚úÖ Removida la limitaci√≥n de batch size
‚úÖ Usando valores de configuraci√≥n directamente

**Cambio en sac.py l√≠neas 659-667**:

```python
# ‚úÖ DESPU√âS (FIXED)
stable_lr = self.config.learning_rate        # Usar config completo: 0.001
stable_batch = self.config.batch_size        # Usar config completo: 32,768
```text

---

## Impacto de la Correcci√≥n

| M√©trica | Antes | Despu√©s | Mejora |
| --- | ------- | --- | -------- |
| Learning Rate | 3.00e-05 | 1.00e-03 | **33.3x m√°s r√°pido** |
| Batch Size | 512 | 32,768 | **64x m√°s grande** |
| Gradient Quality | Muy bajo | √ìptimo para GPU | **Mejor convergencia** |
| Esperado: Reward 500 pasos | 0.5550 (plano) | 0.6x+ (creciente) | **Aprendizaje real** |

---

## Entrenamiento Relanzado

**Comando ejecutado** (19:05:28):

```bash
.\\.venv\\Scripts\\python.exe -m scripts.run_oe3_simulate --config configs/default.yaml
```text

**Configuraci√≥n confirmada**:

- SAC: episodes=2, batch_size=32,768, gradient_steps=256,
  - **learning_rate=0.001**
- PPO: episodes=2, n_steps=32,768, batch_size=32,768, **learning_rate=0.001**
- A2C: episodes=2, n_steps=65,536, **learning_rate=0.001**
- Multiobjetivo: CO2=0.50, Solar=0.20, Costo=0.15, EV=0.10, Grid=0.05

**Terminal ID**: b0dc12af-7904-4f3e-9ec8-b653ea9298b3
**Inicio Baseline**: 19:05:28

---

## Monitoreo Esperado

**Pasos SAC (pr√≥ximas 30-40 min)**:

- Paso 25: reward_avg = 0.56+
- Paso 100: reward_avg = 0.60+ (inicio de aprendizaje)
- Paso 250: reward_avg = 0.65+ (crecimiento claro)
- Paso 500: reward_avg = 0.70+ (convergencia visible)

**Si reward sigue plano o baja**: Habr√° otro bug invisible. Investigar
actor_loss y critic_loss.

---

## Archivo Modificado

**[src/iquitos_citylearn/oe3/agents/sac.py][ref]**

[ref]: src/iquitos_citylearn/oe3/agents/sac.py#L659-L667

```diff
  target_entropy = self.config.target_entropy if self.config.target_entropy is not None else "auto"

- # Learning rate M√ÅS conservador para estabilidad
- stable_lr = min(self.config.learning_rate, 3e-5)  # Max 3e-5 (muy bajo)
-
- # Gamma est√°ndar (SAC maneja bien gamma alto con entropy)
- stable_gamma = self.config.gamma  # Usar config original (0.99)
-
- # Batch size moderado
- stable_batch = min(self.config.batch_size, 512)
+ # Use configured learning rate (not capped anymore)
+ stable_lr = self.config.learning_rate
+
+ # Gamma est√°ndar (SAC maneja bien gamma alto con entropy)
+ stable_gamma = self.config.gamma  # Usar config original (0.99)
+
+ # Use configured batch size (not capped anymore - GPU can handle 32k)
+ stable_batch = self.config.batch_size
```text

---

## Commit

```text
Fix: Remove SAC learning rate cap (3e-5 ‚Üí use config 0.001) and batch_size cap (512 ‚Üí use config 32768)
```text

---

## An√°lisis Post-Mortem

**Por qu√© no se detect√≥ antes**:

1. Los logs mostraban `lr=3.00e-05` pero no indicaban qui√©n la capaba
2. La variable `stable_lr` hac√≠a que pareciera una decisi√≥n deliberada
3. El actor_loss y critic_loss bajaban (falsamente indicaban "progreso")

**Lecciones**:

- Siempre loguear qu√© valor se estaba usando vs qu√© se configur√≥
- Revisar `min()` y `max()` caps en c√≥digo cr√≠tico de RL
- La estabilidad no viene de learning rates ultra-bajos, sino de good reward
  - design + entropy

---

**Estado**: üü¢ **ENTRENAMIENTO RELANZADO CON FIX APLICADO - ESPERAR SAC PHASE
~30 MIN**