[MEJORAS SAC - 18 Enero 2026]

CAMBIOS IMPLEMENTADOS PARA QUE SAC APRENDA:
============================================================

1. PESOS RECOMPENSA (rewards.py)
   ✓ CO2: 0.50 → 0.45 (PRIMARIO, subida relativa)
   ✓ Grid Stability: 0.05 → 0.20 (SUBIDA 4x - CRÍTICO para picos)
   ✓ Solar: 0.20 → 0.15 (REDUCCIÓN - menos prioridad que picos)
   ✓ EV Satisfaction: 0.10 → 0.05 (MITAD - menos prioritario)
   ✓ Cost: 0.15 → 0.15 (IGUAL)

   Efecto: Agente aprende PRIORITARIAMENTE a:
   - Minimizar importación en horas pico (18-21h) [peso x4]
   - Mantener estabilidad de red [peso x4]
   - LUEGO maximizar autoconsumo solar
   - LUEGO satisfacción EV

2. PENALIZACIONES HORA PICO (rewards.py)
   ✓ En pico (18-21h):
     - CO2: Penalizacion 3.0x (vs 1.5x fuera pico)
     - Grid: Penalizacion 4.0x (vs 2.0x fuera pico)
     - SOC < 0.5 pre-pico (16-17h): Penalizacion -0.5

   Efecto: Agente DEBE:
   - Acumular energía antes de pico
   - Minimizar importación durante pico
   - Usar BESS para suplir demanda en pico

3. HIPERPARÁMETROS SAC (default.yaml)
   ✓ batch_size: 65536 → 16384 (4x más pequeño)
   ✓ learning_rate: 3e-4 → 1e-3 (3x más rápido)
   ✓ gradient_steps: 64 → 128 (2x más actualizaciones)
   ✓ resume_checkpoints: true → false (EMPEZAR FRESCO)

   Efecto: Convergencia más rápida y estable

4. NORMALIZACIÓN REWARDS
   ✓ Reward final: clip a [-1.0, 1.0]
   ✓ Componentes individuales clipeados
   ✓ Penalizaciones aditivas (SOC reserve)

   Efecto: Señal de aprendizaje clara y acotada

============================================================

CONFIGURACION NUEVA - SAC MEJORADO:

CO2:   45% (minimizar importacion)
Grid:  20% (minimizar picos - SUBIDA 4x)
Solar: 15% (maximizar autoconsumo)
Cost:  15% (minimizar costo)
EV:     5% (satisfaccion carga)
--------
Total: 100%

Pre-pico (16-17h): SOC > 0.5 REQUERIDO
Pico (18-21h):     Importacion MINIMA
Post-pico:         Recuperar SOC

============================================================

COMANDO PARA RELANZAR:

1. Limpiar checkpoints viejos:
   Remove-Item analyses/oe3/training/checkpoints -Recurse -Force
   Remove-Item outputs/oe3/simulations/sac_results.json -Force

2. Ejecutar con config mejorada:
   python -m scripts.run_oe3_simulate --config configs/default.yaml --skip-uncontrolled

3. Monitorear logs (buscar):
   - r_grid mejorando (< 0.5 en pico = bueno)
   - r_co2 mejorando (> 0.5 = bueno)
   - reward_total mejorando

============================================================

SEÑALES DE APRENDIZAJE ESPERADAS:

SIN APRENDIZAJE:
  - reward_avg = 0.6000 (constante)
  - r_grid siempre -1.0
  - r_co2 siempre < 0

CON APRENDIZAJE:
  - reward_avg sube de 0.6 → 0.7+
  - r_grid mejora (-1.0 → -0.2 → 0.0)
  - r_co2 mejora (negativo → 0.0 → positivo)
  - Importacion en pico baja

============================================================

PROXIMAS MEJORAS (si sigue sin aprender):

1. Agregar observables:
   - Hora del día (one-hot encoding 24)
   - SOC actual (escalar 0-1)
   - SOC reserva requerida
   - Sesiones pendientes por playa
   - Potencia PV disponible

2. Implementar constraints:
   - Throttling de potencia por playa
   - Reserva SOC dinámica
   - Límites de potencia agregada

3. Ajustar reward shaping:
   - Usar RMS normalization
   - Aplicar reward clipping adaptativo
   - Usar running statistics

============================================================
