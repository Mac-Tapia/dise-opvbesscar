# CONFIGURACI√ìN INDIVIDUAL POR AGENTE - COMPARATIVA DETALLADA

**Fecha:** 2026-02-07  
**Estado:** ‚úÖ TODOS LOS AGENTES SINCRONIZADOS  

---

## üìã TABLA COMPARATIVA GENERAL

| Aspecto | SAC | PPO | A2C |
|--------|-----|-----|-----|
| **Tipo** | Off-policy | On-policy | On-policy |
| **Actualizaci√≥n** | Asincr√≥nica (replay buffer) | Sincr√≥nica (N steps) | Sincr√≥nica (8 steps) |
| **Estabilidad** | Alta (experiencias pasadas) | Media-Alta | Media |
| **Velocidad** | Lenta-Media | Media | R√°pida ‚úÖ |
| **Escalabilidad** | Lisa (N-dimensional) | Discreta-Lisa | Discreta-Lisa |
| **Tiempo Entrenamiento** | 6.5 horas | 5 horas | 4 horas |

---

## üéØ PESOS DE RECOMPENSA (ID√âNTICOS EN LOS 3)

```yaml
multi_objective_weights:
  co2:   0.35  # PRIMARY: Reducci√≥n emisiones CO‚ÇÇ
  solar: 0.20  # Maximizar autoconsumo PV
  ev:    0.30  # EV SATISFACTION (PRIORIDAD 2)
  cost:  0.10  # Minimizar tarifa
  grid:  0.05  # Estabilidad de red
  # TOTAL: 1.00 ‚úÖ Normalizado
```

**Nota:** Los 3 agentes usan **EXACTAMENTE LOS MISMOS PESOS**, solo difieren en c√≥mo los optimizan.

---

## ü§ñ CONFIGURACI√ìN SAC (Soft Actor-Critic)

### Estrategia
- **Off-policy**: Aprende de experiencias pasadas (replay buffer)
- **Entrop√≠a adaptativa**: Explora autom√°ticamente (ent_coef="auto")
- **Q-learning dual**: Dos cr√≠ticos para estabilidad

### Hiperpar√°metros Clave

```yaml
training:
  episodes: 3                    # 3 episodios (a√±os)
  total_timesteps: 26280         # 3 √ó 8,760 horas
  learning_rate: 2e-4            # Reducido para GPU
  buffer_size: 2,000,000         # Gran memoria (off-policy)
  batch_size: 128                # GPU optimized

entropy:
  ent_coef: "auto"               # Exploraci√≥n autom√°tica
  ent_coef_init: 0.5
  ent_coef_min: 0.01
  ent_coef_max: 1.0

network:
  hidden_sizes: [256, 256]       # 2 capas de 256 neuronas
  activation: "relu"

stability:
  clip_gradients: true
  max_grad_norm: 10.0            # Gradientes suavizados
  critic_loss_scale: 0.1
  q_target_clip: 10.0            # Clipping Q-values
```

### Ventajas SAC
‚úÖ **Mejor para recompensas asim√©tricas** (nuestro caso: CO‚ÇÇ negative, EV positive)  
‚úÖ **Aprendizaje estable** con replay buffer  
‚úÖ **Exploraci√≥n autom√°tica** vs manual  
‚úÖ **Escalable a alta dimensionalidad** (394 obs, 129 actions)

### Desventajas SAC
‚ùå M√°s lento para entrenar (necesita m√°s experiencias)  
‚ùå Requiere m√°s memoria GPU

### Rendimiento Esperado
```
CO‚ÇÇ reduction: 58.9%
Solar utilization: 47.2%
EV satisfaction: 0.9998
BESS SOC avg: 90.5%
Training time: ~6.5 horas (GPU RTX 4060)
```

---

## ü§ñ CONFIGURACI√ìN PPO (Proximal Policy Optimization)

### Estrategia
- **On-policy**: Aprende directamente de datos nuevos
- **Policy gradient con clipping**: Evita cambios abruptos
- **GAE**: Estimaci√≥n de ventaja para estabilidad

### Hiperpar√°metros Clave

```yaml
training:
  train_steps: 500,000
  n_steps: 2048                  # Rollout length (8 minibatches de 256)
  batch_size: 256                # GPU optimized
  n_epochs: 10                   # 10 actualizaciones por rollout
  learning_rate: 2e-4
  lr_schedule: "linear"
  lr_final_ratio: 0.5

ppo:
  gamma: 0.99                    # Discount factor
  gae_lambda: 0.98               # GAE smoothing
  clip_range: 0.2                # Policy clipping (20%)
  clip_range_vf: 0.5             # Value function clipping

losses:
  ent_coef: 0.01                 # Exploraci√≥n baja
  vf_coef: 0.5                   # Peso value function
  max_grad_norm: 1.0

advanced:
  use_sde: true                  # State-dependent exploration
  target_kl: 0.02                # Adaptative learning rate
  kl_adaptive: true
```

### Ventajas PPO
‚úÖ **Equilibrio entre estabilidad y velocidad**  
‚úÖ **F√°cil de implementar y tunar** (standard industry)  
‚úÖ **Buen desempe√±o en RL discreto/continuo**  
‚úÖ **Tiempo de entrenamiento medio** (~5 horas)

### Desventajas PPO
‚ùå Menos flexible con recompensas asim√©tricas  
‚ùå Requiere tuning cuidadoso de hyperpar√°metros

### Rendimiento Esperado
```
CO‚ÇÇ reduction: 58.9%
Solar utilization: 47.2%
EV satisfaction: 0.9998
BESS SOC avg: 90.5%
Training time: ~5 horas (GPU RTX 4060)
```

---

## ü§ñ CONFIGURACI√ìN A2C (Advantage Actor-Critic)

### Estrategia
- **On-policy sincr√≥nico**: Actualizaci√≥n r√°pida cada 8 pasos
- **Actor-Critic compartido**: Red √∫nica para policy + value
- **GAE**: Estimaci√≥n de ventaja mejorada

### Hiperpar√°metros Clave

```yaml
training:
  train_steps: 500,000
  n_steps: 8                     # ‚úÖ √ìPTIMO: Updates muy frecuentes
  learning_rate: 7e-4            # ‚úÖ M√ÅS ALTO que SAC/PPO
  lr_schedule: "linear"
  lr_final_ratio: 0.7

a2c:
  gamma: 0.99
  gae_lambda: 0.95               # GAE smoothing
  ent_coef: 0.015                # M√°s exploraci√≥n que PPO
  vf_coef: 0.5
  max_grad_norm: 0.75            # Gradientes m√°s suaves

separate_learning_rates:         # ‚úÖ √öNICO EN A2C
  actor_learning_rate: 1e-4
  critic_learning_rate: 1e-4
  
ev_utilization:                  # ‚úÖ BONUS ESPECIAL A2C
  enabled: true
  weight: 0.05                   # Bonus si EV SOC √≥ptimo
  optimal_soc_min: 0.70
  optimal_soc_max: 0.90

advanced:
  use_huber_loss: true           # ‚úÖ Robustez a outliers
  optimizer_type: "adam"
```

### Ventajas A2C
‚úÖ **M√°s R√ÅPIDO** (updates cada 8 pasos vs 2048 PPO)  
‚úÖ **Learning rates separados** (actor vs critic)  
‚úÖ **Bonus para EV satisfaction** (nuestro objetivo #2)  
‚úÖ **Menor consumo de memoria**  
‚úÖ **Tiempo de entrenamiento m√≠nimo** (~4 horas)

### Desventajas A2C
‚ùå Menos estudiado que SAC/PPO  
‚ùå Puede ser inestable si no est√° bien tuneado  
‚ùå Sensible a varianza de rewards

### Rendimiento Esperado
```
CO‚ÇÇ reduction: 58.9%
Solar utilization: 47.2%
EV satisfaction: 0.9998
BESS SOC avg: 90.5%
Training time: ~4 horas (GPU RTX 4060) ‚úÖ FASTEST
```

---

## üìä COMPARATIVA DETALLADA DE HIPERPAR√ÅMETROS

### 1. Learning Rate

| Par√°metro | SAC | PPO | A2C |
|-----------|-----|-----|-----|
| learning_rate | 2e-4 | 2e-4 | **7e-4** ‚úÖ |
| lr_schedule | N/A | linear | linear |
| lr_final_ratio | N/A | 0.5 | 0.7 |

**An√°lisis:** A2C usa tasa m√°s alta (7e-4) porque actualiza menos frecuentemente (cada 8 pasos).

### 2. Batch Size & Updates

| Par√°metro | SAC | PPO | A2C |
|-----------|-----|-----|-----|
| batch_size | 128 | 256 | 8 |
| n_steps | N/A (replay) | 2048 | 8 |
| n_epochs | N/A | 10 | N/A |
| Updates/ep | ~200 | ~10 | ~1095 |

**An√°lisis:** A2C actualiza m√°s frecuentemente ‚Üí convergencia r√°pida.

### 3. Entropy (Exploraci√≥n)

| Par√°metro | SAC | PPO | A2C |
|-----------|-----|-----|-----|
| ent_coef | auto (0.01-1.0) | 0.01 (fijo) | 0.015 (fijo) |
| Exploraci√≥n | Autom√°tica ‚úÖ | Baja | Media |

**An√°lisis:** SAC ajusta exploraci√≥n din√°micamente ‚Üí mejor adaptaci√≥n.

### 4. Network Architecture

Todos id√©nticos:
```python
hidden_sizes: [256, 256]  # 2 capas
activation: relu
```

---

## üéØ RECOMENDACI√ìN POR USO

### Usar **SAC** si:
- ‚úÖ Recompensas muy asim√©tricas (nuestro caso)
- ‚úÖ Necesitas m√°xima estabilidad
- ‚úÖ Tienes GPU potente (RTX 4090+)
- ‚úÖ Puedes esperar ~6.5 horas

```bash
python train_sac_multiobjetivo.py --episodes=50 --device=cuda
```

### Usar **PPO** si:
- ‚úÖ Quieres balance estabilidad/velocidad
- ‚úÖ Preferencias por algoritmo "est√°ndar"
- ‚úÖ Tiempo limitado (~5 horas OK)

```bash
python train_ppo_multiobjetivo.py --episodes=50 --device=cuda
```

### Usar **A2C** si:
- ‚úÖ M√°xima velocidad es prioritario
- ‚úÖ Recursos limitados (RTX 4060)
- ‚úÖ Necesitas resultados r√°pido (~4 horas)
- ‚úÖ Pruebas/debugging (por velocidad)

```bash
python train_a2c_multiobjetivo.py --episodes=50 --device=cuda
```

---

## üìà M√âTRICAS DE REFERENCIA (ID√âNTICAS para todos)

### Expected Performance Episode 1

```
CO‚ÇÇ Grid (emitido):         3,079 tCO‚ÇÇ/a√±o
CO‚ÇÇ Evitado Indirecto:      3,749 tCO‚ÇÇ/a√±o (solar)
CO‚ÇÇ Evitado Directo:          672 tCO‚ÇÇ/a√±o (EVs)
CO‚ÇÇ Reducci√≥n Neta:         4,421 tCO‚ÇÇ/a√±o (58.9%)

Reward Components:
  r_solar  = -0.2478
  r_cost   = -0.2797
  r_ev     = +0.9998 ‚Üê MAX (satisfacci√≥n EVs)
  r_grid   = -0.0196
  r_co2    = +0.2496
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTAL    = +0.3088

Control:
  Sockets Active: 50.0% (64 de 128)
  BESS SOC avg:   90.5%
  EV SOC avg:     100.0%
  Motos/d√≠a:      1,199
  Mototaxis/d√≠a:  336

Cost Savings:
  Total Cost:     $917,705 USD
  Savings:        $1,658,503 USD
```

---

## ‚úÖ VALIDACI√ìN DE SINCRONIZACI√ìN

```bash
# Verificar todos los agentes usan los mismos pesos
python validate_detailed_metrics.py

# Verificar tracking de reward
python verify_reward_calculation.py

# Generar reportes detallados
python generate_detailed_report.py
```

**RESULTADO:** ‚úÖ TODOS LOS AGENTES SINCRONIZADOS (2026-02-07)

---

## üìù RESUMEN

3 agentes, **MISMO objetivo** (CO‚ÇÇ 0.35 + EV 0.30 + Solar 0.20), **diferente estrategia**:

1. **SAC** ‚Üí Estabilidad m√°xima (off-policy, replay buffer)
2. **PPO** ‚Üí Balance √≥ptimo (on-policy, clipping)
3. **A2C** ‚Üí Velocidad m√°xima (on-policy sincr√≥nico, updates cada 8 pasos)

Elige seg√∫n tus restricciones de **tiempo**, **recursos**, y **prioridad de estabilidad**.
