# üîç AUDIT: Recompensas, Observables e Hiperpar√°metros SAC

**Fecha**: 2026-01-18 19:10
**Status**: üü° CR√çTICO - Identifi¬≠cados 8 problemas de escalado y se√±al

---

## 1. PROBLEMAS EN LA RECOMPENSA

### 1.1 Pesos Mal Configurados

**Archivo**: [src/iquitos_citylearn/oe3/rewards.py](src/iquitos_citylearn/oe3/rewards.py#L30-L45)

```python
# ‚ùå ACTUAL (MALO)
co2: float = 0.45              # Comentario dice 0.50 subido, pero en c√≥digo es 0.45
cost: float = 0.15
solar: float = 0.15            # Comentario dice bajado, ambiguo
ev_satisfaction: float = 0.05  # Bajado OK
grid_stability: float = 0.20   # Subido de 0.05 (CONTRAPRODUCENTE)
```text

**Problema**: grid_stability=0.20 es excesivo. Con peak_hours=(18,19,20,21) y 4 horas de 24, solo 16% del d√≠a est√° en pico. Darle 20% de peso a penalidad general es **excesivo** cuando se necesita **CO‚ÇÇ/importaci√≥n en pico**.

**Soluci√≥n**:

```python
# ‚úÖ PROPUESTO
co2: float = 0.50              # Claro: minimizar CO‚ÇÇ es PRIMARY
cost: float = 0.10             # Reducido (costo es secundario)
solar: float = 0.20            # Maximizar solar es CR√çTICO para CO‚ÇÇ
ev_satisfaction: float = 0.10  # Satisfacci√≥n b√°sica de EVs
grid_stability: float = 0.10   # REDUCIDO: picos se controlan por CO‚ÇÇ
```text

**Por qu√©**: grid_stability ya est√° impl√≠cito en CO‚ÇÇ (importaci√≥n en pico = CO‚ÇÇ alto). Duplicar peso causa conflicto.

---

### 1.2 Componentes de Recompensa Mal Escalados

### L√≠neas 150-195 en rewards.py

```python
# ‚ùå PROBLEMA 1: CO‚ÇÇ no distingue pico vs off-peak bastante
if is_peak:
    r_co2 = 1.0 - 3.0 * min(1.0, grid_import_kwh / 500.0)  # 500.0 baseline arbitrario
else:
    r_co2 = 1.0 - 1.5 * min(1.0, grid_import_kwh / 500.0)

# PROBLEMA: ¬øPor qu√© 500.0? Eso es ~5x la demanda t√≠pica de mall (100 kW avg)
# En pico real (18-21h) con 128 chargers @ max = 272 kW + mall load...
# 500.0 es MUY ALTO ‚Üí reward casi siempre positivo ‚Üí sin gradiente de learning
```text

**Baseline deber√≠a ser**:

```python
# ‚úÖ PROPUESTO
# T√≠pico: mall ~100 kWh + chargers ~30 kWh (off-peak) = 130 kWh
# En pico: mall ~150 + chargers ~100 (con BESS support) = 250 kWh target
# Si agente importa >250 en pico, debe sufrir penalizaci√≥n

co2_baseline_offpeak = 130.0   # kWh t√≠pico off-peak
co2_baseline_peak = 250.0      # kWh objetivo en pico (con BESS y FV)

if is_peak:
    # Si importas > 250 kWh en pico (bad), reward = 1 - 2*(250/250) = -1
    # Si importas < 100 kWh en pico (great), reward = 1 - 2*(100/250) = 0.2
    r_co2 = 1.0 - 2.0 * min(1.0, grid_import_kwh / co2_baseline_peak)
else:
    # Off-peak m√°s tolerante pero a√∫n penaliza exceso
    r_co2 = 1.0 - 1.0 * min(1.0, grid_import_kwh / co2_baseline_offpeak)
```text

**Impacto**: Da **RANGO de variaci√≥n** en reward (-1 a +1 vs -0.5 a +0.8 antes) ‚Üí gradientes mejores.

---

### 1.3 Falta Penalizaci√≥n Expl√≠cita de Potencia Pico

### L√≠nea 200 en rewards.py

```python
# ‚ùå ACTUAL
demand_ratio = grid_import_kwh / max(1.0, self.context.peak_demand_limit_kw)

if is_peak:
    r_grid = 1.0 - 4.0 * min(1.0, demand_ratio)  # Penaliza por ENERG√çA, no POTENCIA
```text

**Problema**:

- Penaliza `grid_import_kwh` (energ√≠a acumulada en 1 hora)
- NO penaliza **potencia instant√°nea** (kW)
- SAC nunca ve el "ramp rate" (c√≥mo r√°pido sube la demanda)

**Soluci√≥n**: Agregar observable con potencia instant√°nea (ver abajo en Observables).

---

### 1.4 Mal Escalado de Penalizaci√≥n SOC Reserva

### L√≠nea 215 en rewards.py

```python
# ‚ùå ACTUAL
pre_peak_hours = [16, 17]
if hour in pre_peak_hours and bess_soc < 0.5:
    soc_penalty = -0.5 * (1.0 - bess_soc / 0.5)  # M√°ximo -0.5

# PROBLEMA: Si SOC=0.0, penalty = -0.5 * 1.0 = -0.5
# Pero reward total es suma ponderada. Si soc_penalty suma directamente (NO ponderado),
# puede ser -0.5 cuando reward deber√≠a ser [-1, +1]
```text

**Mejor**:

```python
# ‚úÖ PROPUESTO
if hour in pre_peak_hours and bess_soc < 0.65:  # Target 65% (no 50%)
    # Penalizar faltante
    soc_deficit = 0.65 - bess_soc
    r_soc_reserve = 1.0 - (soc_deficit / 0.65)  # Normalizado a [0, 1]
    components["r_soc_reserve"] = r_soc_reserve
else:
    components["r_soc_reserve"] = 1.0  # Bonus si cumples
```text

---

### 1.5 Suma de Componentes Sin Normalizaci√≥n Correcta

### L√≠nea 220 en rewards.py

```python
# ‚ùå ACTUAL
reward = (
    self.weights.co2 * r_co2 +
    self.weights.cost * r_cost +
    self.weights.solar * r_solar +
    self.weights.ev_satisfaction * r_ev +
    self.weights.grid_stability * r_grid +
    components["soc_reserve_penalty"]  # ‚Üê NOT PONDERADO
)

# PROBLEMA: soc_reserve_penalty se suma directamente sin peso
# Si weight_co2=0.50, entonces co2 contribuye 0.50 * r_co2
# Pero soc_penalty contribuye sin factor ‚Üí DESBALANCEADO

# Ejemplo:
# r_co2=-1 ‚Üí contribuye -0.50
# r_soc=-0.5 ‚Üí contribuye -0.5
# ¬°TRES VECES M√ÅS PESO para SOC que para CO‚ÇÇ!
```text

**Soluci√≥n**:

```python
# ‚úÖ PROPUESTO
reward = (
    self.weights.co2 * r_co2 +
    self.weights.cost * r_cost +
    self.weights.solar * r_solar +
    self.weights.ev_satisfaction * r_ev +
    self.weights.grid_stability * r_grid +
    0.10 * components["r_soc_reserve"]  # Ahora ponderado
)

# Luego normalizar
reward = np.clip(reward, -1.0, 1.0)
```text

---

## 2. PROBLEMAS EN OBSERVABLES

### 2.1 Falta de Flags de Hora Pico y Contexto

**Archivo**: [src/iquitos_citylearn/oe3/simulate.py](src/iquitos_citylearn/oe3/simulate.py) (wrapper)

```python
# ‚ùå ACTUAL: El observable NO incluye:
# - Flag de es_pico (bool): ¬øestamos en 18-21h?
# - SOC_bess actual
# - SOC_bess_target_esperado
# - Potencia FV disponible (kW)
# - Colas de chargers por playa

obs = env.reset()  # obs es solo estado de CityLearn
# obs contiene: building loads, solar gen, BESS SOC, ev_state, etc.
# PERO NO flags √∫tiles para tomar decisi√≥n
```text

**Soluci√≥n**: Agregar observables contextuales:

```python
# ‚úÖ PROPUESTO
additional_obs = np.array([
    float(hour in [18, 19, 20, 21]),  # is_peak (0 o 1)
    bess_soc,                          # SOC BESS actual [0-1]
    0.65 if hour in [16,17] else 0.40,  # SOC_target basado en hora
    solar_gen_available / 1000.0,      # PV disponible (normalizado kW)
    queue_motos / 128.0,               # Fracci√≥n colas motos [0-1]
    queue_mototaxis / 16.0,            # Fracci√≥n colas mototaxis [0-1]
], dtype=np.float32)

obs_extended = np.concatenate([obs_original, additional_obs])
```text

**Beneficio**: SAC ve expl√≠citamente **cu√°ndo es pico**, **cu√°nta energ√≠a solar**, **cu√°ntas colas hay**.

---

### 2.2 Falta Potencia Instant√°nea vs Energ√≠a

**Problema**: CityLearn reporta energ√≠a (kWh) en cada timestep.
SAC NO ve la **tasa de cambio** (potencia = dE/dt en kW).

```python
# ‚ùå ACTUAL
# observation[t] contiene energ√≠a acumulada o rates, pero NO potencia pico

# ‚úÖ PROPUESTO
# Agregar derivada (diferencia)
if t > 0:
    power_grid_kw = (grid_import[t] - grid_import[t-1]) / (timestep_hours)
else:
    power_grid_kw = 0

additional_obs.append(np.clip(power_grid_kw / 200.0, -1, 1))  # Normalizado
```text

---

## 3. PROBLEMAS EN HIPERPAR√ÅMETROS SAC

### 3.1 Entrop√≠a Autom√°tica Muy Alta

**Archivo**: [src/iquitos_citylearn/oe3/agents/sac.py](src/iquitos_citylearn/oe3/agents/sac.py#L130-L145)

```python
# ‚ùå ACTUAL
ent_coef: str = "auto"          # Auto-ajusta entrop√≠a
target_entropy: Optional[float] = -126.0  # -dim(action) = -126

# PROBLEMA:
# - target_entropy = -126 es BAJO (mucha exploraci√≥n)
# - "auto" ajusta ent_coef para mantenerlo, pero esto significa:
#   "Mant√©n exploraci√≥n tan alta que entropy sea siempre -126"
# - Con reward signal d√©bil (como vimos), SAC prefiere EXPLORAR
#   en lugar de EXPLOTAR buenas acciones
```text

**Soluci√≥n**:

```python
# ‚úÖ PROPUESTO
ent_coef: float = 0.01          # Fijo, NO auto (reduce exploraci√≥n innecesaria)
target_entropy: Optional[float] = -50.0  # Menos exploraci√≥n (-dim/2 o menos)

# Con esto, SAC dedica menos capacidad a "hacer ruido"
# y m√°s a "aprender patrones"
```text

**Por qu√©**: Early training necesita exploraci√≥n, pero con reward mal escalada, SAC explora demasiado.

---

### 3.2 Learning Rate Ahora Corregido Pero Gradient Steps Bajo

### L√≠nea 140 en sac.py

```python
# ‚úÖ YA FIXED
learning_rate: float = 0.001    # Antes era capped a 3e-5

# PERO gradient_steps es:
gradient_steps: int = 256       # (en config YAML)

# PROBLEMA: gradient_steps=256 es ALTO
# Significa: por cada timestep en env, SAC hace 256 updates
# Con batch_size=32,768 y buffer_size=8,000,000:
#   - Necesita ~256 timesteps para llenar primer batch
#   - Luego 256 updates √ó 32,768 examples = 8M ejemplos procesados
#   - ESTO ES MUCH√çSIMO para episodios de 8,760 pasos
```text

**Soluci√≥n**:

```python
# ‚úÖ PROPUESTO
gradient_steps: int = 64        # Reducido de 256 a 64
train_freq: int = 4             # Update cada 4 timesteps (unchanged)

# L√ìGICA:
# - train_freq=4: cada 4 pasos en env, hace 1 update
# - gradient_steps=64: cada update procesa 64 minibatches
# - Total: 4 * 64 = 256 ejemplos procesados por step env
#   (vs 256*32768 antes = overkill)
```text

---

### 3.3 Normalizaci√≥n de Recompensa Falta

### L√≠nea 230 en rewards.py

```python
# ‚ùå ACTUAL
reward = np.clip(reward, -1.0, 1.0)  # Clipped pero NO normalizado

# MEJOR:
reward_mean = np.mean([h["reward_total"] for h in self._reward_history[-100:]])
reward_std = np.std([h["reward_total"] for h in self._reward_history[-100:]])

if reward_std > 0.01:
    reward_normalized = (reward - reward_mean) / (reward_std + 1e-8)
    reward_normalized = np.clip(reward_normalized, -2, 2)
else:
    reward_normalized = reward

components["reward_normalized"] = reward_normalized
return reward_normalized, components  # Retornar normalizado
```text

**Beneficio**: SAC ve recompensas con media ~0 y varianza ~1 ‚Üí mejor convergencia.

---

### 3.4 Batch Size Extremadamente Alto

### L√≠nea 138 en sac.py (YAML)

```yaml
# ‚úÖ ACTUAL (despu√©s del fix)
batch_size: int = 32768         # 32K ejemplos por update

# PROBLEMA con esto:
# - RTX 4060: 8.6 GB VRAM
# - 32K √ó 900 dims (obs) √ó 4 bytes = 129 MB solo observaciones
# - + 32K √ó 126 dims (actions) √ó 4 bytes = 16 MB
# - + networks (2 Q-networks + policy) ~200 MB
# - + optimizers/buffers ~500 MB
# - TOTAL: >>1 GB, okay para RTX 4060
# PERO:
# - Minibatch tan grande pierde flexibility
# - Mejor: m√∫ltiples updates peque√±os que 1 update gigante
```text

**Alternativa**:

```python
# ‚úÖ PROPUESTO
batch_size: int = 4096          # Reducido de 32,768 a 4K
gradient_steps: int = 256       # Aumentado (ahora tiene sentido)
train_freq: int = 4             # Unchanged

# L√ìGICA:
# 4,096 √ó 256 = 1M ejemplos por ciclo (vs 32,768 √ó 256 = 8M)
# Menos "shock" de grandebatch, m√°s exploraci√≥n de direcciones
```text

---

## 4. RECOMENDACIONES PRIORITARIAS

### TIER 1 (CR√çTICO - Implement AHORA)

1. ‚úÖ **Pesos Multiobjetivo** ‚Üí [30-45]

   ```python
   co2=0.50, cost=0.10, solar=0.20, ev_satisfaction=0.10, grid_stability=0.10
   ```text

2. ‚úÖ **Baselines CO‚ÇÇ** ‚Üí L√≠nea 150

   ```python
   co2_baseline_offpeak=130, co2_baseline_peak=250  (vs 500 ahora)
   ```text

3. ‚úÖ **Normalizaci√≥n de SOC Penalty** ‚Üí L√≠nea 215-230

   ```python
   r_soc_reserve normalizado a [0,1], luego multiplicado por peso
   ```text

4. ‚úÖ **Agregar Observables de Contexto** ‚Üí simulate.py

   ```python
   is_peak, bess_soc, bess_soc_target, pv_available, queue_motos, queue_mototaxis
   ```text

### TIER 2 (IMPORTANTE - Implement despu√©s de validar TIER 1)

1. ‚úÖ **Entrop√≠a SAC** ‚Üí sac.py l√≠nea 135

   ```python
   ent_coef=0.01 (fixed, no auto), target_entropy=-50 (vs -126)
   ```text

2. ‚úÖ **Gradient Steps & Batch** ‚Üí sac.py l√≠nea 138-140

   ```python
   batch_size=4096, gradient_steps=256 (vs 32768, 256)
   ```text

3. ‚úÖ **Normalizaci√≥n de Reward** ‚Üí rewards.py l√≠nea 230

   ```python
   reward_normalized usando rolling mean/std
   ```text

---

## 5. TESTING STRATEGY

**Despu√©s de TIER 1 changes**:

```text
Paso 100:  reward debe subir a 0.60+  (vs plano 0.56 antes)
Paso 500:  reward debe subir a 0.65+  (clara tendencia)
Paso 1000: reward debe subir a 0.70+  (convergencia visible)
```text

**M√©tricas a monitorear**:

1. `r_co2` mean (debe subir de -0.1 a 0.2+)
2. `r_grid` std (debe BAJAR, menos varianza)
3. `bess_soc` en pre-peak (debe estar 0.60+)
4. `grid_import` en pico (debe bajar vs baseline)

---

**Estado**: üî¥ **8 PROBLEMAS IDENTIFICADOS - ESPERANDO APROBACI√ìN PARA IMPLEMENTAR TIER 1**