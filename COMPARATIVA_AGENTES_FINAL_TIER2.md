# COMPARATIVA AGENTES FINAL - TIER 2 ACTUALIZADO

**Fecha**: 2026-01-18  
**Estado**: TIER 2 APPLIED A TODOS LOS AGENTES  
**ComparaciÃ³n**: A2C vs PPO vs SAC (post-TIER 2)  

---

## ğŸ“Š TABLA COMPARATIVA - HIPERPARÃMETROS TIER 2

| ParÃ¡metro | A2C TIER 2 | PPO TIER 2 | SAC TIER 2 |
|-----------|-----------|-----------|-----------|
| **Learning Rate** | 2.5e-4 | 2.5e-4 | 2.5e-4 |
| **Batch Size** | 1024 (n_steps) | 256 | 256 |
| **EntropÃ­a** | 0.02 | 0.02 | 0.02 |
| **Hidden Sizes** | (512, 512) | (512, 512) | (512, 512) |
| **Activation** | ReLU | ReLU | ReLU |
| **LR Schedule** | Linear (decay) | Linear (decay) | Constant |
| **Red Update** | Every step | Per epoch | 2x per step |
| **ExploraciÃ³n** | Entropy | SDE + Entropy | Alpha (automÃ¡tico) |
| **Gamma** | 0.99 | 0.99 | 0.99 |

---

## ğŸ¯ CARACTERÃSTICAS POR AGENTE

### **A2C (Advantage Actor-Critic)** âœ…

**Ventajas**:

- Convergencia rÃ¡pida (on-policy)
- Simple y robusto
- Bajo memory overhead
- Bueno para problemas densos

**Desventajas**:

- Menos estable que PPO/SAC
- Sample inefficiency (on-policy)

**Casos de uso**:

- Quick prototyping
- Problemas con reward denso
- Entornos con buena exploraciÃ³n natural

**TIER 2 Impact**:

- LR 2.5e-4 â†’ convergencia mÃ¡s estable
- Hidden 512x512 â†’ mejor capacity
- n_steps 1024 â†’ menos variance
- ent_coef 0.02 â†’ mayor exploraciÃ³n

**MÃ©tricas esperadas**:

- Convergencia: 30-50 episodios
- Estabilidad: Media-Alta
- COâ‚‚ anual: ~1.75M kg

---

### **PPO (Proximal Policy Optimization)** â­â­

**Ventajas**:

- Muy robusto
- Clipping + GAE = estabilidad
- Buen balance exploraciÃ³n/explotaciÃ³n
- Excelente para RL continuo

**Desventajas**:

- Convergencia mÃ¡s lenta que A2C
- MÃ¡s hyperparams que SAC

**Casos de uso**:

- ProducciÃ³n (robustez)
- Problemas continuos complejos
- Cuando estabilidad > velocidad

**TIER 2 Impact**:

- LR 2.5e-4 â†’ convergencia suave
- Hidden 512x512 â†’ mayor expresividad
- batch_size 256 â†’ menos ruido
- n_epochs 15 â†’ mÃ¡s updates/step
- ent_coef 0.02 â†’ exploraciÃ³n
- SDE + entropy â†’ exploraciÃ³n dual

**MÃ©tricas esperadas**:

- Convergencia: 50-100 episodios
- Estabilidad: Muy Alta â­
- COâ‚‚ anual: ~1.72M kg

---

### **SAC (Soft Actor-Critic)** â­â­â­

**Ventajas**:

- Off-policy â†’ sample efficient
- EntropÃ­a automÃ¡tica â†’ exploraciÃ³n adaptativa
- Convergencia rÃ¡pida
- Muy estable

**Desventajas**:

- MÃ¡s complejo (dual Q-networks)
- Tuning de alpha crÃ­tico

**Casos de uso**:

- ProducciÃ³n (efficiency + robustness)
- Problemas con reward sparse
- Cuando sample-efficiency importa

**TIER 2 Impact**:

- NormalizaciÃ³n adaptativa â†’ gradientes consistentes
- Baselines dinÃ¡micas â†’ estrategia por hora
- Bonuses BESS â†’ motivaciÃ³n
- LR 2.5e-4 â†’ convergencia suave
- Hidden 512x512 â†’ expresividad
- update_per_timestep 2 â†’ entrenamiento intenso

**MÃ©tricas esperadas**:

- Convergencia: 15-25 episodios â­ RÃPIDO
- Estabilidad: Muy Alta
- COâ‚‚ anual: <1.70M kg â­ MEJOR

---

## ğŸ“ˆ RESULTADOS ESPERADOS TIER 2

### ImportaciÃ³n Grid (kWh/h)

**Off-Peak (0-8h, 9-17h)**:

```
A2C:  130-140 kWh/h
PPO:  125-135 kWh/h  â† Mejor
SAC:  <130 kWh/h     â† Mejor
```

**Peak (18-21h)**:

```
A2C:  280-290 kWh/h
PPO:  260-270 kWh/h  â† Mejor
SAC:  <250 kWh/h     â† Mejor â­
```

### Convergencia (episodios)

```
A2C:  30-50 episodios
PPO:  50-100 episodios
SAC:  15-25 episodios â­ RÃPIDO
```

### COâ‚‚ Anual (kg)

```
A2C:  ~1.75M kg
PPO:  ~1.72M kg  â† Mejor
SAC:  <1.70M kg  â† Mejor â­
```

### Estabilidad (varianza reward)

```
A2C:  Media (fluctÃºa)
PPO:  Alta (muy suave)  â† Mejor
SAC:  Muy Alta (smooth)  â† Mejor â­
```

---

## ğŸ† RANKING AGENTES (TIER 2)

### Por Convergencia âš¡

1. **SAC**: 15-25 ep (sample efficient off-policy)
2. **A2C**: 30-50 ep (fast on-policy)
3. **PPO**: 50-100 ep (thorough but slower)

### Por Estabilidad ğŸ›¡ï¸

1. **PPO**: Clipping + GAE = muy robusto
2. **SAC**: Off-policy smoothing = muy estable
3. **A2C**: On-policy variance = menos estable

### Por Eficiencia EnergÃ©tica ğŸŒ

1. **SAC**: <1.70M kg COâ‚‚ anual
2. **PPO**: ~1.72M kg COâ‚‚ anual
3. **A2C**: ~1.75M kg COâ‚‚ anual

### Por Balance General â­

1. **SAC**: Mejor convergencia + energÃ­a
2. **PPO**: Mejor estabilidad + robustez
3. **A2C**: MÃ¡s rÃ¡pido pero menos pulido

---

## ğŸ’¡ RECOMENDACIONES TIER 2

### Usa **SAC** si

- âœ… Quieres convergencia rÃ¡pida (15-25 ep)
- âœ… Sample efficiency es crÃ­tico
- âœ… Puedes hacer tuning de alpha
- âœ… Meta: energÃ­a mÃ­nima

### Usa **PPO** si

- âœ… Necesitas mÃ¡xima estabilidad
- âœ… Prefieres robusted sobre velocidad
- âœ… Hyperparams tradicionales mejor
- âœ… Meta: producciÃ³n estable

### Usa **A2C** si

- âœ… Necesitas convergencia inicial rÃ¡pida
- âœ… Problemas de memory/compute limitado
- âœ… Quieres simplicidad
- âœ… Meta: prototyping rÃ¡pido

---

## ğŸ“‹ CONFIGURACIÃ“N LADO-A-LADO

### A2C TIER 2

```python
learning_rate:      2.5e-4    # â†“ de 3e-4
n_steps:            1024      # â†‘ de 512
ent_coef:           0.02      # â†‘ de 0.01
hidden_sizes:       (512, 512)  # â†‘ de (256, 256)
activation:         "relu"    # cambio de tanh
lr_schedule:        "linear"  # cambio de constant
```

### PPO TIER 2

```python
learning_rate:      2.5e-4    # â†“ de 3e-4
batch_size:         256       # â†‘ de 128
n_epochs:           15        # â†‘ de 10
ent_coef:           0.02      # â†‘ de 0.01
hidden_sizes:       (512, 512)  # â†‘ de (256, 256)
activation:         "relu"    # cambio de tanh
lr_schedule:        "linear"  # cambio de constant
use_sde:            True      # NEW: ExploraciÃ³n SDE
```

### SAC TIER 2

```python
learning_rate:      2.5e-4    # â†“ de 3e-4
batch_size:         256       # â†“ de 512
ent_coef:           0.02      # â†‘ de 0.01
target_entropy:     -40       # â†“ de -50
hidden_sizes:       (512, 512)  # â†‘ de (256, 256)
activation:         "relu"
update_per_timestep: 2        # NEW: 2x updates
dropout:            0.1       # NEW: regularizaciÃ³n
# + NormalizaciÃ³n adaptativa + baselines dinÃ¡micas
```

---

## ğŸ”„ MIGRATION TIER 1 â†’ TIER 2

Para cada agente, cambios mÃ­nimos:

**PPO**:

```diff
- batch_size: 128 â†’ 256
- learning_rate: 3e-4 â†’ 2.5e-4
+ n_epochs: 10 â†’ 15
+ ent_coef: 0.01 â†’ 0.02
+ hidden: (256,256) â†’ (512,512)
+ lr_schedule: constant â†’ linear
+ use_sde: True
```

**A2C**:

```diff
- learning_rate: 3e-4 â†’ 2.5e-4
+ n_steps: 512 â†’ 1024
+ ent_coef: 0.01 â†’ 0.02
+ hidden: (256,256) â†’ (512,512)
+ lr_schedule: constant â†’ linear
```

**SAC**:

```diff
- learning_rate: 3e-4 â†’ 2.5e-4
- batch_size: 512 â†’ 256
+ ent_coef: 0.01 â†’ 0.02
+ hidden: (256,256) â†’ (512,512)
+ Adaptive reward normalization
+ Dynamic baselines
+ BESS bonuses
```

---

## ğŸ“Š PRUEBAS TIER 2 (2 EPISODIOS CADA)

```
[ ] A2C: 2 episodios (test convergencia)
[ ] PPO: 2 episodios (test estabilidad)
[ ] SAC: 2 episodios (test efficiency)

Monitorear:
- Reward evolution
- ImportaciÃ³n pico/off-peak
- SOC pre-pico
- Convergencia inicial
```

---

## ğŸ“ REFERENCES

- SAC: Haarnoja et al. "Soft Actor-Critic" (2018)
- PPO: Schulman et al. "PPO" (2017)
- A2C: Mnih et al. "Asynchronous Methods" (2016)
- TIER 2 fixes: Iquitos optimization (2026)

---

**Status**: âœ… READY FOR 2-EPISODE TEST RUN
