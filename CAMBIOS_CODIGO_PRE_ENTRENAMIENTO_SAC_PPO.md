# üìù CAMBIOS DE CONFIGURACI√ìN: SAC & PPO PRE-ENTRENAMIENTO

**Status:** üü° PENDIENTE IMPLEMENTACI√ìN (antes de entrenar)
**Cr√≠tico:** TODOS estos cambios deben estar hechos ANTES de `run_oe3_simulate`
**Orden:** Implementar ‚Üí Commit ‚Üí LUEGO entrenar

---

## SAC - Cambios Espec√≠ficos de C√≥digo

### Archivo: `src/iquitos_citylearn/oe3/agents/sac.py`

```python
# CAMBIO 1: Buffer Size (L√≠nea ~45)
# ANTES:
self.buffer_size = 10_000

# DESPU√âS:
self.buffer_size = 100_000  # 10x mayor, reduce contamination


# CAMBIO 2: Learning Rate (L√≠nea ~50)
# ANTES:
self.learning_rate = 2e-4

# DESPU√âS:
self.learning_rate = 5e-5   # 4x menor, mejor convergence


# CAMBIO 3: Tau - Target Network Update (L√≠nea ~60)
# ANTES:
self.tau = 0.001

# DESPU√âS:
self.tau = 0.01  # 10x mayor, m√°s estable


# CAMBIO 4: Network Architecture (L√≠nea ~70)
# ANTES:
self.net_arch = [256, 256]

# DESPU√âS:
self.net_arch = [512, 512]  # M√°s capacidad para 126 acciones


# CAMBIO 5: Batch Size (L√≠nea ~75)
# ANTES:
self.batch_size = 64

# DESPU√âS:
self.batch_size = 256  # 4x mayor, mejor gradient estimation


# CAMBIO 6: Entropy Coefficient - AUTO TUNING (L√≠nea ~55)
# ANTES:
self.ent_coef = 0.2

# DESPU√âS (NUEVO SISTEMA):
self.ent_coef = 'auto'                    # Auto-tune durante training
self.ent_coef_init = 0.5                  # Valor inicial m√°s alto
self.ent_coef_learning_rate = 1e-4        # Learning rate para entropy


# CAMBIO 7: Add Gradient Clipping (L√≠nea ~80, NUEVO)
# ANTES:
# (no existe)

# DESPU√âS (AGREGAR):
self.max_grad_norm = 1.0  # Prevenir divergencia


# CAMBIO 8: Prioritized Experience Replay (L√≠nea ~65, NUEVO)
# ANTES:
# (no existe)

# DESPU√âS (AGREGAR en __init__ o par√°metros):
@dataclass
class SACConfigOptimized:
    # ... existing fields ...
    buffer_size: int = 100_000
    learning_rate: float = 5e-5
    tau: float = 0.01
    ent_coef: str | float = 'auto'  # String for auto-tune
    ent_coef_init: float = 0.5
    ent_coef_lr: float = 1e-4
    net_arch: List[int] = field(default_factory=lambda: [512, 512])
    batch_size: int = 256
    max_grad_norm: float = 1.0
    
    # NEW: Prioritized Replay
    use_prioritized_replay: bool = True
    per_alpha: float = 0.6      # Prioritization exponent
    per_beta: float = 0.4       # Importance sampling
    per_epsilon: float = 1e-6   # Min priority


# CAMBIO 9: Implementar en modelo.learn() (L√≠nea ~120)
# AGREGAR:
self.policy.max_grad_norm = self.max_grad_norm

# Y cuando creas modelo SAC (stable_baselines3):
model = SAC(
    policy="MlpPolicy",
    env=env,
    learning_rate=self.learning_rate,
    buffer_size=self.buffer_size,
    tau=self.tau,
    ent_coef=self.ent_coef,
    batch_size=self.batch_size,
    gradient_steps=-1,
    max_grad_norm=self.max_grad_norm,  # NUEVO
    # PER si stable-baselines3 lo soporta, sino:
    # usar extension custom
)
```

---

## PPO - Cambios Espec√≠ficos de C√≥digo

### Archivo: `src/iquitos_citylearn/oe3/agents/ppo_sb3.py`

```python
# CAMBIO 1: Clip Range (L√≠nea ~45)
# ANTES:
self.clip_range = 0.2

# DESPU√âS:
self.clip_range = 0.5  # 2.5x mayor, permite cambios policy m√°s agresivos


# CAMBIO 2: N_Steps (Trajectory horizon) (L√≠nea ~50)
# ANTES:
self.n_steps = 2048  # ~2.3 d√≠as

# DESPU√âS:
self.n_steps = 8760  # FULL EPISODE = 1 d√≠a completo (365 horas)
# Permite ver causal chain: 8am ‚Üí mediod√≠a ‚Üí noche


# CAMBIO 3: Batch Size (L√≠nea ~55)
# ANTES:
self.batch_size = 64

# DESPU√âS:
self.batch_size = 256  # 4x mayor


# CAMBIO 4: N_Epochs (M√∫ltiples passes) (L√≠nea ~60)
# ANTES:
self.n_epochs = 3

# DESPU√âS:
self.n_epochs = 10  # M√°s passes para mejor convergence


# CAMBIO 5: Learning Rate (L√≠nea ~40)
# ANTES:
self.learning_rate = 3e-4

# DESPU√âS:
self.learning_rate = 1e-4  # 3x menor


# CAMBIO 6: Add Gradient Clipping (L√≠nea ~75, NUEVO)
# ANTES:
# (no existe)

# DESPU√âS (AGREGAR):
self.max_grad_norm = 1.0


# CAMBIO 7: Entropy Coefficient (L√≠nea ~65)
# ANTES:
self.ent_coef = 0.0  # Sin exploraci√≥n

# DESPU√âS:
self.ent_coef = 0.01  # Peque√±o bonus de exploraci√≥n


# CAMBIO 8: Normalize Advantage (L√≠nea ~70, NUEVO)
# ANTES:
self.normalize_advantage = False

# DESPU√âS:
self.normalize_advantage = True  # Normaliza ventajas por batch


# CAMBIO 9: State-Dependent Exploration (L√≠nea ~80, NUEVO)
# ANTES:
# (no existe)

# DESPU√âS (AGREGAR):
self.use_sde = True           # State-Dependent Exploration
self.sde_sample_freq = -1     # Resample every step


# CAMBIO 10: KL Divergence Safety (L√≠nea ~85, NUEVO)
# ANTES:
# (no existe)

# DESPU√âS (AGREGAR):
self.target_kl = 0.02  # Stop training if KL divergence > 0.02


# CAMBIO 11: Agregar a dataclass (L√≠nea ~20-40)
@dataclass
class PPOConfigOptimized:
    # ... existing fields ...
    clip_range: float = 0.5          # CAMBIO 1
    n_steps: int = 8760              # CAMBIO 2
    batch_size: int = 256            # CAMBIO 3
    n_epochs: int = 10               # CAMBIO 4
    learning_rate: float = 1e-4      # CAMBIO 5
    max_grad_norm: float = 1.0       # CAMBIO 6
    ent_coef: float = 0.01           # CAMBIO 7
    normalize_advantage: bool = True # CAMBIO 8
    use_sde: bool = True             # CAMBIO 9
    sde_sample_freq: int = -1        # CAMBIO 9
    target_kl: float = 0.02          # CAMBIO 10
    gae_lambda: float = 0.98         # NUEVO: Better long-term advantages
    clip_range_vf: float = 0.5       # NUEVO: VF clipping


# CAMBIO 12: Implementar en modelo (L√≠nea ~120)
# ANTES:
model = PPO(
    policy="MlpPolicy",
    env=env,
    learning_rate=self.learning_rate,
    n_steps=self.n_steps,
    batch_size=self.batch_size,
    n_epochs=self.n_epochs,
    ent_coef=self.ent_coef,
)

# DESPU√âS:
model = PPO(
    policy="MlpPolicy",
    env=env,
    learning_rate=self.learning_rate,
    n_steps=self.n_steps,
    batch_size=self.batch_size,
    n_epochs=self.n_epochs,
    ent_coef=self.ent_coef,
    clip_range=self.clip_range,
    clip_range_vf=self.clip_range_vf,
    max_grad_norm=self.max_grad_norm,
    normalize_advantage=self.normalize_advantage,
    use_sde=self.use_sde,
    sde_sample_freq=self.sde_sample_freq,
    target_kl=self.target_kl,
    gae_lambda=self.gae_lambda,
)
```

---

## A2C - SIN CAMBIOS (Mantener como referencia)

```python
# ARCHIVO: src/iquitos_citylearn/oe3/agents/a2c_sb3.py

# ‚úÖ NO MODIFICAR
# A2C es el baseline optimizado
# Sus configuraciones ya son adecuadas
# Se mantiene para comparaci√≥n

# Status: REFERENCIA - COMPARACI√ìN JUSTA
```

---

## Resumen de Cambios por Archivo

### `src/iquitos_citylearn/oe3/agents/sac.py`
```
Total cambios: 9
  - 5 cambios en par√°metros existentes (buffer, lr, tau, net, batch)
  - 3 cambios en nuevos par√°metros (ent_coef auto, grad norm, ent_lr)
  - 1 referencia a PER (si stable-baselines3 lo soporta)

L√≠neas afectadas: ~10-15 l√≠neas modificadas
Complejidad: Baja (cambios simples de valores)
```

### `src/iquitos_citylearn/oe3/agents/ppo_sb3.py`
```
Total cambios: 12
  - 5 cambios en par√°metros existentes (clip, n_steps, batch, epochs, lr)
  - 7 cambios en nuevos par√°metros (grad norm, ent, normalize, sde, kl, lambda, vf_clip)

L√≠neas afectadas: ~15-20 l√≠neas modificadas
Complejidad: Media (m√°s par√°metros nuevos)
```

### `src/iquitos_citylearn/oe3/agents/a2c_sb3.py`
```
Total cambios: 0
  - Sin cambios requeridos
  - Mantener como referencia

L√≠neas afectadas: 0
Complejidad: N/A
```

---

## ‚ö†Ô∏è ORDEN CR√çTICO DE IMPLEMENTACI√ìN

```
1Ô∏è‚É£ ANTES DE EMPEZAR:
   $ git checkout -b oe3-optimization-sac-ppo
   $ git commit -m "Backup: Pre-optimization state"

2Ô∏è‚É£ IMPLEMENTAR CAMBIOS (en este orden):
   a) Editar SAC (9 cambios)
   b) Editar PPO (12 cambios)
   c) Validar: pylint src/iquitos_citylearn/oe3/agents/
   d) Validar: python -c "from src.iquitos_citylearn.oe3.agents import SAC, PPO"

3Ô∏è‚É£ COMMIT CAMBIOS:
   $ git add src/iquitos_citylearn/oe3/agents/
   $ git commit -m "Config: Optimize SAC/PPO configurations for OE3
   
   SAC: buffer 10K‚Üí100K, LR 2e-4‚Üí5e-5, tau 0.001‚Üí0.01, PER enabled
   PPO: clip 0.2‚Üí0.5, n_steps 2048‚Üí8760, batch 64‚Üí256, epochs 3‚Üí10
   
   Details in PLAN_CORRECCION_OPTIMIZACION_SAC_PPO.md"

4Ô∏è‚É£ SOLO DESPU√âS DE COMMITS:
   $ python -m scripts.run_oe3_simulate --config configs/default.yaml
```

---

## ‚úÖ Validaci√≥n Post-Cambios

Despu√©s de implementar todos los cambios, ejecutar:

```bash
# 1. Validar sintaxis
python -m py_compile src/iquitos_citylearn/oe3/agents/sac.py
python -m py_compile src/iquitos_citylearn/oe3/agents/ppo_sb3.py

# 2. Importar m√≥dulos
python -c "from src.iquitos_citylearn.oe3.agents import SACAgent, PPOAgent, A2CAgent; print('‚úÖ All agents imported')"

# 3. Validar dataclasses
python -c "from src.iquitos_citylearn.oe3.agents.sac import SACConfig; c = SACConfig(); print(f'‚úÖ SAC config: buffer={c.buffer_size}, lr={c.learning_rate}')"

# 4. Full environment test
python -m scripts.run_oe3_build_dataset --config configs/default.yaml
echo "‚úÖ Dataset built successfully"

# 5. Dry run (1 timestep only)
python -c "
from src.iquitos_citylearn.oe3.simulate import simulate
# Validate agents can be instantiated with new config
print('‚úÖ Agents can be instantiated with optimized configs')
"
```

---

## üìã Checklist Final

```
‚òê SAC cambios: 9/9 implementados
‚òê PPO cambios: 12/12 implementados
‚òê A2C cambios: 0/0 (sin cambios, referencia)

‚òê Archivos compilados sin errores
‚òê Imports validados
‚òê Dataclasses validadas
‚òê Dataset buildeable

‚òê Commit realizado
‚òê Branch creado
‚òê Backup guardado

‚òê Ready para: python -m scripts.run_oe3_simulate
```

---

## üìä Resultados Esperados Despu√©s de Re-Entrenamiento

```
SAC Optimizado:
  CO‚ÇÇ: +4.7% ‚Üí ??? (Esperado: -10% a -15%)
  EVs sin grid: 75% ‚Üí ??? (Esperado: 85% a 90%)
  Convergencia: Oscillating ‚Üí ??? (Esperado: Smooth)

PPO Optimizado:
  CO‚ÇÇ: +0.08% ‚Üí ??? (Esperado: -15% a -20%)
  EVs sin grid: 93% ‚Üí ??? (Esperado: 94% a 96%)
  Convergencia: Flat ‚Üí ??? (Esperado: Accelerating)

A2C Referencia (sin cambios):
  CO‚ÇÇ: -25.1% (mantiene)
  EVs sin grid: 95% (mantiene)
  Convergencia: Smooth (mantiene)
```

Luego de esto, comparaci√≥n JUSTA porque todos los agentes tienen configuraciones √≥ptimas.
