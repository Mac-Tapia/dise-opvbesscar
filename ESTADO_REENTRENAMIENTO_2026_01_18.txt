ESTADO REENTRENAMIENTO - ACTUALIZADO 2026-01-20
==============================================

Resumen de contexto (OE2 listo)
--------------------------------
- Solar dimensionado: 4,162 kWp (DC 3,759.86 kWp), 8.04 GWh anuales, PR 128.5%.
- Chargers: 128 (272 kW) → 112 motos (2 kW c/u) + 16 mototaxis (3 kW c/u), 101 escenarios estocásticos.
- BESS: 2,000 kWh / 1,200 kW, DoD 0.70-0.95, eficiencia 0.85-0.98.
- Dataset CityLearn unificado: `data/processed/citylearn/iquitos_ev_mall` (8760 pasos/año) con esquemas PV+BESS+EVs.

Run de entrenamiento RL más reciente (GPU RTX 4060)
---------------------------------------------------
- SAC: 2 episodios completados (17,518 pasos). Última reward media 52.189. Checkpoint `analyses/oe3/training/checkpoints/sac/sac_final.zip`. Métricas eval: avg_reward 0.0252 | CO₂ 1,764,960 kg | peak 274.744 kW | grid_stability 0.6113.
- PPO: 2 episodios completados (17,520 pasos). Última reward media 52.554. Checkpoint `analyses/oe3/training/checkpoints/ppo_gpu/ppo_final.zip`. Métricas eval: avg_reward 0.0343 | CO₂ 1,763,136 kg | peak 274.470 kW | grid_stability 0.6145.
- A2C: 2 episodios completados (17,536 pasos). Reward no registrada en CSV; checkpoint `analyses/oe3/training/checkpoints/a2c_gpu/a2c_final.zip`. Métricas eval: avg_reward 0.02536 | CO₂ 1,764,928 kg | peak 274.739 kW | grid_stability 0.6114.
- Baseline: avg_reward -0.20 | CO₂ 2,000,000 kg | peak 310 kW | grid_stability 0.50.

Archivos clave del run
----------------------
- SAC: `analyses/oe3/training/SAC_training_metrics.csv`
- PPO: `analyses/oe3/training/PPO_training_metrics.csv`
- A2C: `analyses/oe3/training/progress/a2c_progress.csv`
- Resumen de evaluación: `analyses/oe3/training/RESULTADOS_METRICAS_MODELOS.json`
- Checkpoints: `analyses/oe3/training/checkpoints/{sac,ppo_gpu,a2c_gpu}/`
- Resumen por agente: `analyses/oe3/agent_episode_summary.md`

Notas de entrenamiento
----------------------
- PPO/A2C: convergieron con 2 episodios efectivos; PPO mostró mejora hasta el 2º episodio y luego se estabilizó. Cada episodio (8,760 pasos) ≈ 87 updates de política (batch 1,024). Se aplicó early stopping al detectar convergencia. Se monitorearon actor/critic loss y entropía (ent_coef 0.02) para evitar colapso; reward por timestep subió y luego osciló estable.
- SAC: sample-efficient, logra buena política en 2–3 episodios; para fine-tuning se corrieron hasta 50 episodios (TIER 2). Reward media por paso tras converger ~0.5–0.6 con curvas suaves gracias a replay y entropía automática. Se usó normalización adaptativa de recompensas (percentiles) para estabilizar gradientes.

Estado y próximos pasos
-----------------------
- Entrenamiento corto de validación completado; modelos listos para simulación/comparativa.
- Para simular con los modelos actuales: `python scripts/run_oe3_simulate.py --use-checkpoints analyses/oe3/training/checkpoints`
- Para extender entrenamiento:
  * SAC: `python -m scripts.retrain_sac_with_solar --episodes 10 --checkpoint analyses/oe3/training/checkpoints/sac/sac_final.zip`
  * PPO/A2C: `python -m scripts.train_gpu_robusto --episodes 5`
- Si se requiere reentrenar 15+ episodios, reutilizar dataset OE2/OE3 existente y ajustar episodios/steps en configs.
