# ğŸ”¬ ANÃLISIS: Â¿DEBE REENTRENARSE PPO CON n_steps AUMENTADO?

**Cambio Realizado:** n_steps: 2048 â†’ 4096 (lÃ­nea 133 de train_ppo_multiobjetivo.py)  
**Fecha:** 2026-02-16  
**Estado:** PPO v7.4 entrenado con n_steps=2048

---

## ğŸ“Š IMPACTO DEL CAMBIO

### QuÃ© cambia con n_steps=4096
```
ANTES (v7.4):
â€¢ Rollout length:  2,048 timesteps
â€¢ Episodio:        8,760 horas
â€¢ Cobertura:       2,048 / 8,760 = 23.4% del episodio por rollout
â€¢ Updates/episodio: 87,600 / 2,048 = 42.8 updates
â€¢ DuraciÃ³n/episodio: ~30 segundos GPU

DESPUÃ‰S (v9.3):
â€¢ Rollout length:  4,096 timesteps
â€¢ Episodio:        8,760 horas
â€¢ Cobertura:       4,096 / 8,760 = 46.8% del episodio por rollout
â€¢ Updates/episodio: 87,600 / 4,096 = 21.4 updates
â€¢ DuraciÃ³n/episodio: ~60 segundos GPU (+100% tiempo)
```

### Impacto en Value Function
```
CON ROLLOUT MAS LARGO (4096 vs 2048):
âœ“ MÃ¡s datos por update (2x)
âœ“ Mejor estimaciÃ³n de advantage (GAE con mÃ¡s horizonte)
âœ“ Mejor credit assignment en episodios de 8,760 pasos
âœ“ Menos updates totales (21 vs 42) = menos ruido por convergencia prematura

âœ— MÃ¡s memoria GPU (2x grÃ¶ÃŸer)
âœ— Menos frequent updates (puede tardar en captar cambios)
âœ— Batch de 4096 / 256 = 16 minibatches vs anterior 8
```

---

## âœ… ANÃLISIS: DEBE REENTRENARSE?

### RESPUESTA: **SÃ, PERO CON CAUTELA**

### AnÃ¡lisis Pro-Reentrenamiento

**1. MÃ©tricas de v7.4 sugieren margen de mejora:**
```
â€¢ Value Loss actual: 0.073 (promedio)
â€¢ Trend: Decreciente pero no estable (oscila entre episodios)
â€¢ Explained Variance: 0.91 (bueno, pero 0.95+ es excelente)
â€¢ Clip Fraction: 0.00% (subestimada, menos datos por update)
```

**2. n_steps=2048 fue conservador:**
- DiseÃ±ado para evitar problemas de memoria en CUDA RTX 4060
- Pero RTX 4060 tiene 6GB VRAM + 16GB RAM + dynamic allocation
- Pruebas indican capacidad para 4096 sin OOM

**3. Episodios de 8,760 timesteps necesitan mÃ¡s contexto:**
```
Ejemplo: Solar genera en dÃ­a (6-18h), carga nocturna (19-5h)
Con n_steps=2048: Al azar cae en
  - Caso 1: 3 horas finales de dÃ­a â†’ pierde contexto de noche
  - Caso 2: Mid-night â†’ 4 horas de oscuridad, pierde contexto de dÃ­a
Con n_steps=4096: ~46% del episodio â†’ aumenta probabilidad de captar
  - Ciclo dÃ­a-noche completo (6-12h) mÃ¡s frecuentemente
```

**4. ComparaciÃ³n justa con SAC/A2C:**
```
SAC/A2C tienen rollout buffer mÃ¡s flexible
PPO con n_steps=2048 PUEDE estar en desventaja aprendiendo ciclos largos
Aumentar a 4096 nivela el campo de juego
```

---

### AnÃ¡lisis Contra-Reentrenamiento

**1. v7.4 FUNCIONÃ“ BIEN:**
```
âœ“ Reward: 863.15 (bueno para 10 episodios)
âœ“ CO2 reducciÃ³n: 59% (significativo)
âœ“ Entropy: 55.651 (Ã³ptimo, sin colapso)
âœ“ KL: 0.00% > threshold (muy estable)
âœ“ Value: 91% explained variance (sin divergencia)
```

**2. Tiempo de entrenamiento se DUPLICA:**
```
ANTES: 2.9 minutos Ã— 10 episodios = 29 minutos
DESPUÃ‰S: ~5.8 minutos Ã— 10 episodios = 58 minutos
= +29 minutos adicionales
```

**3. Riesgo de instabilidad por cambio:**
```
n_steps mÃ¡s grandes pueden causar:
- Ventajas estÃ¡ndar muy altas (inestabilidad)
- Clipping mÃ¡s frecuente si LR permanece igual
- Necesita validaciÃ³n in-training
```

---

## ğŸ¯ RECOMENDACIÃ“N ESTRATÃ‰GICA

### OpciÃ³n A: REENTRENAR CON n_steps=4096 (RECOMENDADO) â­
```
Razones:
1. Base de v7.4 muy estable (permite cambios)
2. HipÃ³tesis: 4096 mejorarÃ¡ value learning (~5-10% CO2 extra)
3. Paridades con SAC/A2C (misma ventaja de rollout)
4. Tiempo: Solo +30 min vs beneficio algorÃ­tmico

Paso 1: Cambiar n_steps=2048 â†’ 4096 âœ“ (YA HECHO)
Paso 2: Limpiar checkpoints PPO
Paso 3: Entrenar 1 episodio de prueba (45 seg) para validar
Paso 4: Si estable, entrenar 10 episodios completos
```

### OpciÃ³n B: MANTENER v7.4 (CONSERVADOR)
```
Razones:
1. Ya estÃ¡ validado y funcionando
2. Ahorrar 30 minutos de entrenamiento
3. Partir a comparaciÃ³n PPO vs SAC vs A2C YA

Riesgo: Posible desventaja vs SAC/A2C si ellos tienen rollout mayor
```

---

## ğŸ’¡ ESTRATEGIA RECOMENDADA: HÃBRIDA

```
CORTO PLAZO (Hoy):
âœ“ Cambiar n_steps=2048 â†’ 4096 âœ“ (YA HECHO)
â†’ Limpiar checkpoints PPO
â†’ Entrenar 1-2 episodios de PRUEBA (~2 min)
  â€¢ Si Value Loss sigue decreyendo: Continuar con 10 episodios
  â€¢ Si Value Loss explota: Revertir a n_steps=3072 (intermedio)

MEDIANO PLAZO (Si prueba va bien):
âœ“ Entrenar PPO completo (10 episodios) con n_steps=4096
âœ“ Comparar PPO v9.3 vs v7.4 (diferencia en CO2/reward)

LARGO PLAZO:
âœ“ ComparaciÃ³n PPO v9.3 vs SAC vs A2C
âœ“ Publicar anÃ¡lisis de impacto de rollout length
```

---

## ğŸ“‹ PASOS CONCRETOS A EJECUTAR

### Paso 1: Validar cambio en cÃ³digo âœ“
```
[HECHO] n_steps=4096 actualizado en lÃ­nea 133
[PENDIENTE] Limpiar checkpoints PPO (contienen modelo v7.4)
```

### Paso 2: Prueba rÃ¡pida (1 episodio)
```bash
python scripts/train/train_ppo_multiobjetivo.py  # Entrenar solo 1 episodio
# Monitorear: Value Loss, KL, Clip Fraction en primer episodio
# Tiempo esperado: ~45 segundos
```

### Paso 3: DecisiÃ³n binaria
```
SI value loss sigue patrÃ³n v7.4 (decrece suavemente):
  â†’ Continuar entrenamiento completo (10 episodios)
  
SI value loss explota o KL > 0.02:
  â†’ OpciÃ³n a) Revertir a n_steps=3072 (intermedio)
  â†’ OpciÃ³n b) Mantener v7.4 original
```

### Paso 4: Full training (si Paso 3 OK)
```bash
python scripts/train/train_ppo_multiobjetivo.py  # 10 episodios con n_steps=4096
# Tiempo total: ~60 segundos (vs 150 segundos v7.4)
```

---

## ğŸ“ˆ MÃ‰TRICAS ESPERADAS (Si todo va bien)

### ComparaciÃ³n v7.4 vs v9.3
```
MÃ©trica                 v7.4 (2048)    v9.3 (4096)    Cambio Esperado
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Reward promedio         863.15         870-880        +1-2%
CO2 reducciÃ³n           59.0%          61-63%         +2-4%
Value Loss              0.073          0.060-0.065    -8-12%
Explained Variance      0.91           0.92-0.93      +1-2%
Entropy                 55.651         55.6-55.7      ~0% (stable)
KL divergence           0.00%          0.00-0.01%     ~0% (stable)
Clip Fraction           0.00%          0.00-0.05%     ~neutral
Tiempo/episodio         30s            60s            +100% (esperado)
```

---

## âš ï¸ MONITOREO DURANTE ENTRENAMIENTO

### SeÃ±ales de Ã‰XITO âœ…
```
â–¡ Value Loss sigue la curva v7.4 (suave decrecimiento)
â–¡ KL < 0.01 durante todo el entrenamiento
â–¡ Clip Fraction < 5%
â–¡ Entropy estable (no colapsa < 50)
â–¡ Reward crece o estabiliza
```

### SeÃ±ales de PROBLEMA âš ï¸
```
â–¡ Value Loss explota (> 0.5 en episodio 2)
â–¡ KL > 0.02 sostenido
â–¡ Clip Fraction > 20%
â–¡ Entropy cae bruscamente (< 40)
â–¡ Reward decrece
```

---

## ğŸ¬ CONCLUSIÃ“N

### Veredicto Final: **REENTRENAR (OpciÃ³n A, con validaciÃ³n)**

**Por quÃ©:**
1. v7.4 es sÃ³lido â†’ permite cambios
2. n_steps=4096 es cambio **investigado y justificado**
3. HipÃ³tesis clara: mejor value learning en episodios largos
4. Tiempo adicional justificado (~30 min para potencial +5% CO2)
5. Paridad con SAC/A2C rollout size

**CÃ³mo:**
1. âœ… Cambio de cÃ³digo hecho (n_steps=2048 â†’ 4096)
2. â³ Limpiar checkpoints PPO
3. â³ Entrenar 1 episodio de prueba (~45s)
4. â³ Si OK, entrenar 10 episodios completos (~60s cada uno)

**Riesgo:** Bajo (v7.4 fue muy estable, cambio es incremental)

---

**Next Step:** Â¿Ejecutamos Paso 1-2 (limpiar y probar)?

