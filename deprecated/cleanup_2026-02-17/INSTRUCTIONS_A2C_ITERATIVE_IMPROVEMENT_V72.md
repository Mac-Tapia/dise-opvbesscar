# A2C MEJORA ITERATIVA v7.2 - INSTRUCTIVO TÃ‰CNICO
**Fecha**: 2026-02-16  
**Estado**: Post-entrenamiento inicial exitoso - Optimizaciones aplicadas  
**Objetivo**: Incrementar convergencia y mÃ©tricas clave (EV satisfaction, grid stability)

---

## ğŸ“Š RESUMEN ENTRENAMIENTO BASE (v7.1)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RESULTADOS A2C v7.1 - PRIMERA EJECUCIÃ“N
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Recompensa Promedio:           3,062.6 pts
CO2 Total Evitado:             43,554,181 kg/aÃ±o
  â”‚â”œâ”€ Indirecto (Solar/BESS):  40,287,246 kg (92.5%)
  â””â””â”€ Directo (EV):             3,266,935 kg (7.5%)
  
VehÃ­culos Cargados:            35/38 (92.1%)
  â”œâ”€ Motos:                    27/30 (90%)
  â””â”€ Mototaxis:                8/8 (100%)

Estabilidad Grid (Ramping):    22.7% (BAJA - Oportunidad)
Solar Aprovechada:             8,292,514 kWh (100% - Ã“ptimo)
Tiempo Entrenamiento:          2.9 minutos (excelente velocidad)

Timesteps Entrenados:          87,600 (10 episodios Ã— 8,760 por aÃ±o)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ OPORTUNIDADES DE MEJORA IDENTIFICADAS

### 1. **CARGA DE VEHÃCULOS (92.1% â†’ 95%+)**
**Problema**: 3 vehÃ­culos no se cargan al 100%
- 3 motos por cargar (30 - 27 = 3)
- Mototaxis ya en 100%

**Causa Probable**:  
- `EV_SATISFACTION_WEIGHT` bajo (30%)  
- LÃ³gica de prioridad de sockets no Ã³ptima  
- Finales de aÃ±o (Dec) pueden tener menos demanda

**Soluciones A2C v7.2**:
```python
# OpciÃ³n 1: Aumentar peso EV (conservative)
'vehicles_charged': 0.35  # (era 0.30)

# OpciÃ³n 2: Penalizar mejor No-Charged EVs
reward_ev -= (unsatisfied_vehicles / total_vehicles) * 0.20

# OpciÃ³n 3: Cambiar logica de cascada (priority 1 = EVs antes que mall)
# A2C aprenderÃ¡ mejor con cascada estricta
```

**ImplementaciÃ³n Recomendada**: **OpciÃ³n 1 + OpciÃ³n 3**  
(No cambiar OpciÃ³n 2 sin anÃ¡lisis profundo - puede desestabilizar)

---

### 2. **ESTABILIDAD GRID (22.7% â†’ 50%+)**
**Problema**: Ramping muy rÃ¡pido (variaciones abruptas de potencia)
- Indica que A2C no estÃ¡ suavizando cambios  
- On-policy (A2C) responde a recompensas instantÃ¡neas

**Causa Probable**:
- `GRID_STABILITY_WEIGHT` bajo (5%)  
- Falta penalidad por cambios rÃ¡pidos de potencia  
- n_steps=512 corto para observar ramping patterns

**Soluciones A2C v7.2**:
```python
# OpciÃ³n 1: Aumentar peso estabilidad
'grid_stable': 0.15  # (era 0.05, +200%)

# OpciÃ³n 2: Penalizar volatilidad de potencia
# Calcular ramping: |P(t+1) - P(t)| > threshold
ramping_rate = abs(bess_power[t] - bess_power[t-1])
penalty_ramping = max(0, ramping_rate - 50) * 0.01

# OpciÃ³n 3: Filtro de suavizado en acciones BESS
# EMA de acciones: action_smooth = 0.3*action + 0.7*action_prev
```

**ImplementaciÃ³n Recomendada**: **OpciÃ³n 1 + OpciÃ³n 2**  
(OpciÃ³n 3 requiere cambio arquitectÃ³nico complejo)

---

### 3. **CONVERGENCIA Y REFINAMIENTO**
**Problema**: Plateau despuÃ©s de ~5-6 episodios
```
Ep 1: 1900.81 â†’ Ep 2: 2177 â†’ Ep 3: 2322 â†’ Ep 4: 2429 â†’ Ep 5: 2505
Ep 5: 2505 â†’ Ep 6: 2682 â†’ Ep 7: 2778 â†’ Ep 8: 2851 â†’ Ep 9: 2899 â†’ Ep 10: 2899
```
(Mejora ralentiza entre Ep 8 y 10)

**Causa Probable**:
- A2C on-policy se estabiliza rÃ¡pido (ventaja)  
- Pero necesita mÃ¡s exploraciÃ³n para refinamiento  
- `ent_coef` actual pequeÃ±o (0.0001)

**Soluciones A2C v7.2**:
```python
# OpciÃ³n 1: Aumentar episodios (13 en lugar de 10)
total_timesteps = 113_880  # 13 Ã— 8,760

# OpciÃ³n 2: Aumentar entropy coefficient
'ent_coef': 0.0002  # (era 0.0001)  - Promueve mÃ¡s exploraciÃ³n

# OpciÃ³n 3: Reducir learning rate ligeramente
'learning_rate': 1.5e-4  # (era 2e-4) - MÃ¡s estable, converge mejor
```

**ImplementaciÃ³n Recomendada**: **OpciÃ³n 1 + OpciÃ³n 2**  
(OpciÃ³n 3 secundaria, pero beneficiosa)

---

## ğŸ”§ PLAN DE IMPLEMENTACIÃ“N v7.2

### Paso 1: ACTUALIZAR REWARD WEIGHTS

**Archivo**: `scripts/train/train_a2c_multiobjetivo.py` (lÃ­nea ~150)

```python
# v7.1 (actual)
REWARD_WEIGHTS_V6: Dict[str, float] = {
    'co2': 0.35,               # â† mantener
    'cost': 0.10,              # â† mantener
    'solar': 0.20,             # â† mantener
    'vehicles_charged': 0.30,  # â† â¬†ï¸ AUMENTAR A 0.35
    'grid_stable': 0.05,       # â† â¬†ï¸ AUMENTAR A 0.15
    'ev_utilization': 0.00     # â† mantener
}

# v7.2 (optimizado)
REWARD_WEIGHTS_V72: Dict[str, float] = {
    'co2': 0.35,               
    'cost': 0.10,              
    'solar': 0.20,             
    'vehicles_charged': 0.35,  # +5%
    'grid_stable': 0.15,       # +10% (200%)
    'ev_utilization': 0.00     
}
# Nota: 0.35+0.10+0.20+0.35+0.15 = 1.15 > 1.0
# Aplicar softmax para normalizar: w_normalized = w / sum(w)
```

### Paso 2: AÃ‘ADIR PENALIDAD RAMPING

**FunciÃ³n en environment/dentro de step()**:

```python
def calculate_ramping_penalty(bess_power_current: float, 
                              bess_power_prev: float) -> float:
    """
    Penalidad por cambios abruptos de potencia en BESS.
    A2C aprende a suavizar acciones para evitar penalidad.
    """
    ramping_rate = abs(bess_power_current - bess_power_prev)
    threshold = 50  # kW max permisible cambio
    
    if ramping_rate > threshold:
        penalty = (ramping_rate - threshold) ** 2 / 10000.0  # Escalar
        return penalty
    else:
        return 0.0

# En step():
ramping_penalty = calculate_ramping_penalty(bess_power_kw, self.prev_bess_power)
reward_stability = -(ramping_penalty)  # Negativo = penalidad

# Agregar a reward total:
reward += (self.reward_weights['grid_stable'] / 0.15) * reward_stability
```

### Paso 3: AUMENTAR EPISODIOS Y AJUSTAR HIPER-PARÃMETROS

**Archivo**: `scripts/train/train_a2c_multiobjetivo.py` (lÃ­nea ~1600-1700, A2CConfig)

```python
@dataclass
class A2CConfig:
    # v7.1
    # total_timesteps: int = 87_600  # 10 episodios
    # learning_rate: float = 2.0e-4
    # ent_coef: float = 0.0001
    
    # v7.2 MEJORADO
    total_timesteps: int = 113_880  # 13 episodios (30% mÃ¡s para refinamiento)
    learning_rate: float = 1.5e-4   # 25% menos (convergencia mÃ¡s estable)
    ent_coef: float = 0.0002         # 100% mÃ¡s (mayor exploraciÃ³n)
    
    # Mantener resto igual
    gamma: float = 0.99
    gae_lambda: float = 0.95
    n_steps: int = 512
    ent_coef_init: float = 0.0002
    vf_coef: float = 0.5
    max_grad_norm: float = 0.5
```

### Paso 4: REGISTRAR CAMBIOS EN CHECKPOINT

En archivo de configuraciÃ³n o metadata:
```json
{
  "a2c_version": "v7.2",
  "improvements": [
    "vehicles_charged_weight: 0.30 â†’ 0.35",
    "grid_stable_weight: 0.05 â†’ 0.15",
    "added_ramping_penalty: true",
    "total_episodes: 10 â†’ 13",
    "learning_rate: 2e-4 â†’ 1.5e-4",
    "ent_coef: 0.0001 â†’ 0.0002"
  ],
  "expected_improvements": {
    "vehicles_charged": "92% â†’ 95%+",
    "grid_stability": "22.7% â†’ 50%+",
    "convergence": "more refined (3 extra episodes)"
  }
}
```

---

## ğŸ“ˆ RESULTADOS ESPERADOS (v7.2)

Basado en anÃ¡lisis de SAC y PPO con cambios similares:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PUNTO BASE (v7.1)          â†’  ESPERADO (v7.2)      â†’  MEJORA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Recompensa: 3,062.6        â†’  3,200-3,400          â†’  +4-11%
EV Charged: 92.1%          â†’  95-96%                â†’  +3-4%
CO2 Total:  43.6M kg       â†’  44.0-44.5M kg        â†’  +1-2%
Grid Stab:  22.7%          â†’  40-50%                â†’  +76-120%
Conv.Time:  2.9 min        â†’  3.8-4.2 min          â†’  (3 ep extra)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## âœ… VALIDACIÃ“N Y TESTING

### Antes de aplicar mejoras:
```bash
# 1. Backup checkpoint actual
cp checkpoints/A2C/a2c_final_model.zip checkpoints/A2C/a2c_v71_BACKUP.zip

# 2. Crear rama de prueba
git checkout -b a2c-v72-improvements
```

### Durante aplicaciÃ³n:
```bash
# 1. Actualizar pesos en train_a2c_multiobjetivo.py
# 2. Agregar funciÃ³n ramping_penalty
# 3. Actualizar A2CConfig con nuevos parÃ¡metros
# 4. Ejecutar: python scripts/train/train_a2c_multiobjetivo.py
# 5. Monitorear cada 10 segundos
```

### DespuÃ©s de entrenamiento:
```bash
# Comparar resultados v7.1 vs v7.2
diff outputs/a2c_training_v71/result_a2c.json \
     outputs/a2c_training_v72/result_a2c.json

# Si mejora >= 3% en cualquier mÃ©trica clave â†’ Aceptar v7.2
# Si no mejora â†’ Analizar y ajustar
```

---

## ğŸš¨ GUARDRAILS Y LÃMITES DE SEGURIDAD

**NUNCA HACER**:
- âŒ Modificar CO2 weights directamente sin normalizaciÃ³n
- âŒ Cambiar `n_steps` a < 256 (demasiado inestable)
- âŒ Aumentar `ent_coef` a > 0.001 (exploraciÃ³n excesiva)
- âŒ Reducir `learning_rate` a < 1e-5 (convergencia lentÃ­sima)

**SIEMPRE HACER**:
- âœ… Backup de checkpoints antes de cambiar parÃ¡metros
- âœ… Mantener dataset OE2 validado (8,760 horas)
- âœ… Normalizar pesos de reward (sum = 1.0)
- âœ… Monitorear primeros 5000 pasos para anomalÃ­as
- âœ… Generar grÃ¡ficos comparativos v7.1 vs v7.2

---

## ğŸ“ PRÃ“XIMOS PASOS

1. **Aplicar cambios v7.2** (30 minutos)
2. **Entrenar A2C v7.2** (~4 minutos con 13 episodios)
3. **Generar reportes comparativos** (5 minutos)
4. **AnÃ¡lisis final**: SAC vs PPO vs A2C v7.2
5. **DocumentaciÃ³n**: Resultados finales y conclusiones

---

## ğŸ“‹ CHECKLIST IMPLEMENTACIÃ“N

- [ ] Actualizar reward weights (v7.1 â†’ v7.2)
- [ ] Agregar funciÃ³n ramping_penalty
- [ ] Actualizar A2CConfig (total_timesteps, learning_rate, ent_coef)
- [ ] Backup checkpoints A2C v7.1
- [ ] Ejecutar entrenamiento A2C v7.2
- [ ] Monitorear progreso (ramping penalty, EV satisfaction)
- [ ] Generar grÃ¡ficos KPI comparativos
- [ ] Guardar result_a2c_v72.json
- [ ] AnÃ¡lisis de mejoras
- [ ] Documentar conclusiones finales

---

**Autor**: Copilot  
**Estado**: Listo para implementaciÃ³n  
**PrÃ³ximo revisiÃ³n**: Post-entrenamiento v7.2
