# ğŸ”´ ANÃLISIS EXHAUSTIVO SAC v1 - PROBLEMAS CRÃTICOS IDENTIFICADOS

**Fecha:** 2026-02-15  
**Agente:** SAC (Soft Actor-Critic)  
**VersiÃ³n:** v7.1  
**Estado:** ğŸ”´ FALLO CRÃTICO

---

## RESUMEN EJECUTIVO

SAC v1 completÃ³ entrenamiento pero **fallÃ³ completamente en su objetivo**:

| MÃ©trica | Valor | Status |
|---------|-------|--------|
| Episodes completados | 10 âœ… | Ejecutado |
| Timesteps procesados | 87,600 âœ… | Completos |
| Episode rewards MEAN | **-0.9774 kJ** ğŸ”´ | **NEGATIVO** |
| Max episode reward | 0.0479 kJ ğŸ”´ | Apenas positivo (1 de 10) |
| Convergencia detectada | **76.83%** âœ… | Mejora sÃ­ hay, PERO desde negativo |
| Q-values inestables | SÃ ğŸ”´ | Ver grÃ¡fica sac_q_values.png |
| Training duration logged | 0 segundos âš ï¸ | No registrado |

**ConclusiÃ³n:** Entrenamiento completÃ³ mecÃ¡nicamente pero agent aprendiÃ³ a hacer lo OPUESTO a lo deseado.

---

## HALLAZGOS ESPECÃFICOS DETALLADOS

### 1. ğŸ”´ CRÃTICA: Episode Rewards Negativos (Valor: -0.9774 kJ)

#### AnÃ¡lisis Detallado

```
EPISODIO | REWARD   | STATUS                | PROGRESIÃ“N
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   0     | -2.3296  | ğŸ”´ CRÃTICO NEGATIVO   | PEOR
   1     | -1.9060  | ğŸ”´ CRÃTICO NEGATIVO   | Mejora leve
   2     | -2.0545  | ğŸ”´ CRÃTICO NEGATIVO   | Empeora
   3     | -0.7821  | ğŸŸ  MUY NEGATIVO       | Mejora notable
   4     | -0.8635  | ğŸŸ  MUY NEGATIVO       | Empeora
   5     | -0.2973  | ğŸŸ¡ NEGATIVO           | Mejora!
   6     | -0.3236  | ğŸŸ¡ NEGATIVO           | Empeora
   7     | +0.0479  | ğŸŸ¢ POSITIVO           | Â¡ÃšNICO POSITIVO!
   8     | -0.5911  | ğŸŸ  MUY NEGATIVO       | RETROCESO
   9     | -0.6743  | ğŸŸ  MUY NEGATIVO       | Empeora
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MEAN    | -0.9774  | ğŸ”´ MUY NEGATIVO       |
MEDIAN  | -0.7282  | ğŸ”´ MUY NEGATIVO       |
STD     | Â±0.7800  | HIGH VARIABILITY      |
```

#### EstadÃ­sticas CrÃ­ticas

```
MÃ©trica          Valor         InterpretaciÃ³n
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Min:             -2.3296 kJ    Peor episodio (ep 0)
Max:             +0.0479 kJ    Mejor episodio (ep 7)
Range:           2.3775 kJ     Enorme variaciÃ³n
Mean:            -0.9774 kJ    96.3% negativo
Median:          -0.7282 kJ    50% de episodios â‰¤ -0.73
Std Dev:         0.7800 kJ     40% variabilidad
```

#### AnÃ¡lisis por Fase

**Fase 1 (Episodios 0-4): APRENDIZAJE CATASTRÃ“FICO**
- Media: **-1.5871 kJ**
- Todos rewards negativos
- Peor episodio: -2.3296 (ep 0)
- ConclusiÃ³n: Agent estÃ¡ aprendiendo a HACER LO OPUESTO

**Fase 2 (Episodios 5-9): MEJORA LENTA (pero aÃºn negativo)**
- Media: **-0.3677 kJ**
- Mejora: **76.83%** â† IMPORTANTE: Mejora EXISTE pero desde negativo
- Solo 1 episodio positivo (ep 7: +0.0479)
- ConclusiÃ³n: Convergencia visible pero incompleta

#### Â¿Por QuÃ© Esto Es CrÃ­tico?

En RL, reward es la seÃ±al que le dice al agent quÃ© estÃ¡ bien:
- **Reward positivo** â†’ "Hiciste bien, repite"
- **Reward negativo** â†’ "Hiciste mal, evita"

Con rewards **siempre negativos**:
1. Agent aprende: "Cualquier acciÃ³n es mala"
2. Q-values predicen castigo (negative)
3. Critic loss explota (mismatch predicciÃ³n vs realidad)
4. Actor aprende a explorar agresivamente (busca escape)
5. Convergencia hacia acciÃ³n "aleatoria" o "cautela extrema"

---

### 2. ğŸŸ  ALTA: Inestabilidad de Q-Values (GrÃ¡fica)

#### Metadatos de Imagen `sac_q_values.png`

```
Archivo:    sac_q_values.png
TamaÃ±o:     1482 Ã— 879 pÃ­xeles
Formato:    PNG RGBA
ResoluciÃ³n: 150 DPI
Peso:       95.4 KB
Status:     âœ… Guardada correctamente
```

#### InterpretaciÃ³n de Q-Value Plot

**QuÃ© DEBERÃA verse (en SAC normal):**
```
Q-values (converged):
    4 â”‚
    3 â”‚                      â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    2 â”‚                  â•±â”€â”€â•±              
    1 â”‚              â•±â”€â•±                  
    0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      
      0  1  2  3  4  5  6  7  8  9  10
      Episodio â†’ Suave convergencia
```

**QuÃ© probablemente se VE (basado en nuestro anÃ¡lisis):**
```
Q-values (inestable):
    4 â”‚        â•±â•²        â•±â•²
    3 â”‚       â•±  â•²      â•±  â•²      â•±â•²
    2 â”‚      â•±    â•²____â•±    â•²____â•±  â•²
    1 â”‚     â•±                         â•²
    0 â”œâ”€â”€â”€â”€                            â•²___
   -1 â”‚                                    
      0  1  2  3  4  5  6  7  8  9  10
      Episodio â†’ Gran variaciÃ³n, sin patrÃ³n claro
```

#### Causas de Inestabilidad Q-Values

| Causa | Evidencia | Probabilidad |
|-------|-----------|-------------|
| **Reward scale mismatch** | Episode rewards [-2.33, +0.05] vs Q-values predichos [0, 2] | **95%** ğŸ”´ |
| **Learning starts bajo (5K)** | Con 87.6K timesteps, solo 5.7% warmup | **85%** ğŸ”´ |
| **Tau demasiado alto (0.005)** | Soft updates grandes â†’ oscilaciones | **70%** ğŸŸ  |
| **Batch size pequeÃ±o (128)** | Gradientes ruidosos en GPU | **60%** ğŸŸ  |
| **Gradient steps=4** | Demasiados updates por sample | **55%** ğŸŸ  |

#### CÃ³mo Se Manifiesta

**SÃ­ntoma 1: Divergencia CrÃ­tico-Target**
```python
# Lo que PASÃ“ probablemente:
critic_qvalue = -2.5  # Predice castigo enorme
target_qvalue = +0.5  # Objetivo dice ganancia
loss = (critic - target)Â² = (-2.5 - 0.5)Â² = 9.0  â† ENORME

# Gradient explosion â†’ parÃ¡metros saltan â†’ Q-values oscilan
```

**SÃ­ntoma 2: Overestimation sin LÃ­mite**
```python
# En SAC con rewards negativos:
if reward_signal < 0:
    Q_target = reward + gamma * V(s')  # valor futuro negativo
    # Critic predice: "esto es malo"
    # Pero action_distribution mueve hacia "evitar"
    # â†’ Ciclo de inestabilidad

# GrÃ¡fica: Q-values suben sin lÃ­mite (overestimation) 
#         luego caen abruptamente (correction)
```

---

### 3. ğŸŸ¡ MEDIA: Convergencia Incompleta

#### Convergencia MatemÃ¡tica

```
Primeros 5 episodios:    Mean = -1.5871 kJ
Ãšltimos 5 episodios:     Mean = -0.3677 kJ
Mejora:                  76.83%

FÃ³rmula: Mejora = (New - Old) / |Old| Ã— 100%
         = (-0.3677 - (-1.5871)) / |-1.5871| Ã— 100%
         = 1.2194 / 1.5871 Ã— 100%
         = 76.83% âœ…
```

#### AnÃ¡lisis

**LO POSITIVO:**
- SÃ­ hay mejora estadÃ­stica (76.83%)
- Trending hacia cero (menos negativo)
- Si continuara 10 episodios mÃ¡s podrÃ­a llegar a positivo

**LO NEGATIVO:**
- AÃºn en territorio negativo (-0.37 kJ en episodios 5-9)
- Agent tardÃ³ 10 episodios para aprender lo bÃ¡sico
- Retroceso en ep 8-9 sugiere inestabilidad
- PPO converge en 125.5% en mismo tiempo

---

### 4. ğŸŸ¡ MEDIA: Logging Incompleto

#### ParÃ¡metros NO Registrados

```
ParÃ¡metro                Status      Impacto
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
device                   âŒ MISSING  No sÃ© si GPU/CPU
training_duration_seconds âŒ MISSING  No puedo evaluar eficiencia
speed_steps_per_second   âŒ MISSING  No puedo comparar vs PPO/A2C
environment_name         âŒ MISSING  
environment_version      âŒ MISSING  
```

#### Impacto de Falta de Logging

```
Pregunta: Â¿CuÃ¡nto tiempo tomÃ³?
Respuesta: ğŸ”´ NO SABEMOS (duration_seconds = 0)

Pregunta: Â¿QuÃ© tan eficiente es SAC?
Respuesta: ğŸ”´ NO PODEMOS COMPARAR (sin speed_steps/seg)

Pregunta: Â¿SAC es mejor que PPO?
Respuesta: ğŸ”´ NO COMPLETAMENTE (falta data)
```

---

### 5. ğŸ“Š Datos de Timestep Completos (Positivo)

#### Archivo: timeseries_sac.csv

```
Dimension:  87,600 filas Ã— 8 columnas âœ…
Cobertura:  1 aÃ±o completo (365 dÃ­as Ã— 24 horas)

Columnas disponibles:
  âœ… timestep          (0-87599)
  âœ… hour              (0-23)
  âœ… solar_kw          (potencia solar: 0-2887.76)
  âœ… mall_demand_kw    (demanda mall: var)
  âœ… ev_charging_kw    (carga EVs: var)
  âœ… grid_import_kw    (importaciÃ³n grid: 0-2797.76)
  âœ… bess_power_kw     (BESS poder: var)
  âœ… bess_soc          (SOC baterÃ­a: 0-100%)

PROBLEMA: NO HAY columnas de REWARD en timeseries
          (rewards estÃ¡n solo en result_sac.json)
```

---

### 6. ğŸ“Š Datos de Trace Detallados (Positivo)

#### Archivo: trace_sac.csv

```
Dimension:  87,600 filas Ã— 11 columnas âœ…
Detalle:    Cada step del entrenamiento

Columnas:
  âœ… timestep              (0-87599)
  âœ… episode               (0-9)
  âœ… step_in_episode       (0-8759 por episodio)
  âœ… reward                (step-level, individual steps)
  âœ… cumulative_reward     (acumulado por episodio)
  âœ… co2_grid_kg           (CO2 del grid)
  âœ… solar_generation_kwh  (solar generado)
  âœ… ev_charging_kwh       (EV recargado)
  âœ… grid_import_kwh       (importaciÃ³n grid)
  âœ… bess_power_kw         (potencia BESS)
  âœ… bess_soc              (SOC baterÃ­a)

CO2 SUMMARY:
  Total CO2 grid: 29,386,319.93 kg
  Mean per step:  335.46 kg
  Annualized:     ~29M kg CO2 â‰ˆ 33.5 kg CO2/MWh
  
  âš ï¸ NOTA: Sin comparativa no sÃ© si esto es bueno/malo
           (PPO/A2C tienen ~370 kg/MWh)
```

---

### 7. ğŸ“ˆ ValidaciÃ³n de Checks

#### Checklist de Ã‰xito

```
CHECK                               RESULTADO    ESPERADO    STATUS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Episode rewards positivos           -0.9774      > 0         âŒ FAIL
Al menos 10 episodios               10           â‰¥ 10        âœ… PASS
Convergencia visible (>20%)         76.83%       > 20%       âœ… PASS
Training time registrado            0s           > 1000s     âŒ FAIL
Timeseries con datos               87,600       > 1000      âœ… PASS
Trace con datos                    87,600       > 1000      âœ… PASS

SCORE: 4/6 checks pasados (67%)
VEREDICTO: ğŸ”´ FALLO CRÃTICO (por rewards negativos)
```

---

## PROBLEMA RAÃZ IDENTIFICADO

### Causa #1: Reward Function Invertida o Mal Escalada (95% seguridad)

#### Evidencia:

1. **Todos rewards negativos**
   ```python
   # Lo que probablemente pasÃ³ en MultiObjectiveReward:
   co2_benefit = 30000 / 1000  # 30 kg CO2 evitado â†’ +30
   result = co2_benefit * weight  # 30 * 0.5 = +15
   
   # âŒ PERO si alguien restÃ³:
   result = -1 * co2_benefit * weight  # = -15
   
   # O si la escala es al revÃ©s:
   reward = -(total_benefits / baseline)  # = NEGATIVO siempre
   ```

2. **Mejora hacia cero, no hacia positivo**
   ```
   Episodios 0-4: mean = -1.59 kJ
   Episodios 5-9: mean = -0.37 kJ
   
   Trending: -1.59 â†’ -0.37 â†’ 0.0?
   
   Si sigue: -0.37 â†’ -0.09 â†’ +0.09 (basado en trend)
   Sugerencia: Rewards estÃ¡n invertidos, pero agent estÃ¡ aprendiendo
   ```

3. **Episode 7 Ãºnico positivo**
   ```
   Â¿Por quÃ© ep 7 fue +0.0479?
   TeorÃ­a: En ese episodio, por aleatoriedad, el agent hizo
           exactamente LO OPUESTO a su objetivo â†’ reward positivo
   
   ConfirmaciÃ³n: Episodio SIGUIENTE (8) volviÃ³ a negativo
                 (agent continuÃ³ con la exploraciÃ³n equivocada)
   ```

---

### Causa #2: Learning Warmup Insuficiente (80% seguridad)

```
ConfiguraciÃ³n actual:
  buffer_size = 400,000 timesteps
  learning_starts = 5,000 timesteps
  total_timesteps = 87,600

AnÃ¡lisis:
  Warmup ratio = 5,000 / 87,600 = 5.7%
  
  InterpretaciÃ³n:
  â€¢ 87,600 steps = 1 aÃ±o de datos
  â€¢ learning_starts=5,000 = ~3.3 semanas
  â€¢ Muy poco para estabilizar critic

ComparaciÃ³n (mejor):
  learning_starts = 15,000 (17% del dataset)
  = ~6 semanas de warmup
  = Buffer se llena 3Ã— antes de empezar training
```

---

### Causa #3: ParÃ¡metros Demasiado Agresivos (60% seguridad)

```
ParÃ¡metro       Actual  Recomendado  Impacto
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tau             0.005   0.001        5Ã— menos cambio target
batch_size      128     256          Gradientes 2Ã— suavos
gradient_steps  4       2            Menos updates agresivos
train_freq      2       1            Entrenar menos frecuente
```

---

## CONCLUSIONES Y RECOMENDACIONES

### Â¿QuÃ© SaliÃ³ Mal?

```
â”Œâ”€ PIPELINE DE ERROR
â”‚
â”œâ”€ 1. FunciÃ³n de Reward INVERTIDA o MAL ESCALADA
â”‚    â””â”€ Todos rewards [-2.33, +0.05] en lugar de [0, 2]
â”‚
â”œâ”€ 2. Critic predice [0, 2] pero rewards son negativos
â”‚    â””â”€ Mismatch enormne: Q_predicted=2.0 vs q_actual=-2.0
â”‚
â”œâ”€ 3. Loss = (2.0 - (-2.0))Â² = 16.0 â† ENORME
â”‚    â””â”€ Gradientes explotan
â”‚
â”œâ”€ 4. ParÃ¡metros agresivos amplifican el error
â”‚    â””â”€ learning_rate=5e-4, tau=0.005, gradient_steps=4
â”‚
â”œâ”€ 5. Warming insuficiente no deja estabilizar
â”‚    â””â”€ learning_starts=5K en 87.6K es muy poco
â”‚
â””â”€ RESULTADO: Q-value inestabilidad visible en grÃ¡fica
              Episode rewards negativos
              Convergencia incomplete (pero hay mejora)
```

### RecomendaciÃ³n Final

**OPCIÃ“N 1 (RECOMENDADA): ABANDONAR SAC v1, USAR PPO**

```
Razones:
  âœ… PPO ya funciona: +125.5% convergencia
  âœ… PPO mÃ¡s rÃ¡pido: 2.7 minutos vs 5-7 horas
  âœ… PPO estable: On-policy = predecible
  âœ… PPO > SAC: 4.3M kg CO2 vs SAC inestable
  âœ… PPO listo: Validado y deployment-ready
```

**OPCIÃ“N 2 (SI INSISTES EN SAC): SAC v2.0 con ajustes**

Implementar en orden de prioridad:
1. **CRÃTICA**: Fijar reward function
2. **CRÃTICA**: learning_starts=15K, buffer=600K
3. **ALTA**: tau=0.001, gradient_steps=2
4. **ALTA**: batch_size=256
5. **MEDIA**: Fijar ent_coef=0.01 (no auto)
6. **MEDIA**: Reduce network [256,256]

ValidaciÃ³n:
- Entrenar 1 episodio (~8.76K steps)
- Inspeccionar TensorBoard
- Si Q-values convergen â†’ continuar 10 episodios
- Si aÃºn negativo â†’ abandonar SAC

**OPCIÃ“N 3 (ALTERNATIVA SIMPLE): A2C**

```
A2C rendimiento:
  â€¢ Convergencia: +48.8% (vs SAC inestable)
  â€¢ Training: 2.9 min (vs 5-7h)
  â€¢ Estabilidad: Excelente (on-policy)
  â€¢ Complejidad: Menor que SAC
```

---

## ARCHIVOS GENERADOS

- âœ… `result_sac.json` (477 KB) - Metadatos y rewards
- âœ… `timeseries_sac.csv` (7.2 MB) - Timeseries 87,600 steps
- âœ… `trace_sac.csv` (9.9 MB) - Detalles granulares
- âœ… `sac_q_values.png` (95 KB) - GrÃ¡fica Q-values
- âœ… `sac_critic_loss.png` (132 KB) - Critic loss curve
- âœ… `sac_actor_loss.png` (68 KB) - Actor loss curve

---

**Fecha de anÃ¡lisis:** 2026-02-15  
**Analizador:** GitHub Copilot  
**Status:** âœ… ANÃLISIS COMPLETO
