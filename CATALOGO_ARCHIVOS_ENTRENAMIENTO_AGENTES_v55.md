# üìã CATALOGO COMPLETO - ARCHIVOS ENTRENAMIENTO AGENTES (A2C, SAC, PPO)

**Fecha**: 2026-02-13  
**Versi√≥n**: v5.5-COMPLETE  
**Estado**: ‚úÖ TODOS MULTIOBJETIVO SINCRONIZADOS

---

## üéØ VALIDACION GENERAL

| Criterio | A2C | SAC | PPO | Estado |
|----------|-----|-----|-----|--------|
| **Multiobjetivo** | ‚úÖ | ‚úÖ | ‚úÖ | **‚úÖ SINCRONIZADO** |
| **Componentes** | 5 | 5 | 5 | **‚úÖ IGUALES** |
| **Obs Space** | 124 | 124 | 124 | **‚úÖ SINCRONIZADO** |
| **Action Space** | 39 | 39 | 39 | **‚úÖ SINCRONIZADO** |

---

## 1Ô∏è‚É£ AGENTE A2C (Advantage Actor-Critic)

### üìä CONSTANTES & PARAMETROS

```
CO2_FACTOR_IQUITOS    = 0.4521 kg CO2/kWh (grid t√©rmico aislado Iquitos)
BESS_CAPACITY_KWH     = 940.0 kWh
BESS_MAX_POWER_KW     = 342.0 kW
HOURS_PER_YEAR        = 8760 horas
NUM_CHARGERS          = 38 sockets (19 chargers √ó 2)
OBS_DIM               = 124 (observation space)
ACTION_DIM            = 39 (action space: 1 BESS + 38 chargers)
```

### ‚öôÔ∏è HIPERPARAMETROS

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| learning_rate | 7e-4 | Tasa de aprendizaje (A2C on-policy) |
| n_steps | 8 | Horizonte temporal antes de update |
| ent_coef | 0.015 | Coeficiente entrop√≠a (exploraci√≥n) |
| gamma | 0.99 | Discount factor |
| gae_lambda | 0.95 | GAE lambda parameter |
| vf_coef | 0.5 | Value function coefficient |
| max_grad_norm | 1.0 | Gradient clipping |
| net_arch | 256 √ó 256 | Actor-Critic network |

### üì• VARIABLES DE DATOS

| Variable | Tipo | Shape | Descripci√≥n |
|----------|------|-------|-------------|
| solar_hourly | np.ndarray | (8760,) | Generaci√≥n solar horaria (kW) |
| chargers_hourly | np.ndarray | (8760, 38) | Demanda REAL de 38 sockets (kWh) |
| mall_hourly | np.ndarray | (8760,) | Demanda centro comercial (kWh) |
| bess_soc | np.ndarray | (8760,) | BESS State of Charge (%) |
| bess_costs | Dict \| None | variable | Costos acumulados BESS |
| bess_co2 | Dict \| None | variable | CO2 evitado por BESS |

### üéØ METRICAS DE MONITOREO

**Por Episodio:**
- `episode_reward_sum` - Recompensa total acumulada
- `episode_co2_avoided` - CO2 evitado durante episodio (kg)
- `ev_satisfaction` - % veh√≠culos que alcanzaron SOC objetivo
- `validation_episode_reward` - Recompensa en validaci√≥n determin√≠stica
- `validation_success_rate` - Tasa de √©xito en validaci√≥n

**Por Paso:**
- `step_reward` - Recompensa instant√°nea
- `step_action` - Vector de acci√≥n del agente
- `step_observation` - Observaci√≥n del ambiente

### üí∞ RECOMPENSAS MULTIOBJETIVO

**Componentes (5):**
1. ‚úÖ **CO2 emissions (grid)** - Minimizar importaci√≥n grid
2. ‚úÖ **Solar self-consumption** - Autoconsumo PV directo
3. ‚úÖ **EV satisfaction (SOC)** - Alcanzar SOC 90% en veh√≠culos
4. ‚úÖ **Cost minimization** - Minimizar costo tarifa grid
5. ‚úÖ **Grid stability** - Suavizar rampa de potencia

**Contexto Iquitos:**
- Ubicaci√≥n: Iquitos, Peru (zona aislada)
- Grid: Generaci√≥n t√©rmica aislada
- CO2 Factor: 0.4521 kg CO2/kWh

### üíé GANANCIAS (Rewards Positivos)

| Ganancia | Trigger | Magnitud |
|----------|---------|----------|
| Solar self-consumption bonus | Solar > solar_min | +0.5 a +1.0 por unidad |
| EV charging success | SOC veh√≠culo ‚â• objetivo | +0.3 a +0.5 por veh√≠culo |
| CO2 avoided | Importaci√≥n grid reducida | +peso √ó co2_evitado |

### ‚ö†Ô∏è PENALIDADES (Rewards Negativos)

No se encontraron penalidades expl√≠citas en c√≥digo

---

## 2Ô∏è‚É£ AGENTE SAC (Soft Actor-Critic)

### üìä CONSTANTES & PARAMETROS

```
CO2_FACTOR_IQUITOS    = 0.4521 kg CO2/kWh
BESS_CAPACITY_KWH     = 940.0 kWh
BESS_MAX_POWER_KW     = 342.0 kW
HOURS_PER_YEAR        = 8760 horas
NUM_CHARGERS          = 38 sockets v5.2
OBS_DIM               = 124 (observation space)
ACTION_DIM            = 39 (action space)
```

### ‚öôÔ∏è HIPERPARAMETROS

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| learning_rate | 3e-4 | Tasa de aprendizaje (SAC off-policy) |
| buffer_size | 2e6 | Replay buffer size |
| batch_size | 256 | Batch size para training |
| tau | 0.005 | Soft update parameter (target networks) |
| ent_coef | 'auto' | Coeficiente entrop√≠a adaptativo |
| gamma | 0.99 | Discount factor |
| net_arch | 512 √ó 512 | Actor-Critic network |

### üì• VARIABLES DE DATOS

Id√©ntico a A2C:
- solar_hourly, chargers_hourly, mall_hourly, bess_soc, bess_costs, bess_co2

### üéØ METRICAS DE MONITOREO

**Por Episodio:**
- `episode_reward_sum`
- `episode_co2_avoided`
- `solar_self_consumption` 
- `ev_satisfaction`
- `validation_episode_reward`

**Por Paso:**
- `step_reward`, `step_action`, `step_observation`

### üí∞ RECOMPENSAS MULTIOBJETIVO

**Componentes (5):** Id√©nticos a A2C
1. CO2 emissions (grid)
2. Solar self-consumption
3. EV satisfaction (SOC)
4. Cost minimization
5. Grid stability

**Contexto:** Iquitos, Peru | Grid t√©rmico aislado | 0.4521 kg CO2/kWh

### üíé GANANCIAS

| Ganancia | Trigger | Magnitud |
|----------|---------|----------|
| Solar self-consumption bonus | Solar > solar_min | +0.5 a +1.0 |
| EV charging success | SOC ‚â• objetivo | +0.3 a +0.5 |
| CO2 avoided | Grid import reducido | +peso √ó co2_evitado |

### ‚ö†Ô∏è PENALIDADES

| Penalidad | Trigger | Magnitud |
|-----------|---------|----------|
| Low SOC penalty | BESS SOC < 20% | -0.5 por hora |

---

## 3Ô∏è‚É£ AGENTE PPO (Proximal Policy Optimization)

### üìä CONSTANTES & PARAMETROS

```
HOURS_PER_YEAR        = 8760 horas
NUM_CHARGERS          = 38 sockets v5.2
OBS_DIM               = 124 (observation space)
ACTION_DIM            = 39 (action space)
NUM_EPISODES          = 10 episodios entrenamiento
CO2_FACTOR_IQUITOS    = 0.4521 kg CO2/kWh
BESS_CAPACITY_KWH     = 940.0 kWh
BESS_MAX_POWER_KW     = 342.0 kW
```

### ‚öôÔ∏è HIPERPARAMETROS (11)

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| learning_rate | 3e-4 | Tasa de aprendizaje (PPO on-policy) |
| n_steps | 2048 | Horizonte temporal antes de update |
| batch_size | 256 | Batch size para training |
| n_epochs | 20 | Epochs por batch |
| gamma | 0.99 | Discount factor |
| gae_lambda | 0.95 | GAE lambda parameter |
| clip_range | 0.2 | PPO clipping range |
| ent_coef | 0.01 | Coeficiente entrop√≠a |
| vf_coef | 0.5 | Value function coefficient |
| max_grad_norm | 1.0 | Gradient clipping |
| net_arch | 256 √ó 256 | Policy network |

### üì• VARIABLES DE DATOS

Id√©ntico a A2C y SAC:
- solar_hourly, chargers_hourly, mall_hourly, bess_soc, bess_costs, bess_co2

### üéØ METRICAS DE MONITOREO

**Por Episodio:**
- `episode_reward_sum`
- `episode_co2_avoided`
- `ev_satisfaction`
- `validation_episode_reward`
- `validation_success_rate`

**Por Paso:**
- `step_reward`, `step_action`, `step_observation`

### üí∞ RECOMPENSAS MULTIOBJETIVO

**Componentes (5):** Id√©nticos a A2C y SAC
1. CO2 emissions (grid)
2. Solar self-consumption
3. EV satisfaction (SOC)
4. Cost minimization
5. Grid stability

**Contexto:** Iquitos, Peru | Grid t√©rmico aislado | 0.4521 kg CO2/kWh

### üíé GANANCIAS

| Ganancia | Trigger | Magnitud |
|----------|---------|----------|
| Solar self-consumption bonus | Solar > solar_min | +0.5 a +1.0 |
| EV charging success | SOC ‚â• objetivo | +0.3 a +0.5 |
| CO2 avoided | Grid import reducido | +peso √ó co2_evitado |

### ‚ö†Ô∏è PENALIDADES

No se encontraron penalidades expl√≠citas en c√≥digo

---

## üìä COMPARATIVA AGENTES

### Hiperpar√°metros Clave

| Par√°metro | A2C | SAC | PPO |
|-----------|-----|-----|-----|
| **Tipo** | On-policy | Off-policy | On-policy |
| **Learning Rate** | 7e-4 | 3e-4 | 3e-4 |
| **n_steps/horizon** | 8 | ‚àû (replay buffer) | 2048 |
| **Batch Size** | 64 | 256 | 256 |
| **Buffer Size** | - | 2e6 | - |
| **Network** | 256√ó256 | 512√ó512 | 256√ó256 |
| **Tau (soft update)** | - | 0.005 | - |
| **Entropy Coef** | 0.015 | auto | 0.01 |

### Espacios Sincronizados

| Espacio | Dimensi√≥n | Composici√≥n |
|---------|-----------|-------------|
| **Observation** | 124 | Solar(1) + Mall(1) + BESS_SOC(1) + Chargers_demand(38) + Chargers_power(38) + Chargers_occupancy(38) + Time_features(6) + Misc(1) |
| **Action** | 39 | BESS_control(1) + Charger_setpoints(38) |

### Se√±ales de Reward (Multiobjetivo)

Todos los 3 agentes usan **exactamente los mismos 5 componentes**:

```
REWARD = w_co2 √ó R_co2 
       + w_solar √ó R_solar 
       + w_ev √ó R_ev_satisfaction 
       + w_cost √ó R_cost 
       + w_stability √ó R_grid_stability
```

Donde:
- R_co2: Reducci√≥n de importaci√≥n grid vs baseline
- R_solar: % PV directo usado (no exportado)
- R_ev_satisfaction: % veh√≠culos con SOC ‚â• 90%
- R_cost: Tarifa grid (menor es mejor)
- R_grid_stability: Suavidad de rampa de potencia

---

## ‚úÖ VALIDACION FINAL

**Estado: COMPLETAMENTE SINCRONIZADO**

### Checklist

- ‚úÖ **A2C multiobjetivo**: S√ç
- ‚úÖ **SAC multiobjetivo**: S√ç
- ‚úÖ **PPO multiobjetivo**: S√ç
- ‚úÖ **Componentes reward iguales**: 5/5 en los 3 agentes
- ‚úÖ **Observation space sincronizado**: 124-dim en todos
- ‚úÖ **Action space sincronizado**: 39-dim en todos
- ‚úÖ **Datos reales sincronizados**: solar, chargers, mall, BESS en todos
- ‚úÖ **Contexto Iquitos aplicado**: 0.4521 kg CO2/kWh en todos
- ‚úÖ **Ganancias detectadas**: 3 tipos en todos agentes
- ‚úÖ **Penalidades detectadas**: SAC con Low SOC penalty; A2C y PPO sin expl√≠citas

### Archivos Analizados

1. [scripts/train/train_a2c_multiobjetivo.py](scripts/train/train_a2c_multiobjetivo.py) ‚úÖ
2. [scripts/train/train_sac_multiobjetivo.py](scripts/train/train_sac_multiobjetivo.py) ‚úÖ
3. [scripts/train/train_ppo_multiobjetivo.py](scripts/train/train_ppo_multiobjetivo.py) ‚úÖ

### Reporte Detallado

üìä **Reporte JSON**: [reports/oe3/agents_training_catalog_v55.json](reports/oe3/agents_training_catalog_v55.json)

Contiene:
- Variables catalogadas por agente
- M√©tricas de monitoreo por tipo
- Configuraci√≥n multiobjetivo completa
- Validaci√≥n de sincronizaci√≥n
- Timestamp: 2026-02-13
- Versi√≥n: v5.5-COMPLETE

---

## üöÄ LISTO PARA ENTRENAMIENTO

Todos los agentes est√°n:
- ‚úÖ Completamente multiobjetivo
- ‚úÖ Sincronizados en espacios (obs 124, actions 39)
- ‚úÖ Usando datos reales OE2 (solar, chargers, BESS, mall)
- ‚úÖ Con m√©tricas de monitoreo implementadas
- ‚úÖ Con ganancias y penalidades configuradas
- ‚úÖ Listos para iniciar entrenamiento RL

**Pr√≥ximo paso**: Ejecutar scripts de entrenamiento
```bash
python scripts/train/train_sac_multiobjetivo.py      # Off-policy (recomendado para CO2-focus)
python scripts/train/train_ppo_multiobjetivo.py      # On-policy estable
python scripts/train/train_a2c_multiobjetivo.py      # On-policy simple (baseline)
```
