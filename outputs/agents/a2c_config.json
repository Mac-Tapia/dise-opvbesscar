{
  "agent": "A2C",
  "version": "2.0",
  "description": "Advantage Actor-Critic with synchronous updates and separate learning rates",

  "hyperparameters": {
    "train_steps": 500000,
    "n_steps": 2048,
    "learning_rate": 1e-4,
    "lr_schedule": "linear",
    "lr_final_ratio": 0.7
  },

  "a2c_specific": {
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.75,
    "normalize_advantage": true
  },

  "separate_learning_rates": {
    "actor_learning_rate": 1e-4,
    "critic_learning_rate": 1e-4,
    "actor_lr_schedule": "linear",
    "critic_lr_schedule": "linear",
    "actor_lr_final_ratio": 0.7,
    "critic_lr_final_ratio": 0.7
  },

  "entropy_schedule": {
    "type": "exponential",
    "initial_value": 0.01,
    "final_value": 0.001,
    "decay_rate": 0.998
  },

  "ev_utilization_bonus": {
    "enabled": true,
    "weight": 0.05,
    "optimal_soc_min": 0.70,
    "optimal_soc_max": 0.90,
    "overcharge_threshold": 0.95,
    "decay_rate": 0.98
  },

  "network": {
    "hidden_sizes": [256, 256],
    "activation": "relu"
  },

  "advanced": {
    "use_huber_loss": true,
    "huber_delta": 1.0,
    "optimizer_type": "adam",
    "use_amp": true
  },

  "normalization": {
    "normalize_observations": true,
    "normalize_rewards": true,
    "reward_scale": 0.1,
    "clip_obs": 5.0,
    "clip_reward": 1.0
  },

  "device": {
    "type": "auto",
    "use_amp": true,
    "pin_memory": true,
    "deterministic_cuda": false
  },

  "checkpoint": {
    "save_dir": "outputs/checkpoints/A2C",
    "save_frequency": 1000,
    "save_final": true,
    "resume_path": null
  },

  "logging": {
    "verbose": 1,
    "log_interval": 500,
    "progress_path": "outputs/agents/a2c_progress.csv",
    "progress_interval_episodes": 1
  },

  "reproducibility": {
    "seed": 42,
    "deterministic_cuda": false
  },

  "performance_targets": {
    "expected_co2_reduction_percent": 24,
    "expected_solar_utilization_percent": 60,
    "expected_training_hours_rtx4060": 4
  },

  "best_practices": {
    "synchronous_updates": "Collect n_steps=2048 before each policy update",
    "separate_networks": "Actor and critic networks share backbone but have separate heads",
    "separate_learning_rates": "Actor and critic can have different learning rates",
    "advantage_normalization": "Normalize advantages within each mini-batch",
    "entropy_decay": "Gradually reduce exploration with exponential decay",
    "ev_utilization": "Reward maximum simultaneous EV charging with SOC bonus",
    "huber_loss": "More robust value function training than MSE"
  }
}
