{
  "agent": "SAC",
  "version": "2.0",
  "description": "Soft Actor-Critic with entropy regularization and off-policy learning",

  "hyperparameters": {
    "episodes": 5,
    "total_timesteps": 43800,
    "learning_rate": 5e-5,
    "buffer_size": 200000,
    "batch_size": 256,
    "gamma": 0.995,
    "tau": 0.02
  },

  "entropy": {
    "ent_coef": "auto",
    "ent_coef_init": 0.5,
    "ent_coef_lr": 1e-3,
    "target_entropy": "auto",
    "ent_coef_min": 0.01,
    "ent_coef_max": 1.0,
    "ent_decay_rate": 0.9995
  },

  "network": {
    "hidden_sizes": [256, 256],
    "activation": "relu",
    "output_activation": "tanh"
  },

  "stability": {
    "clip_gradients": true,
    "max_grad_norm": 10.0,
    "critic_max_grad_norm": 1.0,
    "critic_loss_scale": 0.1,
    "q_target_clip": 10.0,
    "q_value_clip": 10.0,
    "gradient_accumulation_steps": 1
  },

  "normalization": {
    "normalize_observations": true,
    "normalize_rewards": false,
    "reward_scale": 1.0,
    "clip_obs": 10.0,
    "clip_reward": 10.0
  },

  "device": {
    "type": "auto",
    "use_amp": true,
    "pin_memory": true,
    "num_workers": 0
  },

  "checkpoint": {
    "save_dir": "outputs/checkpoints/SAC",
    "save_frequency": 1000,
    "save_final": true,
    "resume_path": null
  },

  "logging": {
    "verbose": 1,
    "log_interval": 500,
    "progress_path": "outputs/agents/sac_progress.csv",
    "progress_interval_episodes": 1
  },

  "reproducibility": {
    "seed": 42,
    "deterministic_cuda": false
  },

  "performance_targets": {
    "expected_co2_reduction_percent": 26,
    "expected_solar_utilization_percent": 65,
    "expected_training_hours_rtx4060": 6
  },

  "best_practices": {
    "off_policy": "Use replay buffer for sample efficiency",
    "entropy_regularization": "Auto-tune entropy for exploration/exploitation balance",
    "gradient_clipping": "Prevent divergence in critic network",
    "separate_critic_clipping": "More aggressive clipping for critic (1.0 vs actor 10.0)",
    "warmup_steps": 1000,
    "kl_monitoring": "Track policy divergence from initial policy"
  }
}
