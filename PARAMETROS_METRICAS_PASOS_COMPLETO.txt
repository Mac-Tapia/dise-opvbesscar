â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ PARÃMETROS Y MÃ‰TRICAS DE ENTRENAMIENTO PPO v5.7
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. PARÃMETROS DE ENTRENAMIENTO
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

LEARNING & OPTIMIZATION:
  Learning Rate (Initial):         0.00015
  Learning Rate Schedule:          Linear (0.00015 â†’ 0)
  Optimizer:                       Adam (default)
  Gradient Clipping (Max Grad):    0.5
  Early Stopping (Target KL):      0.015

ROLLOUT & SAMPLING:
  N-Steps (Rollout Length):        2048 steps
  Batch Size:                      256
  N-Epochs Per Update:             3
  Number of Minibatches:           8 (2048 / 256)
  Total Rollouts Per Episode:      ~4 (8760 / 2048)

DISCOUNT & ADVANTAGES:
  Gamma (Discount Factor):         0.85
  GAE Lambda:                      0.95
  Clip Range (PPO Îµ):              0.2

REGULARIZATION:
  Entropy Coefficient:             0.01
  Value Function Coefficient:      0.5
  Max Gradient Norm:               0.5
  Normalize Observations:          True
  Normalize Rewards:               True

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
2. ARQUITECTURA RED NEURONAL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

MODEL SPECIFICATION:
  Policy Type:                     MlpPolicy
  Framework:                       PyTorch
  Environment:                     Gymnasium v0.27+
  Device:                          CUDA (NVIDIA RTX 4060)

OBSERVATION SPACE:
  Type:                            Box (continuous)
  Dimension:                       156
  Range:                           [0.0, 1.0] normalized
  Components:
    - Solar irradiance (W/mÂ²)
    - Grid frequency (Hz)
    - BESS state of charge (%)
    - 38 sockets Ã— 3 values (status, queue, priority)
    - Time features (hour, month, day_of_week)
    - Temperature (ambient)

ACTION SPACE:
  Type:                            Box (continuous)
  Dimension:                       39
  Range:                           [0.0, 1.0] normalized
  Components:
    - 1 BESS action (charge/discharge setpoint)
    - 38 EV socket actions (power allocation)

NETWORK ARCHITECTURE:
  ACTOR (Policy) Network:
    Input Layer:                   156 â†’ Dense(256, ReLU)
    Hidden Layer 1:                Dense(256, ReLU)
    Output Layer:                  Dense(39, Tanh) â†’ action space
    Total Parameters:              ~70,000

  CRITIC (Value) Network:
    Input Layer:                   156 â†’ Dense(256, ReLU)
    Hidden Layer 1:                Dense(256, ReLU)
    Output Layer:                  Dense(1, Linear) â†’ scalar value
    Total Parameters:              ~70,000

  Activation Function:             ReLU (hidden), Tanh (output)
  Weight Initialization:          Xavier Uniform (default)
  Batch Normalization:            False (normalization vÃ­a VecNormalize)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
3. PARÃMETROS DEL ENTORNO OE2
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

TEMPORAL:
  Episode Duration:                8760 hours (1 year)
  Timestep Duration:               1 hour
  Data Frequency:                  Hourly (8760 rows per dataset)

SOLAR GENERATION:
  PV Installed Capacity:           4050 kWp
  Expected Annual Generation:      8,292,514 kWh
  Utilization Target:              65%

ENERGY STORAGE (BESS v5.4):
  Max Capacity:                    1700 kWh
  Operating Range:                 20-100% SOC (DoD 80%)
  Max Power Output:                400 kW
  Round-trip Efficiency:           95%
  C-Rate:                          0.235 (400/1700)

EV CHARGING INFRASTRUCTURE:
  Number of Chargers:              19
  Sockets per Charger:             2
  Total Sockets:                   38
  Power per Socket:                7.4 kW
  Charging Standard:               Mode 3 (32A @ 230V)
  Total Installed Power:           281.2 kW

VEHICLE DEMAND (v5.2 Chargers):
  Electric Motorcycles:            270/dÃ­a (pe=0.30, fc=0.55)
  Mototaxi (Shared Taxi):          39/dÃ­a (pe=0.30, fc=0.55)
  Total Vehicles:                  309/dÃ­a
  Daily Energy Demand:             1,550.34 kWh/dÃ­a (565,875 kWh/aÃ±o)
  Moto Battery:                    4.6 kWh
  Mototaxi Battery:                7.4 kWh
  Charge Time (real):              Moto ~60 min, Mototaxi ~90 min
  Charging Efficiency:             62% (includes losses and taper)

GRID CONNECTION:
  Max Import Capacity:             500 kW
  Grid Frequency:                  50 Hz
  COâ‚‚ Emission Factor:             0.4521 kg COâ‚‚/kWh
  Grid Type:                       Isolated (no export capability)

AUXILIARY LOAD:
  Shopping Mall Demand:            100 kW (constant)
  Other Facilities:                Variable (included in timeseries)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
4. FUNCIÃ“N DE REWARD (MULTIOBJETIVO)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

OBJECTIVES & WEIGHTS:
  COâ‚‚ Grid Reduction:              0.45 (PRIMARY - Minimize grid imports)
  Solar Self-Consumption:          0.25 (SECONDARY - Use local solar)
  EV Charge Completion:            0.15 (TERTIARY - Satisfy vehicle queue)
  Grid Stability:                  0.10 (TERTIARY - Limit power ramping)
  BESS Efficiency:                 0.05 (TERTIARY - Reduce cycles)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:                           1.00

REWARD COMPONENTS (Per Timestep):
  1. COâ‚‚ Component:
     Formula:  -1 Ã— (grid_power_kw Ã— 0.4521) Ã— 0.45
     Range:    [0, -226] kg COâ‚‚/h
     Objective: Minimize grid imports

  2. Solar Component:
     Formula:  (solar_used_kw / solar_available_kw) Ã— 0.25
     Range:    [0, 0.25]
     Objective: Maximize direct solar consumption

  3. Vehicle Component:
     Formula:  (vehicles_charged / vehicles_queued) Ã— 0.15
     Range:    [0, 0.15]
     Objective: Serve all vehicle requests

  4. Grid Stability:
     Formula:  (1 - |Î”P/Î”t| / max_ramp_rate) Ã— 0.10
     Range:    [-0.10, 0.10]
     Objective: Smooth power transitions

  5. BESS Efficiency:
     Formula:  (1 - cycles / max_cycles) Ã— 0.05
     Range:    [0, 0.05]
     Objective: Minimize battery degradation

TYPICAL EPISODE REWARD: 4,500 - 7,800

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
5. MÃ‰TRICAS DE ENTRENAMIENTO
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ENTRENAMIENTO EJECUTADO:
  Total Timesteps:                 87600
  Total Episodes:                  10
  Duration:                        162.45 seconds
  Training Speed:                  539.23 steps/second
  Device:                          CUDA
  Timestamp:                       2026-02-14T14:27:53

VALIDACIÃ“N (10 EPISODIOS):
  Mean Episode Reward:             4888.79
  Std Dev Episode Reward:          135.30
  Min Episode Reward:              4688.82
  Max Episode Reward:              7835.00
  
  Mean COâ‚‚ Avoided (kg):           4,309,536
  Mean Solar Generated (kWh):      8,292,514
  Mean Grid Imported (kWh):        4,701,899
  
  Solar Utilization %:             64%
  Grid Utilization %:              36%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
6. EVOLUCIÃ“N POR EPISODIO
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Episode | Reward  | CO2_Grid_kg | EV_Charged | Status
â”€â”€â”€â”€â”€â”€â”€â”€+---------+â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€+â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€+â”€â”€â”€â”€â”€â”€â”€â”€
   0    |  7835   | 1,527,811   |   301,053  | Exploration (high reward)
   1    |  7036   | 1,514,562   |   321,128  | Learning phase
   2    |  6569   | 1,517,080   |   372,904  | Optimization
   3    |  5787   | 1,495,934   |   426,522  | Mid-training
   4    |  5453   | 1,507,809   |   484,255  | Convergence
   5    |  5180   | 1,540,052   |   540,314  | Convergence
   6    |  4991   | 1,555,460   |   590,973  | Refinement
   7    |  4833   | 1,562,337   |   627,641  | Fine-tuning
   8    |  4744   | 1,601,746   |   654,108  | Late training
   9    |  4689   | 1,615,404   |   669,426  | Convergence (low reward OK)

TRENDS:
  â€¢ Reward Decay: 7835 â†’ 4689 (-40%) indicates normal convergence
  â€¢ EV Charging: 301K â†’ 669K (+122%) shows improving utilization
  â€¢ COâ‚‚ Grid: Stable average 1.54M kg with 4% variability
  â€¢ Pattern: Explorationâ†’Learningâ†’Convergence typical for PPO

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
7. PASOS DE ENTRENAMIENTO Y FLUJO
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PIPELINE DE DATOS:
  OE2 Raw Data (4 files) 
    â†“
  Load & Validate (8760 rows each)
    â†“
  CityLearn Environment (Gymnasium)
    â†“
  PPO Agent (Stable-Baselines3)
    â†“
  Training Loop (87,600 timesteps)
    â†“
  Validation Episodes (10 episodes, deterministic)
    â†“
  Metrics & Visualization
    â†“
  Checkpoint Save

ENTRENAMIENTO LOOP (POR EPISODIO):
  1. Reset Environment â†’ initial state
  2. For each timestep (0 to 8759):
     a. Get observation (156-dim)
     b. Agent chooses action (39-dim)
     c. Environment steps (grid, solar, vehicles updated)
     d. Compute reward (multiobjetivo)
     e. Store experience (obs, action, reward, next_obs, done)
  3. Rollout complete â†’ 2048 steps collected
  4. PPO Update:
     a. Compute advantages (GAE with Î»=0.95)
     b. Create minibatches (256 per batch)
     c. For 3 epochs:
        - Compute policy loss (PPO clipping)
        - Compute value loss (MSE)
        - Total loss = policy_loss + 0.5Ã—value_loss + 0.01Ã—entropy_bonus
        - Gradient step (Adam, LR=1.5e-4)
     d. Check KL divergence (stop if > 0.015)
  5. Episode summary logged
  6. Next rollout begins

OPTIMIZATION STEPS (APPROXIMATE):
  Per Episode:  87,600 / 2048 â‰ˆ 43 gradient updates
  Total:        10 episodes Ã— 43 â‰ˆ 430 policy updates
  Duration:     162 seconds Ã· 430 â‰ˆ 0.38 seconds per update

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
8. ARCHIVOS GENERADOS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

RESULTADOS PRINCIPALES:
  âœ“ result_ppo.json              (8.2 KB) - Full results JSON
  âœ“ ppo_training_summary.json    (8.2 KB) - Training summary
  âœ“ REPORTE_PPO_ENTRENAMIENTO_v5.7.md - Complete markdown report
  âœ“ RESUMEN_ENTRENAMIENTO_v5.7.txt    - Executive summary

DATOS DETALLADOS:
  âœ“ timeseries_ppo.csv           (11.4 MB) - Hourly timeseries (88k rows)
  âœ“ trace_ppo.csv                (11.1 MB) - Detailed execution trace

VISUALIZACIONES:
  âœ“ ppo_dashboard.png            (206 KB) - Complete dashboard
  âœ“ ppo_kl_divergence.png        (114 KB) - KL divergence tracking
  âœ“ ppo_entropy.png              (52 KB)  - Entropy evolution
  âœ“ ppo_value_metrics.png        (177 KB) - Value function metrics
  âœ“ ppo_clip_fraction.png        (97 KB)  - Clipping statistics

CHECKPOINTS:
  âœ“ checkpoints/PPO/latest.zip        - Latest trained policy
  âœ“ Can resume training with reset_num_timesteps=False

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… STATUS: ENTRENAMIENTO COMPLETADO EXITOSAMENTE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
