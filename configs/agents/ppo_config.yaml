ppo:
  name: "Proximal Policy Optimization"
  description: "On-policy agent with clipped objective"

  # Training hyperparameters
  training:
    train_steps: 500000
    n_steps: 2048  # OPCIÓN A: Mantener para ratio óptimo (~8 mini-batches)
    batch_size: 256  # GPU optimized (era 128)
    n_epochs: 10
    learning_rate: 2e-4  # OPCIÓN A: Reducido 33% para GPU (batch_size=256, n_steps=2048)
    lr_schedule: "linear"
    lr_final_ratio: 0.5

  # PPO-specific
  ppo:
    gamma: 0.99
    gae_lambda: 0.98
    clip_range: 0.2
    clip_range_vf: 0.5
    normalize_advantage: true

  # Entropy and value function
  losses:
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 1.0

  # Entropy decay schedule
  entropy_schedule:
    type: "exponential"  # "constant", "linear", or "exponential"
    final_value: 0.001
    decay_rate: 0.999

  # Network architecture
  network:
    hidden_sizes: [256, 256]
    activation: "relu"
    ortho_init: true

  # Normalizations
  normalization:
    normalize_observations: true
    normalize_rewards: true
    reward_scale: 0.1
    clip_obs: 5.0
    clip_reward: 1.0

  # Advanced features
  advanced:
    use_sde: true  # State-dependent exploration
    target_kl: 0.02
    kl_adaptive: true
    kl_adaptive_down: 0.5
    kl_adaptive_up: 1.05

  # Multi-Objective Weights (ACTUALIZADOS 2026-02-07)
  multi_objective_weights:
    co2: 0.35
    solar: 0.20
    ev: 0.30
    cost: 0.10
    grid: 0.05

  # Device
  device: "auto"  # "cuda", "mps", "cpu", or "auto"
  use_amp: true

  # Logging
  verbose: 1
  log_interval: 500
  checkpoint_freq_steps: 1000
  save_final: true

  # Reproducibility
  seed: 42

performance:
  expected_co2_reduction: 0.29  # 29% reduction vs baseline
  expected_solar_utilization: 0.68  # 68% utilization
  training_time_hours: 5  # Approximate on RTX 4060
