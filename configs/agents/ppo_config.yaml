ppo:
  name: Proximal Policy Optimization
  description: On-policy agent with clipped objective
  training:
    train_steps: 500000
    n_steps: 2048
    batch_size: 256
    n_epochs: 10
    learning_rate: 2e-4
    lr_schedule: linear
    lr_final_ratio: 0.5
  ppo:
    gamma: 0.99
    gae_lambda: 0.98
    clip_range: 0.2
    clip_range_vf: 0.5
    normalize_advantage: true
  losses:
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 1.0
  entropy_schedule:
    type: exponential
    final_value: 0.001
    decay_rate: 0.999
  network:
    hidden_sizes:
    - 256
    - 256
    activation: relu
    ortho_init: true
  normalization:
    normalize_observations: true
    normalize_rewards: true
    reward_scale: 0.1
    clip_obs: 5.0
    clip_reward: 1.0
  advanced:
    use_sde: true
    target_kl: 0.02
    kl_adaptive: true
    kl_adaptive_down: 0.5
    kl_adaptive_up: 1.05
  multi_objective_weights:
    co2: 0.5
    solar: 0.2
    ev: 0.15
    grid: 0.1
    cost: 0.05
  tariffs_osinergmin_usd_per_kwh:
    generation_solar: 0.1
    storage_bess: 0.06
    distribution_ev_charge: 0.12
    integrated_tariff: 0.28
  device: auto
  use_amp: true
  verbose: 1
  log_interval: 500
  checkpoint_freq_steps: 1000
  save_final: true
  seed: 42
performance:
  expected_co2_reduction: 0.808
  expected_solar_utilization: 0.798
  expected_ev_satisfaction: 1.0
  expected_bess_soc_avg: 0.85
  motos_per_day_avg: 270
  mototaxis_per_day_avg: 39
  annual_ev_demand_kwh: 352887
  cost_savings_usd: 30530
  training_time_hours: 5
