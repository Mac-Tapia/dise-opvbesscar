agents:
  sac:
    name: "Soft Actor-Critic"
    type: "off-policy"
    enabled: true

  ppo:
    name: "Proximal Policy Optimization"
    type: "on-policy"
    enabled: true

  a2c:
    name: "Advantage Actor-Critic"
    type: "on-policy"
    enabled: true

training:
  episodes: 3
  max_timesteps: 26280  # 3 complete years × 8,760 hours (required for seasonal pattern learning)
  evaluation_frequency: 1
  checkpoint_frequency: 1000

environment:
  episode_length: 8760  # 1 year in hours
  time_step_minutes: 60

# Reward weights (2026-02-07 - UNIFIED NOMENCLATURE)
reward_weights:
  co2: 0.35              # Primary: CO₂ grid minimization
  solar: 0.20            # Secondary: Solar self-consumption
  ev: 0.30               # PRIMARY PRIORITY: EV satisfaction (charge completion)
  cost: 0.10             # Tertiary: Cost optimization
  grid: 0.05             # Tertiary: Grid stability
  # NOTE: Total = 1.00 (sum verified: 0.35+0.20+0.30+0.10+0.05)

# Infrastructure
infrastructure:
  solar_capacity_kwp: 4050
  bess_capacity_kwh: 4520
  bess_power_kw: 600
  num_chargers: 128
  charger_power_kw_per_socket: 10.0

# CO2 context
co2_context:
  region: "Iquitos"
  country: "Peru"
  grid_co2_intensity: 0.4521  # kg CO2/kWh

# Data paths
data:
  solar: "data/interim/oe2/solar/pv_generation_timeseries.csv"
  chargers: "data/interim/oe2/chargers/individual_chargers.json"
  schema: "data/interim/oe3/schema.json"

outputs:
  checkpoints_dir: "outputs/checkpoints"
  agents_dir: "outputs/agents"
  results_dir: "outputs/results"
