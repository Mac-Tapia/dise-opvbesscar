a2c:
  name: Advantage Actor-Critic
  description: Synchronous on-policy agent with shared network
  training:
    episodes: 10
    train_steps: 87600
    n_steps: 8
    learning_rate: 7e-4
    lr_schedule: linear
    lr_final_ratio: 0.7
  a2c:
    gamma: 0.99
    gae_lambda: 0.95
    ent_coef: 0.015
    vf_coef: 0.5
    max_grad_norm: 0.75
    normalize_advantage: true
  separate_learning_rates:
    actor_learning_rate: 1e-4
    critic_learning_rate: 1e-4
    actor_lr_schedule: linear
    critic_lr_schedule: linear
    actor_lr_final_ratio: 0.7
    critic_lr_final_ratio: 0.7
  entropy_schedule:
    type: exponential
    initial_value: 0.01
    final_value: 0.001
    decay_rate: 0.998
  ev_utilization:
    enabled: true
    weight: 0.05
    optimal_soc_min: 0.7
    optimal_soc_max: 0.9
    overcharge_threshold: 0.95
    decay_rate: 0.98
  multi_objective_weights:
    co2: 0.35
    cost: 0.1
    solar: 0.2
    ev: 0.3
    grid: 0.05
  tariffs_osinergmin_usd_per_kwh:
    generation_solar: 0.1
    storage_bess: 0.06
    distribution_ev_charge: 0.12
    integrated_tariff: 0.28
  network:
    hidden_sizes:
    - 256
    - 256
    activation: relu
  advanced:
    use_huber_loss: true
    huber_delta: 1.0
    optimizer_type: adam
    use_amp: true
  normalization:
    normalize_observations: true
    normalize_rewards: true
    reward_scale: 0.1
    clip_obs: 5.0
    clip_reward: 1.0
  device: auto
  deterministic_cuda: false
  verbose: 1
  log_interval: 500
  checkpoint_freq_steps: 1000
  save_final: true
  seed: 42
performance:
  expected_co2_reduction: 0.808
  expected_solar_utilization: 0.798
  expected_ev_satisfaction: 1.0
  expected_bess_soc_avg: 0.85
  motos_per_day_avg: 270
  mototaxis_per_day_avg: 39
  annual_ev_demand_kwh: 352887
  cost_savings_usd: 30530
  training_time_hours: 4
tariff:
  hp_soles_kwh: 0.45
  hfp_soles_kwh: 0.28
  difference_soles_kwh: 0.17
  factor_hp_hfp: 1.607
rewards:
  hp_weight: 0.4
  hfp_weight: 0.15
