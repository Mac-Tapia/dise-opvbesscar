sac:
  name: "Soft Actor-Critic"
  description: "Off-policy actor-critic agent with entropy regularization"

  # Training hyperparameters
  training:
    episodes: 3
    total_timesteps: 26280  # 3 complete years × 8,760 hours for seasonal pattern learning
    learning_rate: 2e-4  # OPCIÓN A: Reducido 33% para GPU (batch_size=128)
    buffer_size: 2000000  # Aumentado para GPU (era 200000)
    batch_size: 128  # GPU optimized (era 256, ajustado para estabilidad)
    gamma: 0.995
    tau: 0.02

  # Entropy (exploration)
  entropy:
    ent_coef: "auto"
    ent_coef_init: 0.5
    ent_coef_lr: 1e-3
    ent_coef_min: 0.01
    ent_coef_max: 1.0

  # Network architecture
  network:
    hidden_sizes: [256, 256]
    activation: "relu"

  # Stability features
  stability:
    clip_gradients: true
    max_grad_norm: 10.0
    critic_max_grad_norm: 1.0
    critic_loss_scale: 0.1
    q_target_clip: 10.0

  # Device
  device: "auto"  # "cuda", "mps", "cpu", or "auto"
  use_amp: true

  # Logging
  verbose: 1
  log_interval: 500
  checkpoint_freq_steps: 1000
  save_final: true

  # Reproducibility
  seed: 42

performance:
  expected_co2_reduction: 0.26  # 26% reduction vs baseline
  expected_solar_utilization: 0.65  # 65% utilization
  training_time_hours: 6.5  # 5-7 hours on RTX 4060 (26,280 timesteps ≈ 3 years of hourly data)
