# üî¥ SAC DIVERGENCIA: DIAGN√ìSTICO RA√çZ

## El Colapso en 1 Gr√°fico

```
OPTIMAL POLICY (PPO):                SAC POLICY (COLLAPSED):
‚îú‚îÄ Grid: 7.2M kWh ‚úÖ                ‚îî‚îÄ Grid: 13.2M kWh ‚ùå (2.3x peor)
‚îú‚îÄ PV Utilization: 100% ‚úÖ           ‚îî‚îÄ PV Utilization: 0.1% ‚ùå
‚îú‚îÄ EV Charging: 1.5M kWh ‚úÖ          ‚îî‚îÄ EV Charging: 0 kWh ‚ùå
‚îî‚îÄ CO‚ÇÇ Reduction: -23% ‚úÖ            ‚îî‚îÄ CO‚ÇÇ Increase: +126% ‚ùå

¬øQU√â PAS√ì?
Red neuronal aprendi√≥ pol√≠tica INVERSA: "Ignore solar, always max grid"
```

---

## üîç Las 4 Causas (En Orden de Criticidad)

### 1Ô∏è‚É£ CLIP_OBS = 5.0 ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **CAUSA RA√çZ PRIMARIA**

**Lo que pas√≥**:
```python
Observaci√≥n: Grid=13,200,000 kWh
             ‚Üì [Prescale por 0.001]
Prescaled:   13,200 kW
             ‚Üì [Normalizaci√≥n con stats malas - Episode 1]
Normalized:  ??? (stats no entrenadas)
             ‚Üì [CLIP A [-5.0, +5.0]] ‚¨ÖÔ∏è AQU√ç MUERE LA INFORMACI√ìN
Clipped:     5.0

Grid=13.2M ‚Üí [5.0]
Grid=10M   ‚Üí [5.0]
Grid=8M    ‚Üí [5.0]
Grid=6M    ‚Üí [5.0]

RED NEURONAL VE: [5, 5, 5, 5, ...] ‚Üí TODAS LAS OBSERVACIONES ID√âNTICAS
```

**Consecuencia**:
- Network no puede aprender: "Grid alto = diferente de Grid bajo"
- Colapsa a pol√≠tica aleatoria o por defecto
- Se queda en "siempre maximizar grid" (es local optimal)

**Soluci√≥n**: `clip_obs = 100.0` (permite valores post-normalizados sin destruir informaci√≥n)

---

### 2Ô∏è‚É£ ENT_COEF_INIT = 0.1 ‚ö†Ô∏è‚ö†Ô∏è **EXPLORACI√ìN INSUFICIENTE**

**Lo que pas√≥**:
```
Entrop√≠a = 0.1 significa: "Explora s√≥lo 10% del tiempo"

Episode 1: Pol√≠tica aleatoria (OK)
           Algunos acciones = "maximizar grid"
           Algunas acciones = "usar solar"
           Todas tienen rewards negativos (est√° aprendiendo)

Episode 2: Policy empieza a enfocarse
           "Maximizar grid" obtiene mejor reward en algunos timesteps
           (Porque climatiza el mall, aunque sea sub√≥ptimo)
           Entrop√≠a baja (0.1) = NO EXPLORA otras opciones

Episode 3: Policy converged
           "Siempre maximizar grid" ‚Üí LOCKED IN
           Entrop√≠a baja ‚Üí NO HAY ESCAPE
           Diverged
```

**Soluci√≥n**: `ent_coef_init = 0.5` (explora 50% del tiempo ‚Üí m√°s posibilidad de descubrir solar)

---

### 3Ô∏è‚É£ ENT_COEF_LR = 1e-5 ‚ö†Ô∏è‚ö†Ô∏è **ADAPTACI√ìN MUY LENTA**

**Lo que pas√≥**:
```
Cambio de entrop√≠a por timestep: 1e-5
Cambio por episodio (8,760 pasos): 8,760 √ó 1e-5 = 0.087
Cambio por 3 episodios: 3 √ó 0.087 = 0.26

Entrop√≠a progresa: 0.1 ‚Üí 0.1 ‚Üí 0.1 ‚Üí 0.1 ‚Üí ... (casi nada)
(Esperado: 0.1 ‚Üí 0.3 ‚Üí 0.5 ‚Üí ... para adaptarse)
```

**Consecuencia**:
- SAC no detecta: "Necesito m√°s exploraci√≥n para aprender solar control"
- Entrop√≠a queda congelada en 0.1 = stuck with insufficient exploration
- Network nunca descubre la pol√≠tica √≥ptima

**Soluci√≥n**: `ent_coef_lr = 1e-3` (200x m√°s r√°pido, adapta por-episodio)

---

### 4Ô∏è‚É£ MAX_GRAD_NORM = 0.5 ‚ö†Ô∏è **GRADIENTES BLOQUEADOS**

**Lo que pas√≥**:
```
SAC es off-policy:
‚îú‚îÄ Aprende de experiencias viejas (del replay buffer)
‚îú‚îÄ TD errors grandes ‚Üí Gradientes naturalmente grandes
‚îî‚îÄ Necesita max_grad_norm = 5-20

Configuraci√≥n:
‚îú‚îÄ max_grad_norm = 0.5 (para on-policy like PPO)
‚îú‚îÄ learning_rate = 5e-5 (bajo)
‚îú‚îÄ Combinado: Actualizaciones microsc√≥picas (~1e-6 por paso)
‚îî‚îÄ Network stuck en random initialization
```

**Consecuencia**:
- Policy casi no cambia (gradientes clippeados + lr bajo)
- Network no puede escapar de random initialization
- Local minimum de "siempre grid" parece bueno vs "no learning"

**Soluci√≥n**: `max_grad_norm = 10.0` (permite gradientes SAC naturales)

---

## üìä Tabla de Fixes Aplicados

| L√≠nea | Par√°metro | Anterior | Nuevo | Raz√≥n |
|-------|-----------|----------|-------|-------|
| 479 | `clip_obs` | 5.0 | 100.0 | ‚≠ê‚≠ê‚≠ê Cr√≠tico - Permite network ver diferencias |
| 153 | `ent_coef_init` | 0.1 | 0.5 | ‚≠ê‚≠ê‚≠ê Cr√≠tico - Exploraci√≥n suficiente |
| 154 | `ent_coef_lr` | 1e-5 | 1e-3 | ‚≠ê‚≠ê Alto - Adaptaci√≥n por-episodio |
| 161 | `max_grad_norm` | 0.5 | 10.0 | ‚≠ê‚≠ê Alto - Updates no bloqueados |

---

## ‚úÖ Status

**Diagn√≥stico**: COMPLETO
**Causas Identificadas**: 4 (documentadas en archivos de an√°lisis)
**Fixes Aplicados**: Todos 4 (en `sac.py`)
**Pr√≥ximo Paso**: Test run con 5 episodios ‚Üí Verificar convergencia normal

---

## üß™ C√≥mo Verificar que Funcion√≥

```bash
# Run 5-episode SAC test
python -m scripts.run_oe3_simulate --config configs/default.yaml --agents=sac

# Check results
python -m scripts.run_oe3_co2_table --config configs/default.yaml

# Expected:
# - SAC Grid Import: 7-8M kWh (vs broken 13.2M)
# - SAC PV Utilization: >80% (vs broken 0.1%)
# - SAC CO‚ÇÇ Reduction: -20 to -25% (vs broken -126%)
```

Si SAC devuelve esos n√∫meros ‚Üí ‚úÖ Fixes funcionaron
Si sigue con 13.2M grid ‚Üí ‚ùå Problema m√°s profundo (network architecture o reward function)

---

## üìù Iron√≠a: "Critical Fixes" que Causaron el Problema

Los comentarios "CRITICAL FIX" en el c√≥digo fueron INTENTOS de prevenir divergencia:

```python
# L√≠nea 153 - COMENTARIO ORIGINAL:
# "üî¥ CRITICAL FIX: 0.5‚Üí0.1 (prevent entropy explosion)"
# ‚ùå RESULTADO: Previno exploraci√≥n, caus√≥ convergencia local

# L√≠nea 161 - COMENTARIO ORIGINAL:
# "üî¥ CRITICAL FIX: 1.0‚Üí0.5 (stricter gradient clipping)"
# ‚ùå RESULTADO: Bloque√≥ gradientes, network no aprendi√≥ nada

# L√≠nea 479 - COMENTARIO ORIGINAL:
# "Clipping m√°s agresivo"
# ‚ùå RESULTADO: Destruy√≥ informaci√≥n, network vio datos id√©nticos
```

**Lecci√≥n**: Esos "fixes" eran apropiados para **otros tipos de problemas** (image RL, instabilidad num√©rica), pero en energ√≠a + CityLearn causaron el opuesto: **convergencia a pol√≠tica peor**.

---

## üìã Documentaci√≥n Generada

1. **`DIAGNOSTICO_SAC_DIVERGENCIA_2026_02_02.md`** - An√°lisis t√©cnico profundo (3,000+ palabras)
2. **`RESUMEN_CAUSAS_SAC_Y_FIXES.md`** - Detalle de cada fix con ejemplos (1,500+ palabras)
3. **Este archivo** - Resumen ejecutivo (quick reference)

Todos en: `d:\dise√±opvbesscar\`
