# üîç AUDITOR√çA COMPARATIVA: SAC vs PPO vs A2C - CRITICAL ISSUES IDENTIFIED

## Executive Summary

**Resultado:** Se han identificado **8 ISSUES CR√çTICOS** en PPO y A2C que pueden causar los mismos problemas que en SAC (divergencia, undertraining, instabilidad).

**Status:** Ready to apply coordinated fixes

---

## CRITICAL ISSUES FOUND

### Issue #1: ‚ùå MISSING DATASET VALIDATION IN PPO.learn()
**SAC Status:** ‚úÖ Implementado (_validate_dataset_completeness)
**PPO Status:** ‚ùå S√ç EXISTE (l√≠nea ~250-280) - **PERO NUNCA SE LLAMA EN learn()**
**A2C Status:** ‚ùå S√ç EXISTE (l√≠nea ~220-250) - **PERO NUNCA SE LLAMA EN learn()**

**Problem:** M√©todos learn() no llaman a _validate_dataset_completeness()
**Impact:** Entrenamiento puede ejecutar con datos corruptos sin avisar
**Fix:** Agregar validaci√≥n al inicio de learn()

```python
# DEBE ESTAR EN learn() (l√≠nea ~300+):
def learn(self, total_timesteps: Optional[int] = None, **kwargs):
    # VALIDACI√ìN CR√çTICA: Verificar dataset completo antes de entrenar
    self._validate_dataset_completeness()  # ‚Üê FALTA ESTO
```

---

### Issue #2: ‚ùå MISSING TORCH SETUP IN A2C
**SAC Status:** ‚úÖ _setup_torch_backend() llamado en __init__
**PPO Status:** ‚úÖ _setup_torch_backend() llamado en __init__
**A2C Status:** ‚ùå **FALTA COMPLETAMENTE - no hay _setup_torch_backend()**

**Problem:** A2C no configura CUDA/torch backend
**Impact:** GPU no optimizada, Mixed Precision deshabilitado, CUDA no seeded
**Fix:** Agregar _setup_torch_backend() a A2C

---

### Issue #3: ‚ùå INCOMPLETE DEVICE INFO IN A2C
**SAC Status:** ‚úÖ get_device_info() retorna info completa
**PPO Status:** ‚úÖ get_device_info() retorna info completa
**A2C Status:** ‚ùå **NO EXISTE - missing get_device_info()**

**Problem:** No se puede diagnosticar estado del GPU
**Impact:** Debugging dif√≠cil en problemas de device
**Fix:** Agregar get_device_info() a A2C (copiar de SAC/PPO)

---

### Issue #4: ‚ö†Ô∏è INCONSISTENT ENTROPY DECAY SCHEDULES
**SAC Status:** ‚úÖ ent_coef_schedule = "linear", ent_coef_final = 0.001
**PPO Status:** ‚úÖ ent_coef_schedule = "linear", ent_coef_final = 0.001
**A2C Status:** ‚ö†Ô∏è ent_coef_schedule = "linear", ent_coef_final = 0.0001 **‚Üê 10X LOWER**

**Problem:** A2C usa entropy final 10x m√°s baja que SAC/PPO
**Impact:** Puede causar convergencia prematura en A2C
**Fix:** Harmonizar: A2C ent_coef_final = 0.001 (como SAC/PPO)

```python
# ANTES (A2C):
ent_coef_final: float = 0.0001  # 10x menor que SAC/PPO

# DESPU√âS:
ent_coef_final: float = 0.001   # Consistente con SAC/PPO
```

---

### Issue #5: ‚ö†Ô∏è INCONSISTENT REWARD SCALING
**SAC Status:** ‚úÖ reward_scale = 1.0
**PPO Status:** ‚úÖ reward_scale = 0.1
**A2C Status:** ‚úÖ reward_scale = 0.1 **PERO config.py l√≠nea ~57 dice "0.1: evita Q-explosion"**

**Problem:** SAC (off-policy) usa 1.0, PPO/A2C (on-policy) usan 0.1 - OK, pero inconsistencia
**Impact:** Menor, pero puede afectar comparaci√≥n de agents
**Fix:** Documentar WHY diferente (OK como est√°)

---

### Issue #6: ‚ùå MISSING NORMALIZE_ADVANTAGE FLAG IN PPO
**SAC Status:** N/A (off-policy)
**PPO Status:** ‚ùå **NO TIENE normalize_advantage FLAG** (SB3 tiene built-in pero no documentado)
**A2C Status:** ‚úÖ normalize_advantages: bool = True

**Problem:** PPO no expone control sobre advantage normalization
**Impact:** Inconsistencia con A2C, menos transparencia
**Fix:** Agregar a PPOConfig:
```python
normalize_advantage: bool = True  # ‚Üê AGREGAR (SB3 built-in parameter)
```

---

### Issue #7: ‚ö†Ô∏è PPO HUBER LOSS IMPLEMENTATION
**SAC Status:** N/A (off-policy)
**PPO Status:** ‚úÖ S√ç TIENE use_huber_loss = True + custom policy class (l√≠nea ~300-350)
**A2C Status:** ‚úÖ S√ç TIENE use_huber_loss = True + custom policy class (l√≠nea ~350-400)

**Problem:** Implementaciones CASI ID√âNTICAS pero copy-pasted (no DRY)
**Impact:** Maintenance burden, diferencias podr√≠an divergir
**Fix:** Considerar extraer a agent_utils.py (pero LOW PRIORITY - funciona)

---

### Issue #8: ‚ùå MISSING LEARNING RATE SCHEDULE PASSING TO SB3
**SAC Status:** ‚úÖ learning_rate passed (constant only)
**PPO Status:** ‚ö†Ô∏è lr_schedule = "linear" EN CONFIG pero **NO SE USA AL CREAR PPO()**
**A2C Status:** ‚ö†Ô∏è lr_schedule = "linear" EN CONFIG pero **NO SE USA AL CREAR A2C()**

**Problem:** Config define schedule pero SB3 no lo recibe
**Impact:** Learning rate nunca decae - stays constant (defeats purpose)
**Fix:** Implementar learning rate schedule callback o usar SB3's learning_rate parameter correctly

---

## SUMMARY: ISSUES BY SEVERITY

| Issue | SAC | PPO | A2C | Severity | Fix Time |
|-------|-----|-----|-----|----------|----------|
| #1: Missing validation call in learn() | ‚úÖ | ‚ùå | ‚ùå | CRITICAL | 1 min |
| #2: Missing torch setup | ‚úÖ | ‚úÖ | ‚ùå | HIGH | 2 min |
| #3: Missing get_device_info | ‚úÖ | ‚úÖ | ‚ùå | MEDIUM | 1 min |
| #4: Inconsistent entropy decay final | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | MEDIUM | 1 min |
| #5: Inconsistent reward scaling | ‚úÖ | ‚ö†Ô∏è | ‚úÖ | LOW | 0 min (OK as is) |
| #6: Missing normalize_advantage flag | N/A | ‚ùå | ‚úÖ | LOW | 2 min |
| #7: Duplicate Huber loss code | N/A | ‚úÖ | ‚úÖ | LOW | 5 min (extract) |
| #8: LR schedule not passed to SB3 | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | HIGH | 3 min |

---

## RECOMMENDED ACTION PLAN

**Phase 1 (CRITICAL - 5 min):**
1. Add _validate_dataset_completeness() call to PPO.learn() and A2C.learn()
2. Add _setup_torch_backend() and get_device_info() to A2C
3. Harmonize A2C ent_coef_final from 0.0001 ‚Üí 0.001

**Phase 2 (HIGH - 3 min):**
4. Implement learning rate schedule callback for PPO and A2C

**Phase 3 (MEDIUM - 2 min):**
5. Add normalize_advantage to PPOConfig
6. Verify Huber loss implementations are consistent

---

## IMPLEMENTATION ORDER

```
1. PPO: Add _validate_dataset_completeness() call
2. A2C: Add _validate_dataset_completeness() call
3. A2C: Add _setup_torch_backend() method
4. A2C: Add get_device_info() method
5. A2C: Change ent_coef_final to 0.001
6. PPO: Add normalize_advantage to config
7. PPO & A2C: Verify LR schedule passing (if needed)
```

---

## FILES TO MODIFY

- `src/iquitos_citylearn/oe3/agents/ppo_sb3.py` - **Lines ~130 (config), ~320 (learn method)**
- `src/iquitos_citylearn/oe3/agents/a2c_sb3.py` - **Lines ~60 (config), ~180-200 (new methods), ~240 (learn method)**

---

**Prepared by:** GitHub Copilot  
**Date:** 2026-02-03 00:55 UTC  
**Status:** Ready for implementation
