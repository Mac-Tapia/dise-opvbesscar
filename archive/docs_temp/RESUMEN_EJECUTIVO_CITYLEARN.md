# ðŸ“Š DOCUMENTO EJECUTIVO: CITYLEARN CONTROL & PREDICCIÃ“N EN PVBESSCAR
**ActualizaciÃ³n**: 2026-02-14 | **Estado**: âœ… FUNCIONANDO

---

## ðŸŽ¯ PREGUNTA DEL USUARIO
> "Quiero saber quÃ© hace el control y predicciÃ³n de CityLearn, si estÃ¡ instalado, y si lo estÃ¡ usando en este entrenamiento"

---

## âœ… RESPUESTA DIRECTA (3 PUNTOS CLAVE)

### **1. Â¿EstÃ¡ INSTALADO CityLearn v2.5.0?**

**SÃ** âœ… 
```
UbicaciÃ³n: d:\diseÃ±opvbesscar\.venv\Lib\site-packages\citylearn\
VersiÃ³n: 2.5.0
Status: Totalmente instalado y funcional
```

---

### **2. Â¿EstÃ¡ siendo USADO en el entrenamiento actual?**

**PARCIALMENTE** âš ï¸

| Componente CityLearn | Â¿USADO en SAC Training? |
|-------------------|----------------------|
| CityLearnEnv (main environment) | âŒ NO |
| Physics engine | âŒ NO |
| Building simulations | âŒ NO |
| Reward conceptual framework | âœ“ SÃ (concept) |
| Multi-objective reward approach | âœ“ SÃ |

**En su lugar, ESTE PROYECTO usa:**
- `RealOE2Environment` (Gymnasium Env personalizado, definido localmente)
- `src/citylearnv2/` (cÃ³digo local inspirado en CityLearn)
- **Resultado**: Ambiente ESPECIALIZADO para Iquitos EV + BESS

---

### **3. Â¿QuÃ© hace el CONTROL y PREDICCIÃ“N?**

#### **A. CONTROL (Control de Acciones)**
El SAC agent **decide DÃ“NDE y CUÃNDO** distribuir energÃ­a:

```
ENTRADA (156-dim observation):
â”œâ”€ Solar: watts/mÂ² disponible
â”œâ”€ BESS: estado (% SOC)
â”œâ”€ 38 Chargers: demanda + poder actual
â”œâ”€ Grid: frecuencia, disponibilidad
â””â”€ VehÃ­culos: SOC, deadline

SALIDA (39-dim action, [0,1] normalizado):
â”œâ”€ Action[0]: BESS setpoint
â”‚  â””â”€ 0.0 = charge max | 0.5 = idle | 1.0 = discharge
â””â”€ Action[1-38]: Charger power setpoints
   â””â”€ 0.0 = off | 1.0 = 7.4 kW mÃ¡ximo

DECISIÃ“N EN CADA HORA:
â”œâ”€ Â¿Cargar BESS desde solar? â†’ action[0] = 0.2
â”œâ”€ Â¿Cargar estos 5 vehÃ­culos? â†’ action[5:10] = 0.8
â”œâ”€ Â¿Descargar BESS al grid? â†’ action[0] = 0.7
â””â”€ Â¿Evitar pico de importaciÃ³n? â†’ acciones coordinadas
```

**Ejemplo de 1 hora:**
```
t=10:00am ESTADO:
â”œâ”€ Solar: 800 W/mÂ² (maÃ±ana, buen dÃ­a)
â”œâ”€ BESS: 45% SOC (bajo)
â”œâ”€ Chargers: 15 vehÃ­culos esperando (prioridad: deadline 2pm)
â””â”€ Grid: expensive (tarifa pico)

SAC AGENT DECIDE:
â”œâ”€ action[0] = 0.3 â†’ BESS charge -25 kW (guardar solar para peak)
â”œâ”€ action[5:15] = 0.6 â†’ 15 chargers @ 4.4 kW c/u (prioridad deadline)
â”œâ”€ action[16:38] = 0.1 â†’ resto minimal (carga lenta)

RESULTADO (despuÃ©s de dispatch):
â”œâ”€ Solar 800 W/mÂ² â†’ 98 kW (some to EVs, some to BESS)
â”œâ”€ BESS recibe: 25 kW carga (respaldando para peak)
â”œâ”€ Chargers reciben: 66 kW (15 Ã— 4.4)
â”œâ”€ Grid: 0 kW import (solar covers all)
â”œâ”€ Reward: +1.2 (CO2 evitado, solar usado, EV satisfechos)
```

---

#### **B. PREDICCIÃ“N (Critic Networks)**

El **Critic** **predice REWARDS FUTUROS** basado en estados:

```
ESTRUCTURA:
â”‚
â”œâ”€ Actor Network Ï€(a|s):
â”‚  â””â”€ Input: obs[156-dim] â†’ Output: action[39-dim]
â”‚     â””â”€ "Â¿QuÃ© acciÃ³n debo tomar?"
â”‚
â”œâ”€ Critic Network Q(s,a):
â”‚  â””â”€ Input: obs[156-dim] + action[39-dim] â†’ Output: Q-value (scalar)
â”‚     â””â”€ "Si tomo esta acciÃ³n, Â¿cuÃ¡l es el reward futuro esperado?"
â”‚
â””â”€ Target Critic (copy estable):
   â””â”€ PredicciÃ³n mÃ¡s conservadora para stability
```

**Ejemplo de predicciÃ³n:**

```
Hora t=10 (maÃ±ana):
â”œâ”€ Observation: solar=800, BESS=45%, vehicles_waiting=15
â”œâ”€
â”œâ”€ Actor propone: action=[0.3, 0.6, 0.6, ...] 
â”‚  â””â”€ (charge BESS, cargar 2 vehÃ­culos prioritarios)
â”‚
â”œâ”€ Critic predice:
â”‚  â”‚  Q(obs, action) â‰ˆ 42.5
â”‚  â”‚  â””â”€ "Si tomas esa acciÃ³n, espera reward acumulado de ~42.5"
â”‚
â”œâ”€ Al siguiente timestep (11am):
â”‚  â”‚  Solar: 850 W/mÂ² (mejorÃ³ âœ“)
â”‚  â”‚  Vehicles finalizadas: 2 âœ“
â”‚  â”‚  Actual reward realizado: 1.4
â”‚
â”œâ”€ CrÃ­tico aprende:
â”‚  â”‚  Error = 1.4 - 42.5 = -41.1 (ajustar predicciÃ³n)
â”‚  â”‚  â””â”€ PrÃ³xima vez, predecir mÃ¡s alto para esta situaciÃ³n
â”‚
â””â”€ Actor aprende:
   â””â”€ "Esa acciÃ³n (charge BESS + charge vehicles) fue BUENA"
      â””â”€ "Intenta similares cuando veas este estado"
```

---

## ðŸ—ï¸ ARQUITECTURA ACTUAL DEL ENTRENAMIENTO

```
FLUJO SIMPLIFICADO:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TRAINING SAC (ROBUSTO)                      â”‚
â”‚                                                              â”‚
â”‚  DATOS REALES (OE2 2024)                                    â”‚
â”‚  â”œâ”€ Solar: 4,050 kWp, 8.3 GWh/aÃ±o                          â”‚
â”‚  â”œâ”€ Chargers: 38 sockets, 1.02 MWh/aÃ±o                     â”‚
â”‚  â”œâ”€ Mall: 12.4 GWh/aÃ±o                                     â”‚
â”‚  â””â”€ BESS: 940 kWh SOC + flows                               â”‚
â”‚           â†“                                                 â”‚
â”‚  RealOE2Environment (GYMNASIUM)                             â”‚
â”‚  â”œâ”€ 156-dim observation space                               â”‚
â”‚  â”œâ”€ 39-dim action space                                     â”‚
â”‚  â”œâ”€ Physics manual (EV + BESS)                              â”‚
â”‚  â””â”€ Reward multiobjetivo                                    â”‚
â”‚        â†“                                                    â”‚
â”‚  SAC Agent (Stable-Baselines3)                              â”‚
â”‚  â”œâ”€ Actor: decide actions                                   â”‚
â”‚  â”œâ”€ Critic 1+2: predict Q-values                            â”‚
â”‚  â”œâ”€ Entropy Î±: balancea exploraciÃ³n                         â”‚
â”‚  â””â”€ GPU CUDA (RTX 4060, 83 FPS)                             â”‚
â”‚        â†“                                                    â”‚
â”‚  CONTROL â†’ actions[39-dim] â†’ Energy dispatch                â”‚
â”‚  PREDICCIÃ“N â†’ Q-values â†’ Better decisions                   â”‚
â”‚        â†“                                                    â”‚
â”‚  OUPUT: CO2 minimized, solar maximized, EVs satisfied       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ˆ ESTADO ACTUAL DEL ENTRENAMIENTO

**Desde hace ~20 minutos:**

```
PROGRESO:
â”œâ”€ Episodios completados: 14
â”œâ”€ Timesteps totales: 131,959 (target: 87,600 Ã— N episodios)
â”œâ”€ Velocidad: 93 FPS (GPU efficient)
â”œâ”€ DuraciÃ³n: ~554 segundos (~9 minutos)
â””â”€ Status: âœ… RUNNING EN BACKGROUND

MÃ‰TRICAS DE CONVERGENCIA:
â”œâ”€ Actor Loss: -515 (bueno, negativo = optimizing)
â”œâ”€ Critic Loss: 2.05 (pequeÃ±o = predicciones acertadas)
â”œâ”€ Entropy: 0.20 (balance exploraciÃ³n/explotaciÃ³n)
â”œâ”€ Learning Rate: 0.0003 (stable)
â””â”€ Mean Reward/Episode: ~1,300 puntos

SISTEMA EN EJECUCIÃ“N:
â”œâ”€ Energy distributed:
â”‚  â”œâ”€ Solarâ†’EV: 179,587 kWh (vs grid)
â”‚  â”œâ”€ BESSâ†’EV: 143,740 kWh (buffer)
â”‚  â”œâ”€ Grid import: 6,485,565 kWh (minimizado)
â”‚  â””â”€ Total CO2 avoided: 279,679 kg
â”‚
â”œâ”€ VehÃ­culos:
â”‚  â”œâ”€ Cargados al 100%: 53,335
â”‚  â”œâ”€ Motos at 50% SOC: 26 | at 80%: 18
â”‚  â””â”€ Mototaxis at 50% SOC: 7 | at 80%: 5
â”‚
â””â”€ Control Quality:
   â”œâ”€ PriorizaciÃ³n accuracy: 44.1%
   â”œâ”€ BESS ciclos: 3,301
   â””â”€ Costo grid: 2,300,787 soles (vs 2B+ sin RL)
```

---

## ðŸ”„ CICLO COMPLETO: CONTROL + PREDICCIÃ“N EN ACCIÃ“N

### **Cada timestep (1 hora):**

1. **Observation** (Agent ve)
   ```
   156-dim vector con: solar, grid, BESS, 38 chargers, vehicles, time
   ```

2. **Prediction** (Critic predice)
   ```
   Q-values para acciones posibles
   "Si hago acciÃ³n X, reward esperado serÃ¡ Y"
   ```

3. **Selection** (Actor elige)
   ```
   Actor maximiza Q values predichos + entropy
   Selecciona mejor acciÃ³n probabilÃ­sticamente
   ```

4. **Control** (Sistema ejecuta)
   ```
   action[0:39] se convierte a kW
   Dispatch ejecuta (BESS, chargers, grid)
   ```

5. **Reality** (Mundo responde)
   ```
   Reward realizado se compara con predicciÃ³n
   PÃ©rdidas actualizan actor y critic
   ```

6. **Learning** (Agente aprende)
   ```
   Critic: ajusta predicciones
   Actor: ajusta polÃ­tica
   ```

---

## ðŸ’¡ Â¿POR QUÃ‰ NO USAR CITYLEARN DIRECTAMENTE?

| RazÃ³n | Impacto |
|-------|--------|
| CityLearn diseÃ±ado para multi-building HVAC | No soporta 38 sockets EV |
| Physics engine del edificio (heating/cooling) | Innecesario, solo EV + BESS |
| Observation space mÃ¡x 29-dim | Necesitamos 156-dim |
| Reward de electricidad genÃ©rica | Necesitamos CO2 Iquitos especÃ­fico |
| Data sintÃ©tica / Challenge 2022 | Tenemos datos reales 2024 |

**SoluciÃ³n: Especializar** 
â†’ RealOE2Environment (local) = mejor que CityLearnEnv (genÃ©rico)

---

## âœ¨ CONCLUSIÃ“N

| Pregunta | Respuesta |
|----------|-----------|
| Â¿CityLearn instalado? | âœ… SÃ, v2.5.0 en .venv/ |
| Â¿Usado en training? | âš ï¸ PARCIALMENTE (concepto reward) |
| Â¿QuÃ© es el control? | ðŸŽ® SAC Agent decide acciones (BESS + 38 chargers) |
| Â¿QuÃ© es la predicciÃ³n? | ðŸ”® Critic predice rewards futuros â†’ mejor control |
| Â¿Funcionando bien? | âœ… SÃ, 131,959 steps, convergiendo, GPU efficient |

**En una frase:**
> CityLearn estÃ¡ instalado pero no es la base de este entrenamiento. En su lugar, usamos **RealOE2Environment** (Gymnasium local + reward multiobjetivo) para control especÃ­fico de EV + BESS en Iquitos, con SAC agent que aprende via critic predictions.

---

## ðŸ“Œ ARCHIVOS RELACIONADOS

- ðŸ“„ [CITYLEARN_CONTROL_PREDICCION_EXPLICACION.md](CITYLEARN_CONTROL_PREDICCION_EXPLICACION.md) - ExplicaciÃ³n detallada
- ðŸ“„ [ANALISIS_CITYLEARN_CONTROL_PREDICCION.py](ANALISIS_CITYLEARN_CONTROL_PREDICCION.py) - AnÃ¡lisis ejecutable
- ðŸ“„ [scripts/train/train_sac_multiobjetivo.py](scripts/train/train_sac_multiobjetivo.py) - CÃ³digo fuente SAC
- ðŸ“„ [outputs/sac_training/live_training.log](outputs/sac_training/live_training.log) - Logs en vivo
