# Agent Training Configuration - Phase 8
# All agents optimized for CityLearn v2 with 8,760 hourly timesteps
# OE3: CO2 minimization + solar utilization + cost reduction

# ============================================================================
# SAC (Soft Actor-Critic) Configuration
# ============================================================================
# Type: Off-policy, sample-efficient entropy regularization
# Best for: Exploration in sparse reward environments
# Expected Training Time: 60-90 minutes (50 episodes × 8,760 steps)

sac:
  enabled: true
  # Basic Training Parameters
  algorithm: "SAC"
  framework: "stable-baselines3"
  total_timesteps: 438000000 # 50 episodes × 8,760 steps/episode
  learning_rate: 2.0e-4
  learning_rate_schedule: null # No decay, constant
  # Network Architecture
  policy_type: "MlpPolicy"
  hidden_sizes: [1024, 1024]
  activation_function: "relu"
  net_arch: null # Use default (policy and value networks)
  # SAC-Specific Hyperparameters
  batch_size: 256
  buffer_size: 1000000
  learning_starts: 10000 # Start training after 10k steps
  train_freq: 1 # Update after every step
  gradient_steps: 1 # Gradient step per environment step
  # Entropy & Exploration
  ent_coef: "auto" # Automatic entropy coefficient tuning
  target_entropy: "auto" # Infer from action space
  use_sde: true # Stochastic Dynamics Exploration
  sde_sample_freq: -1 # Sample every step
  # Target Network Updates
  target_update_interval: 1 # Update target networks every step
  target_smoothing_coefficient: 0.005 # Tau for soft updates
  # Regularization & Stability
  use_amp: true # Mixed precision training (faster, lower memory)
  max_grad_norm: 10.0
  # Device & Parallel
  device: "auto" # Detect GPU automatically (cuda/cpu)
  n_cpu_tf_sess: 1
  seed: 42
  # Checkpoint & Resume
  save_freq: 8760 # Save after each episode (8,760 steps)
  save_path: "checkpoints/SAC"
  resume_checkpoint: null # Set to path to resume training
  # Logging
  verbose: 1
  tensorboard_log: "analyses/logs/SAC"
  log_interval: 100 # Log every 100 training steps
  # Expected Performance
  # - Episode reward: -1,000 to +500 (reward is negative for CO2)
  # - Convergence: ~20-30 episodes
  # - CO2 reduction: 20-26% vs baseline
  # - Solar utilization: 60-65%

# ============================================================================
# PPO (Proximal Policy Optimization) Configuration
# ============================================================================
# Type: On-policy, stable gradient estimation
# Best for: Stable convergence, reliable training
# Expected Training Time: 90-120 minutes (50 episodes × 8,760 steps)

ppo:
  enabled: true
  # Basic Training Parameters
  algorithm: "PPO"
  framework: "stable-baselines3"
  total_timesteps: 438000000 # 50 episodes × 8,760 steps/episode
  learning_rate: 2.0e-4
  learning_rate_schedule: "linear" # Linear decay to 0 over training
  # Network Architecture
  policy_type: "MlpPolicy"
  hidden_sizes: [1024, 1024]
  activation_function: "relu"
  net_arch: null # Use default
  # PPO-Specific Hyperparameters
  n_steps: 2048 # Rollout length (steps collected before update)
  batch_size: 128 # Minibatch size for gradient updates
  n_epochs: 20 # Number of epoch for SGD/Adam
  gamma: 0.99 # Discount factor
  gae_lambda: 0.98 # GAE decay factor (0.95-0.99)
  clip_range: 0.1 # Clipping range (0.1-0.3)
  clip_range_vf: null # Value function clipping (None = no clipping)
  # Entropy & Exploration
  ent_coef: 0.0 # Entropy coefficient (0 = no bonus)
  vf_coef: 0.5 # Value function coefficient
  max_grad_norm: 0.5
  # Regularization & Stability
  use_amp: true # Mixed precision
  normalize_advantage: true # Normalize advantage function
  # Device & Parallel
  device: "auto" # Detect GPU
  n_cpu_tf_sess: 1
  seed: 42
  # Checkpoint & Resume
  save_freq: 8760 # Save after each episode
  save_path: "checkpoints/PPO"
  resume_checkpoint: null
  # Logging
  verbose: 1
  tensorboard_log: "analyses/logs/PPO"
  log_interval: 100
  # Expected Performance
  # - Episode reward: -800 to +300 (stable convergence)
  # - Convergence: ~25-35 episodes
  # - CO2 reduction: 25-29% vs baseline
  # - Solar utilization: 65-70%
  # - Stability: Highest (least variance)

# ============================================================================
# A2C (Advantage Actor-Critic) Configuration
# ============================================================================
# Type: On-policy, simple advantage-based
# Best for: Fast training, reliable baseline
# Expected Training Time: 60-90 minutes (50 episodes × 8,760 steps)

a2c:
  enabled: true
  # Basic Training Parameters
  algorithm: "A2C"
  framework: "stable-baselines3"
  total_timesteps: 438000000 # 50 episodes × 8,760 steps/episode
  learning_rate: 2.0e-4
  learning_rate_schedule: "linear" # Linear decay
  # Network Architecture
  policy_type: "MlpPolicy"
  hidden_sizes: [1024, 1024]
  activation_function: "relu"
  net_arch: null # Use default
  # A2C-Specific Hyperparameters
  n_steps: 2048 # Multi-step advantage calculation
  gamma: 0.99 # Discount factor
  gae_lambda: 1.0 # GAE decay (1.0 = MC, <1.0 = TD)
  ent_coef: 0.0 # Entropy regularization
  vf_coef: 0.5 # Value function coefficient
  max_grad_norm: 0.5 # Gradient clipping
  # Parallel Environments
  n_envs: 1 # Number of parallel environments (1 for CityLearn)
  use_rms_prop: true # Use RMSProp instead of Adam
  # Regularization & Stability
  use_amp: true # Mixed precision
  normalize_advantage: true
  # Device & Parallel
  device: "auto" # Detect GPU
  n_cpu_tf_sess: 1
  seed: 42
  # Checkpoint & Resume
  save_freq: 8760 # Save after each episode
  save_path: "checkpoints/A2C"
  resume_checkpoint: null
  # Logging
  verbose: 1
  tensorboard_log: "analyses/logs/A2C"
  log_interval: 100
  # Expected Performance
  # - Episode reward: -900 to +200 (decent convergence)
  # - Convergence: ~20-30 episodes
  # - CO2 reduction: 20-25% vs baseline
  # - Solar utilization: 60-65%
  # - Speed: Fast (simplest algorithm)

# ============================================================================
# ENVIRONMENT & EPISODE Configuration
# ============================================================================
environment:
  # CityLearn v2 Environment
  environment_type: "CityLearnEnv"
  schema_path: "outputs/schema_latest.json" # Generated by dataset_builder
  # Episode Configuration
  episode_length: 8760 # 1 year of hourly steps
  num_episodes: 50 # Training episodes per agent
  reset_num_timesteps: false # Accumulate timesteps across episodes
  # Observation & Action Spaces
  # Observation: 534 dims (flattened from building + charger states)
  # Action: 126 dims (charger power setpoints, normalized [0,1])
  obs_space_size: 534
  action_space_size: 126
  action_space_type: "continuous" # Continuous [0,1] actions
  # Reward Configuration
  reward_function: "multiobj" # Multi-objective: CO2, solar, cost, EV, grid
  reward_weights:
    co2: 0.50 # Primary: CO2 minimization
    solar: 0.20 # Secondary: Solar self-consumption
    cost: 0.10 # Tertiary: Cost reduction
    ev_satisfaction: 0.10 # EV charging requirement
    grid_stability: 0.10 # Peak demand smoothing
  # Termination Criteria
  terminate_on_max_steps: true
  max_steps_per_episode: 8760
  early_termination: false

# ============================================================================
# GLOBAL TRAINING Configuration
# ============================================================================
training:
  # Execution Mode
  mode: "serial" # serial (one agent at a time) or parallel (if multi-GPU)
  agents_to_train: ["SAC", "PPO", "A2C"]
  agent_sequence: ["SAC", "PPO", "A2C"] # Order to train
  # Validation
  validate_every_n_episodes: 5 # Run evaluation every 5 episodes
  validation_episodes: 1 # Test on how many episodes
  validation_deterministic: true # Use deterministic policy (greedy)
  # Monitoring
  monitor_training: true
  monitor_interval: 100 # Log metrics every 100 steps
  save_video: false # Too expensive for 8,760 steps
  # Checkpointing
  enable_checkpointing: true
  checkpoint_interval: 8760 # After each episode
  keep_last_n_checkpoints: 5 # Keep 5 most recent
  # Hardware
  use_gpu: true # Detect and use GPU if available
  gpu_memory_limit: null # Auto (use all available)
  precision: "mixed" # mixed (AMP) or full (FP32)
  # Random Seeds
  seed: 42
  np_seed: 42
  torch_seed: 42

# ============================================================================
# EVALUATION Configuration
# ============================================================================
evaluation:
  # Metrics to Track
  metrics:
    - "episode_reward" # Total reward per episode
    - "co2_reduction" # vs baseline
    - "solar_utilization" # % of PV generation used
    - "cost_savings" # $ saved vs baseline
    - "grid_stability" # Peak demand reduction
    - "ev_satisfaction" # % of EV demand met
  # Baseline Comparison
  compare_to_baseline: true
  baseline_path: "analyses/logs/baseline_uncontrolled.json"
  # Reporting
  generate_plots: true
  plot_path: "analyses/plots"
  save_metrics: true
  metrics_path: "analyses/metrics"
  # Final Report
  generate_final_report: true
  report_path: "COMPARACION_BASELINE_VS_RL.txt"
# ============================================================================
# NOTES FOR PHASE 8 EXECUTION
# ============================================================================
#
# 1. PYTHON VERSION REQUIREMENT: Python 3.11 (not 3.13)
#    - Install following PYTHON_3.11_SETUP_GUIDE.md
#    - Verify: python --version → Python 3.11.x
#
# 2. CITYLEARN INSTALLATION:
#    pip install citylearn>=2.5.0
#
# 3. DATASET GENERATION (Required Before Training):
#    python -m scripts.run_oe3_build_dataset --config configs/default.yaml
#    Output: outputs/schema_latest.json + 128 charger_simulation_*.csv
#
# 4. QUICK VALIDATION BEFORE FULL TRAINING:
#    python scripts/train_quick.py --episodes 1 --agent PPO
#    Should complete in ~5 minutes without errors
#
# 5. FULL TRAINING COMMAND:
#    python scripts/train_agents_serial.py --device cuda --episodes 50
#    Expected: 4-6 hours for all 3 agents (SAC + PPO + A2C)
#
# 6. MONITORING:
#    # In separate terminal:
#    python scripts/monitor_training_live_2026.py
#    Updates every 5 seconds with progress
#
# 7. RESOURCE ALLOCATION:
#    - Single GPU: Run sequentially (~4-6 hours)
#    - Multi-GPU: Can parallelize (reduce to ~2-3 hours)
#    - CPU-only: Not recommended (~12-18 hours)
#
# 8. TROUBLESHOOTING:
#    - GPU OOM: Reduce batch_size or n_steps
#    - Slow training: Ensure CUDA GPU detected
#    - Reward explosion: Check dataset generation
#    - Checkpoint incompatible: Delete old checkpoints
#
# 9. SUCCESS CRITERIA:
#    - ✓ All 3 agents trained 50+ episodes
#    - ✓ Training converges (reward stabilizes)
#    - ✓ CO2 reduction ≥ 20%
#    - ✓ Solar utilization ≥ 60%
#    - ✓ No crashes or OOM errors
#
# 10. NEXT STEPS AFTER TRAINING:
#     - Review COMPARACION_BASELINE_VS_RL.txt
#     - Analyze training logs in analyses/logs/
#     - Compare agent performance
#     - Generate final performance report
#
