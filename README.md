# ğŸ”‹âš¡ pvbesscar - RL-based EV Charging Optimization

**OptimizaciÃ³n de carga EV con energÃ­a solar mediante Reinforcement Learning**

Iquitos, PerÃº - Control inteligente de 38 sockets de carga (270 motos + 39 mototaxis/dÃ­a) usando agentes RL (SAC/PPO/A2C) para minimizar COâ‚‚ en red aislada.

---

## ğŸ¯ Resumen Ejecutivo

**pvbesscar** implementa un sistema completo de dos fases para optimizar infraestructura de carga EV:

- **OE2 (Dimensioning):** Especificaciones de infraestructura
  - 19 cargadores (15 motos + 4 mototaxis) Ã— 2 sockets = **38 puntos de carga**
  - Solar: **4,050 kWp** PVGIS
  - BESS: **2,000 kWh** con eficiencia 95%
  - Precio: **0.4521 kg COâ‚‚/kWh** (thermal generation Iquitos)

- **OE3 (Control):** Control inteligente con RL
  - 3 agentes: **SAC, PPO, A2C** (Stable-Baselines3)
  - 8,760 horas (1 aÃ±o) simulaciÃ³n real
  - Reward multiobjetivo: COâ‚‚ (45%), solar (15%), vehÃ­culos (25%), grid (5%), BESS (5%), priority (5%)
  - **Potencial:** COâ‚‚ -26% to -29% vs baseline

---

## ğŸ“‚ Estructura del Proyecto

```
pvbesscar/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dimensionamiento/oe2/          # OE2: Dimensionamiento
â”‚   â”‚   â”œâ”€â”€ disenocargadoresev/        # Specs chargers (19 units)
â”‚   â”‚   â”œâ”€â”€ generacionsolar/           # PVGIS solar generation
â”‚   â”‚   â””â”€â”€ balance_energetico/        # Energy balance
â”‚   â”œâ”€â”€ agents/                         # OE3: RL Agents
â”‚   â”‚   â”œâ”€â”€ sac.py                     # SAC = off-policy (best CO2)
â”‚   â”‚   â”œâ”€â”€ ppo_sb3.py                 # PPO = on-policy stable
â”‚   â”‚   â”œâ”€â”€ a2c_sb3.py                 # A2C = on-policy simple
â”‚   â”‚   â””â”€â”€ no_control.py              # Baseline
â”‚   â”œâ”€â”€ dataset_builder_citylearn/      # CityLearn v2 integration
â”‚   â”‚   â”œâ”€â”€ data_loader.py             # OE2â†’OE3 pipeline
â”‚   â”‚   â”œâ”€â”€ rewards.py                 # MultiObjectiveReward
â”‚   â”‚   â””â”€â”€ dataset_builder.py         # Dataset construction
â”‚   â””â”€â”€ utils/                          # Shared utilities
â”‚       â”œâ”€â”€ agent_utils.py
â”‚       â”œâ”€â”€ logging.py
â”‚       â””â”€â”€ time.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ oe2/                            # OE2 artifacts (real data)
â”‚   â”‚   â”œâ”€â”€ chargers/chargers_ev_ano_2024_v3.csv      (8,760 hours)
â”‚   â”‚   â”œâ”€â”€ bess/bess_ano_2024.csv                    (8,760 hours)
â”‚   â”‚   â”œâ”€â”€ Generacionsolar/pv_generation_*.csv      (8,760 hours)
â”‚   â”‚   â””â”€â”€ demandamallkwh/demand*.csv               (8,760 hours)
â”‚   â””â”€â”€ processed/                      # Processed data
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train/
â”‚       â”œâ”€â”€ train_sac.py               # SAC training script
â”‚       â”œâ”€â”€ train_ppo.py               # PPO training script
â”‚       â”œâ”€â”€ train_a2c.py               # A2C training script
â”‚       â””â”€â”€ common_constants.py        # Shared constants
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml                   # Main config
â”‚   â””â”€â”€ agents/                        # Agent-specific configs
â”œâ”€â”€ checkpoints/                        # Trained weights
â”‚   â”œâ”€â”€ SAC/
â”‚   â”œâ”€â”€ PPO/
â”‚   â”œâ”€â”€ A2C/
â”‚   â””â”€â”€ Baseline/
â”œâ”€â”€ logs/                               # Training logs
â”œâ”€â”€ outputs/                            # Results & metrics
â””â”€â”€ README.md                           # This file
```

---

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Create virtual environment
python -m venv .venv
.venv\Scripts\Activate.ps1              # Windows PowerShell
# or
source .venv/bin/activate              # Linux/Mac

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-training.txt  # For GPU (RTX 4060+)
```

### 2. Verify Data Integrity
```bash
# Check all datasets have 8,760 hours (1 year)
python -c "import pandas as pd; \
  assert len(pd.read_csv('data/oe2/chargers/chargers_ev_ano_2024_v3.csv'))==8760; \
  print('âœ“ Data OK')"
```

### 3. Train Agent (Choose One)

**Option A: SAC (Recommended - best COâ‚‚ reduction)**
```bash
python scripts/train/train_sac.py --episodes 10 --log-dir outputs/sac_test/
# Duration: ~5-7 hours (GPU RTX 4060)
```

**Option B: PPO (Stable, good COâ‚‚)**
```bash
python scripts/train/train_ppo.py --episodes 10 --log-dir outputs/ppo_test/
# Duration: ~4-6 hours (GPU RTX 4060)
```

**Option C: A2C (Fast)**
```bash
python scripts/train/train_a2c.py --episodes 10 --log-dir outputs/a2c_test/
# Duration: ~3-5 hours (GPU RTX 4060)
```

### 4. View Results
```bash
# Results saved to outputs/ and logs/
# Checkpoints auto-resume from checkpoints/{SAC,PPO,A2C}/
```

---

## ğŸ“Š Key Concepts

### OE2 (Dimensioning Phase)
Defines infrastructure specifications:
- **19 chargers** (Mode 3, 7.4 kW per socket @ 32A/230V)
- **4,050 kWp** solar capacity (PVGIS hourly data)
- **2,000 kWh** battery storage (Pb-acid, 95% eff, 20% min SOC)
- **Daily demand:** 270 motos + 39 mototaxis

### OE3 (Control Phase)
Trains RL agents to optimize dispatch:
- **Observation:** Solar W/mÂ², Grid Hz, BESS % SOC, 38 socket states, time features (156 dims)
- **Action:** Continuous [0,1] power setpoints for BESS + 38 sockets (39 dims)
- **Episode:** 8,760 timesteps (1 year @ 1 hour per step)
- **Reward:** COâ‚‚ minimization primary objective

### Multi-Objective Reward (v6.0)
```python
Reward = 0.45Ã—CO2_reduction + 0.15Ã—solar_self_consumption + 0.25Ã—vehicle_charging
       + 0.05Ã—grid_stability + 0.05Ã—BESS_optimization + 0.05Ã—priority_dispatch
```

---

## ğŸ”§ Configuration

**Main config:** `configs/default.yaml`
```yaml
BESS_CAPACITY_KWH: 2000
CO2_FACTOR_IQUITOS: 0.4521
CHARGER_MAX_KW: 3.7          # Per socket (2 Ã— socket = ~7.4 kW per charger)
MOTOS_PER_DAY: 270
MOTOTAXIS_PER_DAY: 39
HOURS_PER_YEAR: 8760
```

**Synchronized across all agents** via `scripts/train/common_constants.py`

---

## ğŸ“ˆ Expected Performance

### Baseline (No Control)
- COâ‚‚: ~10,200 kg/year
- Solar utilization: ~40%
- Grid dependency: 100%

### RL Agents (After Tuning)
| Agent | COâ‚‚ (kg/y) | Reduction | Solar Util. | Training Time |
|-------|-----------|-----------|------------|---------------|
| **SAC** | ~7,500 | **-26%** | 65% | 5-7h (GPU) |
| **PPO** | ~7,200 | **-29%** | 68% | 4-6h (GPU) |
| **A2C** | ~7,800 | **-24%** | 60% | 3-5h (GPU) |

---

## ğŸ§ª Validation & Testing

### Data Integrity Check
```bash
# Verify all datasets have 8,760 rows
python test_consistency_sac_ppo_a2c.py
```

### Architecture Audit
```bash
# Full system validation
python audit_architecture.py
# Outputs: Component verification, integration checks, readiness status
```

### Training Readiness
```bash
# Single episode test (5 minutes)
python scripts/train/train_sac.py --episodes 1 --log-dir outputs/test/
# Check: checkpoints/SAC/latest.zip, logs/training/*.log
```

---

## ğŸ“ Agent Details

### SAC (Soft Actor-Critic)
- **Type:** Off-policy
- **Best for:** Asymmetric rewards, COâ‚‚ minimization
- **Training:** 26,280 steps/episode = 5-7 hours GPU
- **Advantage:** Handles sparse rewards, good exploration
- **Recommended:** âœ… YES for production

### PPO (Proximal Policy Optimization)
- **Type:** On-policy
- **Best for:** Stable convergence, bounded updates
- **Training:** 26,280 steps/episode = 4-6 hours GPU
- **Advantage:** Clip prevents divergence, good for beginners
- **Recommended:** âœ… YES (alternative to SAC)

### A2C (Advantage Actor-Critic)
- **Type:** On-policy
- **Best for:** Simple, fast training
- **Training:** 26,280 steps/episode = 3-5 hours GPU
- **Advantage:** Faster than PPO/SAC
- **Note:** May not capture COâ‚‚ nuances as well

---

## ğŸ“ Checkpoints & AutoResume

Agents automatically load and resume from latest checkpoint:

```python
# Auto-loads from checkpoints/{SAC,PPO,A2C}/latest.zip
agent = make_sac(env)

# Resume training (accumulates steps)
agent.learn(total_timesteps=10000, reset_num_timesteps=False)
```

**Checkpoint metadata:** `TRAINING_CHECKPOINTS_SUMMARY_*.json`
```json
{
  "agent": "SAC",
  "episode": 42,
  "total_steps": 1_102_680,
  "best_reward": -185.3,
  "timestamp": "2026-02-18 14:23:00"
}
```

---

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| "38 sockets not found" | Check `data/oe2/chargers/chargers_ev_ano_2024_v3.csv` has 19 chargers Ã— 2 |
| GPU out of memory | Reduce training `batch_size` in config |
| Reward NaN | Verify solar CSV has 8,760 rows exactly |
| Checkpoint load error | Delete `checkpoints/{Agent}/` and restart training |
| Constants mismatch | Verify `scripts/train/common_constants.py` has CHARGER_MAX_KW=3.7 |

---

## ğŸ“š References

- **Architecture:** See [READINESS_REPORT_v72.md](READINESS_REPORT_v72.md)
- **Validation:** See [DOCUMENTO_EJECUTIVO_VALIDACION_v72.md](DOCUMENTO_EJECUTIVO_VALIDACION_v72.md)
- **Data Details:** `data/oe2/` subdirectories
- **Constants:** `scripts/train/common_constants.py`
- **Rewards:** `src/dataset_builder_citylearn/rewards.py`

---

## ğŸ‘¥ Requirements & Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| Python | 3.11+ | Runtime |
| stable-baselines3 | 2.0+ | RL agents |
| gymnasium | 0.27+ | RL environment API |
| CityLearn | v2 | Energy simulation |
| PyTorch | 2.5.1 | Neural network backend |
| pandas | Latest | Data handling |
| numpy | Latest | Numerical computing |

**Install:**
```bash
pip install -r requirements.txt                 # CPU
pip install -r requirements-training.txt        # GPU (CUDA 12.1)
```

---

## âœ… Project Status

- âœ… **Architecture:** 100% implemented (OE2 + OE3)
- âœ… **Data:** 100% validated (8,760 hours real data)
- âœ… **Agents:** 100% synchronized (SAC/PPO/A2C)
- âœ… **Training Ready:** YES - start immediately
- âœ… **Production Ready:** YES - infrastructure ready

**Next Steps:**
1. Run: `python scripts/train/train_sac.py --episodes 10`
2. Monitor: Check `checkpoints/SAC/` and `logs/training/`
3. Evaluate: Analyze COâ‚‚ reduction vs baseline

---

**Last Updated:** 2026-02-18  
**Version:** 7.2  
**Status:** âœ… Production Ready  
**Contact:** [Project Team]

---

## ğŸ“œ License

[Add license info here]

## ğŸ“§ Support

[Add contact/issue tracker info here]
