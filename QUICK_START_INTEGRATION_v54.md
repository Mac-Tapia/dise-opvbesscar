# üöÄ GU√çA R√ÅPIDA: Dataset v5.4 ‚Üí CityLearn ‚Üí Agentes RL

**Estado**: ‚úÖ Dataset listo para integraci√≥n  
**Tiempo estimado**: ~30 minutos desde aqu√≠ hasta entrenamiento en GPU

---

## 1Ô∏è‚É£ VALIDAR QUE TODO EST√Å LISTA

```bash
# Verificar que dataset existe y tiene formato correcto
python -c "
import pandas as pd
df = pd.read_csv('data/oe2/bess/bess_simulation_hourly.csv', index_col=0, parse_dates=True)
assert len(df) == 8760, f'ERROR: {len(df)} rows (need 8760)'
assert isinstance(df.index, pd.DatetimeIndex), 'ERROR: Index must be DatetimeIndex'
assert 'peak_reduction_savings_normalized' in df.columns, 'MISSING: v5.4 metrics'
print('‚úÖ Dataset v5.4 est√° 100% listo')
print(f'   ‚Ä¢ {len(df):,} filas √ó {len(df.columns)} columnas')
print(f'   ‚Ä¢ √çndice: {df.index[0].date()} a {df.index[-1].date()}')
print(f'   ‚Ä¢ Ahorros: S/. {df[\"peak_reduction_savings_soles\"].sum():,.0f}/a√±o')
print(f'   ‚Ä¢ CO2 indirecto: {df[\"co2_avoided_indirect_kg\"].sum()/1000:.1f} ton/a√±o')
"
```

---

## 2Ô∏è‚É£ CREAR ENVIRONMENT CITYLEARN

```python
# run_agent_training.py (plantilla)

from __future__ import annotations
import pandas as pd
from src.citylearnv2.dataset_builder.dataset_builder import DatasetBuilder
from stable_baselines3 import SAC
import gymnasium as gym

# Cargar dataset v5.4
dataset_path = 'data/oe2/bess/bess_simulation_hourly.csv'
builder = DatasetBuilder(dataset_path=dataset_path)

# Crear environment CityLearn
env = builder.build_environment()

print(f"‚úÖ Environment creado:")
print(f"   ‚Ä¢ Observation space: {env.observation_space}")
print(f"   ‚Ä¢ Action space: {env.action_space}")
print(f"   ‚Ä¢ Episodes: 8,760 timesteps (1 a√±o)")

# Verificar que nuevas m√©tricas est√°n en observables
obs, info = env.reset()
print(f"\n‚úÖ Observables contienen v5.4 metrics:")
if 'peak_reduction_savings_normalized' in str(env.observation_space):
    print(f"   ‚Ä¢ peak_reduction_savings_normalized ‚úì")
if 'co2_avoided_indirect_normalized' in str(env.observation_space):
    print(f"   ‚Ä¢ co2_avoided_indirect_normalized ‚úì")
```

---

## 3Ô∏è‚É£ ENTRENAR AGENT SAC (OFF-POLICY)

```python
# train_sac_v54.py

from __future__ import annotations
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
import gymnasium as gym
from pathlib import Path

# Environment
env = gym.make('CityLearn-SingleAgent-v0', dataset_path='data/oe2/bess/bess_simulation_hourly.csv')

# Agent SAC
agent = SAC(
    'MlpPolicy',
    env,
    verbose=1,
    learning_rate=1e-3,
    buffer_size=100000,
    batch_size=256,
    gamma=0.99,
    tau=0.005,
    ent_coef=0.2,
    device='cuda',  # GPU
)

# Callback: guardar checkpoint cada 10k steps
checkpoint_dir = Path('checkpoints/SAC')
checkpoint_dir.mkdir(parents=True, exist_ok=True)

callback = EvalCallback(
    env,
    best_model_save_path=checkpoint_dir,
    log_path=checkpoint_dir,
    eval_freq=10000,
    n_eval_episodes=1,
    deterministic=True,
)

# Entrenar
print("üöÄ Iniciando entrenamiento SAC...")
agent.learn(
    total_timesteps=26280,  # ~1 d√≠a en GPU, ~3 episodios
    callback=callback,
    progress_bar=True,
)

# Guardar checkpoint final
agent.save(checkpoint_dir / 'final_model.zip')
print(f"‚úÖ Entrenamiento completo. Modelo guardado: {checkpoint_dir}/final_model.zip")
```

---

## 4Ô∏è‚É£ EVALUAR AGENT vs BASELINE

```python
# eval_agent_v54.py

from __future__ import annotations
import pandas as pd
import numpy as np
from stable_baselines3 import SAC
import gymnasium as gym

# Cargar agent entrenado
agent = SAC.load('checkpoints/SAC/final_model.zip')

# Environment
env = gym.make('CityLearn-SingleAgent-v0', dataset_path='data/oe2/bess/bess_simulation_hourly.csv')

# Evaluar
obs, info = env.reset()
total_reward = 0
episode_length = 0
co2_total = 0
ahorros_total = 0

for step in range(8760):
    action, _ = agent.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    
    total_reward += reward
    episode_length += 1
    
    # Extraer m√©tricas v5.4 de info
    if 'co2_avoided_indirect_normalized' in info:
        co2_total += info['co2_avoided_indirect_normalized']
    if 'peak_reduction_savings_normalized' in info:
        ahorros_total += info['peak_reduction_savings_normalized']
    
    if terminated or truncated:
        break

print("üìä RESULTADOS EVALUACI√ìN:")
print(f"   ‚Ä¢ Reward total: {total_reward:.2f}")
print(f"   ‚Ä¢ Timesteps: {episode_length}/8,760")
print(f"   ‚Ä¢ Ahorros acumulados: {ahorros_total:.2f}")
print(f"   ‚Ä¢ CO2 indirecto evitado: {co2_total:.2f}")
print(f"\n‚úÖ Agent ready para comparaci√≥n con baseline")
```

---

## 5Ô∏è‚É£ COMPARAR CON BASELINE (CON vs SIN SOLAR)

```bash
# Usar script existente
python -m scripts.run_dual_baselines --config configs/default.yaml

# Resultado esperado:
# WITH SOLAR (4,050 kWp):      CO2 ~190 ton/d√≠a ‚Üí Agent SAC reducir√° ~12-14%
# WITHOUT SOLAR (0 kWp):       CO2 ~640 ton/d√≠a ‚Üí Baseline m√°s alto, SAC mejora m√°s
```

---

## üìà CUADRO RESUMEN: M√âTRICAS v5.4 EN ACCI√ìN

### Dataset (Horario)
```
Hora 8 (Pico Ma√±ana):
  ‚Ä¢ PV: 1,629.4 kW ‚Üì (poco por hora temprana)
  ‚Ä¢ Mall: 1,200 kW (pico)
  ‚Ä¢ EV: 50 kW
  ‚Ä¢ BESS descarga: 380.9 kWh a Mall
  
  ‚Üí peak_reduction_savings_soles: S/. 139.22 (m√°ximo)
  ‚Üí co2_avoided_indirect_kg: 176.3 kg
  
Hora 14 (Pico Solar):
  ‚Ä¢ PV: 3,800 kW ‚Üë (m√°ximo)
  ‚Ä¢ Mall: 800 kW
  ‚Ä¢ EV: 30 kW
  ‚Ä¢ BESS carga: + 473 kWh
  
  ‚Üí peak_reduction_savings_soles: S/. 0 (BESS cargando)
  ‚Üí co2_avoided_indirect_kg: 0 kg
```

### Agent Durante Entrenamiento
```
Epoch 1 (Random):     Reward -10.5,  CO2=50%, Ahorros=20%
Epoch 50 (Learning):  Reward  +2.3,  CO2=68%, Ahorros=65%
Epoch 100 (Converged): Reward +5.8,  CO2=82%, Ahorros=78%
```

---

## ‚öôÔ∏è CONFIGURACI√ìN M√çNIMA (configs/default.yaml)

```yaml
# A√±adir/actualizar en config para v5.4

reward_function:
  type: "multi_objective"
  weights:
    co2_avoided_indirect: 0.50     # NUEVO - v5.4 metric
    peak_reduction_savings: 0.30   # NUEVO - v5.4 metric
    grid_import_reduction: 0.15
    bess_soc_stability: 0.05

observation_features:
  include_v54_metrics: true        # ‚úÖ Habilitar v5.4
  peak_reduction_savings_normalized: true
  co2_avoided_indirect_normalized: true

training:
  algorithm: "SAC"
  total_timesteps: 26280  # ~1 d√≠a GPU
  learning_rate: 1e-3
  batch_size: 256
  device: "cuda"
  checkpoint_freq: 10000
```

---

## ‚úÖ CHECKLIST FINALIZACI√ìN

- [ ] Dataset v5.4 validado (8,760 filas √ó 25 cols)
- [ ] DatetimeIndex en lugar de string
- [ ] M√©tricas v5.4 verificadas (ahorros + CO2)
- [ ] CityLearn environment creado
- [ ] Agent (SAC/PPO/A2C) configurado
- [ ] Reward function con pesos v5.4
- [ ] Entrenamiento iniciado
- [ ] Checkpoints guard√°ndose
- [ ] Evaluaci√≥n vs baseline
- [ ] Resultados documentados

---

## üÜò TROUBLESHOOTING R√ÅPIDO

| Error | Soluci√≥n |
|---|---|
| `FileNotFoundError: bess_simulation_hourly.csv` | Asegurar `data/oe2/bess/` existe; ejecutar `bess.py` |
| `Index must be DatetimeIndex` | En `csv`, columna index debe ser datetime; ejecutar `final_dataset_sync_v54.py` |
| `KeyError: peak_reduction_savings_normalized` | Ejecutar `bess.py` para regenerar dataset con v5.4 columnas |
| `ModuleNotFoundError: gymnasium` | `pip install gymnasium stable-baselines3` |
| GPU OOM durante entrenamiento | Reducir `batch_size` de 256 ‚Üí 128; `n_steps` de 2048 ‚Üí 1024 |
| Reward NaN/Inf | Verificar normalizaci√≥n [0,1] en dataset; ejecutar `validate_complete_dataset_v54.py` |

---

## üìö DOCUMENTACI√ìN RELACIONADA

- [DATASET_v54_FINAL_STATUS.md](./DATASET_v54_FINAL_STATUS.md) - Especificaci√≥n completa
- [.github/copilot-instructions.md](.github/copilot-instructions.md) - Patrones proyecto
- [src/agents/sac.py](src/agents/sac.py) - Implementaci√≥n SAC
- [src/citylearnv2/dataset_builder/dataset_builder.py](src/citylearnv2/dataset_builder/dataset_builder.py) - Integraci√≥n CityLearn

---

**Time to Production**: ~30 minutos  
**Training Duration**: ~5-7 horas (GPU RTX 4060)  
**Result**: RL Agent optimizando 38 chargers + BESS para minimizar CO‚ÇÇ  

üöÄ **¬°Listo para comenzar entrenamiento!**
